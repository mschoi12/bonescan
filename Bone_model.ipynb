{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "##필요한 라이브러리 import##\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "import random\n",
    "import cv2                \n",
    "import os\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "import torchvision\n",
    "from torchvision import datasets\n",
    "import torch\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "import torchvision.models as models\n",
    "import torchvision.transforms as transforms\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F;\n",
    "import torch.optim as optimizer\n",
    "from PIL import ImageFile\n",
    "from PIL import Image\n",
    "import IPython\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 개의 train set\n",
      "23 개의 test set\n"
     ]
    }
   ],
   "source": [
    "##저는 로컬로 작업을 해서 경로명을 사진이 저장된 제 PC로 설정하였습니다\n",
    "##colab에서 사용하실 경우 경로만 바꿔주셔서 데이터를 불러오시면 됩니다. \n",
    "import random\n",
    "import numpy as np\n",
    "from glob import glob\n",
    "\n",
    "eye_train_files = np.array(glob(\"./Metastasis/train/*/*\"))\n",
    "eye_test_files = np.array(glob(\"./Metastasis//test/*/*\"))\n",
    "\n",
    "#eye_train_files = np.array(glob(\"C:/Users/BrainK/Downloads/glaucoma/train/*/*\"))\n",
    "#eye_test_files = np.array(glob(\"C:/Users/BrainK/Downloads/glaucoma/test/*/*\"))\n",
    "\n",
    "\n",
    "\n",
    "print('%d 개의 train set' % len(eye_train_files))\n",
    "print('%d 개의 test set' % len(eye_test_files)) \n",
    "#보통 Train / Test / Validation 세트를 6:2:2로 나눕니다.\n",
    "#폴더별로 나누셔도 되고 아니면 한 폴데에 클래스만 나누신 후 랜덤으로 선택하셔도 좋습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "##이미지를 로드하는 함수입니다\n",
    "def image_loader(img_path, transform, use_cuda):\n",
    "    image = Image.open(img_path).convert('RGB')\n",
    "    img = transform(image)[:3,:,:].unsqueeze(0)\n",
    "    if use_cuda:\n",
    "        img = img.cuda()\n",
    "    return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mean_std(dataset):\n",
    "    meanRGB = [np.mean(image.numpy(), axis=(1,2)) for image,_ in dataset]\n",
    "    stdRGB = [np.std(image.numpy(), axis=(1,2)) for image,_ in dataset]\n",
    "\n",
    "    meanR = np.mean([m[0] for m in meanRGB])\n",
    "    meanG = np.mean([m[1] for m in meanRGB])\n",
    "    meanB = np.mean([m[2] for m in meanRGB])\n",
    "\n",
    "    stdR = np.mean([s[0] for s in stdRGB])\n",
    "    stdG = np.mean([s[1] for s in stdRGB])\n",
    "    stdB = np.mean([s[2] for s in stdRGB])\n",
    "\n",
    "    print(meanR, meanG, meanB)\n",
    "    print(stdR, stdG, stdB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "##분류되어있는 데이터를 각 폴더명에 맞게 분류하여 로드합니다\n",
    "\n",
    "newpath = Path(\"./Metastasis/\").expanduser()\n",
    "#newpath = Path(\"C:/Users/BrainK/Downloads/glaucoma/\").expanduser()\n",
    "\n",
    "train_path = str(newpath.joinpath(\"train\"))\n",
    "valid_path = str(newpath.joinpath(\"test\"))\n",
    "test_path = str(newpath.joinpath(\"test\"))\n",
    "\n",
    "IMAGE_SIZE = 512,1024 #파일 크기를 입력해주시면 됩니다. \n",
    "\n",
    "MEANS = [0.5]\n",
    "DEVIATIONS = [0.5]\n",
    "\n",
    "# 데이터를 보강하고 tensor로 변환합니다.\n",
    "train_transform = transforms.Compose([transforms.Resize(IMAGE_SIZE),\n",
    "                                transforms.ToTensor()])\n",
    "test_transform =  transforms.Compose([transforms.Resize(IMAGE_SIZE), \n",
    "                        #transforms.CenterCrop(240), \n",
    "                        transforms.ToTensor(), \n",
    "                        transforms.Normalize(MEANS, DEVIATIONS)]) \n",
    "\n",
    "#각 폴더명에 맞게 불러옵니다\n",
    "training = datasets.ImageFolder(train_path , transform=train_transform)\n",
    "validation = datasets.ImageFolder(valid_path , transform=test_transform)\n",
    "testing = datasets.ImageFolder(test_path , transform=test_transform)\n",
    "\n",
    "\n",
    "train_batches = torch.utils.data.DataLoader(training, batch_size=32, shuffle=True, num_workers=0,pin_memory=True) #batch-size를 너무 작게 잡거나 크게 잡으면 학습이 제대로 진행되지 않을 가능성이 큽니다. 만약 GPU메모리가 부족하다면 batch size를 작게 사용하세요. \n",
    "valid_batches = torch.utils.data.DataLoader(validation, batch_size=2, shuffle=True, num_workers=0,pin_memory=True) #shuffle은 데이터셋을 무작위로 선택하여 학습하는 기능인데 True로 두었을때(적용했을때) 결과가 더 좋았습니다.\n",
    "test_batches = torch.utils.data.DataLoader(testing, batch_size=8,num_workers=0,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9385255 0.9385255 0.9385255\n",
      "0.10765342 0.10765342 0.10765342\n"
     ]
    }
   ],
   "source": [
    "get_mean_std(training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "##분류되어있는 데이터를 각 폴더명에 맞게 분류하여 로드합니다\n",
    "\n",
    "newpath = Path(\"./Metastasis/\").expanduser()\n",
    "#newpath = Path(\"C:/Users/BrainK/Downloads/glaucoma/\").expanduser()\n",
    "\n",
    "train_path = str(newpath.joinpath(\"train\"))\n",
    "valid_path = str(newpath.joinpath(\"valid\"))\n",
    "test_path = str(newpath.joinpath(\"test\"))\n",
    "\n",
    "IMAGE_SIZE = 512,1024 #파일 크기를 입력해주시면 됩니다. \n",
    "\n",
    "MEANS = [0.9385255]\n",
    "DEVIATIONS = [0.10765342]\n",
    "\n",
    "#MEANS = [0.5,0.5,0.5]\n",
    "#DEVIATIONS = [0.5,0.5,0.5]\n",
    "\n",
    "\n",
    "# 데이터를 보강하고 tensor로 변환합니다.\n",
    "train_transform = transforms.Compose([#transforms.Resize(IMAGE_SIZE),\n",
    "                                #transforms.RandomResizedCrop(1000), #이미지를 자르고 크기를 변경합니다\n",
    "                                transforms.RandomRotation(degrees=15), #이미지를 돌립니다\n",
    "                                transforms.ColorJitter(), #이미지의 색상을 변경합니다\n",
    "                                #transforms.CenterCrop(size=800),#이미지의 가운데를 자릅니다\n",
    "                                transforms.RandomHorizontalFlip(),#이미지를 뒤집습니다\n",
    "                                transforms.ToTensor(),#텐서로 변환합니다\n",
    "                                transforms.Normalize(MEANS, DEVIATIONS)])\n",
    "test_transform =  transforms.Compose([transforms.Resize(IMAGE_SIZE), \n",
    "                        #transforms.CenterCrop(240), \n",
    "                        transforms.ToTensor(),\n",
    "                        transforms.Normalize(MEANS, DEVIATIONS)]) \n",
    "\n",
    "#각 폴더명에 맞게 불러옵니다\n",
    "training = datasets.ImageFolder(train_path , transform=train_transform)\n",
    "validation = datasets.ImageFolder(valid_path , transform=test_transform)\n",
    "testing = datasets.ImageFolder(test_path , transform=test_transform)\n",
    "\n",
    "\n",
    "train_batches = torch.utils.data.DataLoader(training, batch_size=4, shuffle=True, num_workers=0,pin_memory=True) #batch-size를 너무 작게 잡거나 크게 잡으면 학습이 제대로 진행되지 않을 가능성이 큽니다. 만약 GPU메모리가 부족하다면 batch size를 작게 사용하세요. \n",
    "valid_batches = torch.utils.data.DataLoader(validation, batch_size=4, shuffle=True, num_workers=0,pin_memory=True) #shuffle은 데이터셋을 무작위로 선택하여 학습하는 기능인데 True로 두었을때(적용했을때) 결과가 더 좋았습니다.\n",
    "test_batches = torch.utils.data.DataLoader(testing, batch_size=8,num_workers=0,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(valid_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer = models.resnet34(pretrained=True) #선행학습 된 모델을 불러옵니다. 저는 resent152로 선택하여 진행했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer = models.resnet152(pretrained=False) #선행학습 된 모델을 불러옵니다. 저는 resent152로 선택하여 진행했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_transfer = models.wide_resnet101_2(pretrained=False) #선행학습 된 모델을 불러옵니다. 저는 resent152로 선택하여 진행했습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "jupyter": {
     "outputs_hidden": true
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ResNet(\n",
       "  (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "  (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (relu): ReLU(inplace=True)\n",
       "  (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "  (layer1): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(64, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "        (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(128, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer2): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(256, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer3): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(512, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (3): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (4): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (5): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (6): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (7): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (8): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (9): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (10): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (11): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (12): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (13): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (14): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (15): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (16): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (17): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (18): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (19): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (20): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (21): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (22): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (layer4): Sequential(\n",
       "    (0): Bottleneck(\n",
       "      (conv1): Conv2d(1024, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (downsample): Sequential(\n",
       "        (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "        (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "    (2): Bottleneck(\n",
       "      (conv1): Conv2d(2048, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv2): Conv2d(1024, 1024, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "      (bn2): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (conv3): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "      (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      (relu): ReLU(inplace=True)\n",
       "    )\n",
       "  )\n",
       "  (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
       "  (fc): Linear(in_features=2048, out_features=1000, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transfer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "use_cuda = torch.cuda.is_available()\n",
    "\n",
    "for param in model_transfer.parameters():\n",
    "    param.requires_grad = True # 사전 학습된 gradient를 유지합니다. \n",
    "\n",
    "from collections import OrderedDict\n",
    "classifier = nn.Sequential(OrderedDict([ #모델의 마지막 부분을 다시 정의하여 학습합니다.\n",
    "                          ('fc1', nn.Linear(2048, 2)), #Fully conneted layer부분인데 입력단은 원래 개수로 맞춰주시고 마지막단의 개수는 class의 개수로 맞춰주시면 됩니다.앞서 구조의 맨 마지막 부분인 (fc): Linear(in_features=2048, out_features=1000, bias=True)를 보시면 됩니다. \n",
    "                          #('dropout', nn.Dropout(0.8)), #layer를 더 추가하셔도 됩니다. 다만 저의 경우 이 부분은 간단하게 두는것이 결과가 좋았습니다. \n",
    "                          #('fc2', nn.Linear(1024, 512)), \n",
    "                          #('dropout2', nn.Dropout(0.8)),\n",
    "                          #('fc3', nn.Linear(512, 2)), \n",
    "                          ('output', nn.LogSoftmax(dim=1))\n",
    "                          ]))\n",
    "    \n",
    "model_transfer.fc = classifier \n",
    "\n",
    "if use_cuda:\n",
    "    model_transfer = model_transfer.cuda()\n",
    "    \n",
    "# https://pytorch.org/tutorials/beginner/finetuning_torchvision_models_tutorial.html 를 참고하셔도 좋습니다.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion_transfer = nn.NLLLoss() #loss function을 NLLLoss(CrossEntropy)로 설정합니다. CrossEntropy가 NLLLoss와 softmax의 조합입니다.\n",
    "#optimizer_transfer = optimizer.SGD(model_transfer.parameters(), lr=0.0001, weight_decay=5e-4, momentum=0.9) #optimizer설정부분입니다. Adam이 성능이 좋아서 대부분 Adam을 사용합니다. learning rate는 0.0001~0.01사이로 설정하시면 됩니다. learning rate이 작을수록 보통 epoch을 더 많이 두어 오래 보셔야합니다.\n",
    "optimizer_transfer = optimizer.Adam(model_transfer.parameters(), lr=0.0001, weight_decay=5e-4) #optimizer설정부분입니다. Adam이 성능이 좋아서 대부분 Adam을 사용합니다. learning rate는 0.0001~0.01사이로 설정하시면 됩니다. learning rate이 작을수록 보통 epoch을 더 많이 두어 오래 보셔야합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#### 학습 부분입니다.\n",
    "\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "\n",
    "def train(n_epochs, train_loader, valid_loader, model, optimizer, criterion, use_cuda, save_path):\n",
    "    \n",
    "    if use_cuda:\n",
    "        model = model.cuda() \n",
    "    \n",
    "    valid_loss_min = np.Inf \n",
    "    train_losses_a = list()\n",
    "    valid_losses_a = list() \n",
    "    for epoch in range(1, n_epochs+1):\n",
    "        train_losses = 0.0\n",
    "        valid_losses = 0.0\n",
    "        \n",
    "       \n",
    "        model.train() \n",
    "        for data, target in train_loader:\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            optimizer.zero_grad()\n",
    "            output = model.forward(data)\n",
    "            loss = criterion(output, target)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_losses += loss.item()*data.size(0)\n",
    "            \n",
    "        model.eval()\n",
    "        for data, target in valid_loader:\n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            valid_losses += loss.item()*data.size(0)\n",
    "\n",
    "        train_losses = train_losses/len(train_loader.sampler)\n",
    "        valid_losses = valid_losses/len(valid_loader.sampler)\n",
    "        train_losses_a.append(train_losses)\n",
    "        valid_losses_a.append(valid_losses)\n",
    "\n",
    "        print('Epoch: {} \\tTraining Loss: {:.6f} \\tValidation Loss: {:.6f}'.format(\n",
    "            epoch, train_losses, valid_losses))\n",
    "\n",
    "        if valid_losses <= valid_loss_min: \n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}).  Saving model ...'.format(\n",
    "            valid_loss_min,\n",
    "            valid_losses))\n",
    "            torch.save(model.state_dict(), save_path) \n",
    "            valid_loss_min = valid_losses \n",
    "    \n",
    "    return model, train_losses_a, valid_losses_a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_path = Path(\"./Metastasis/\").expanduser() #validation loss가 감소할때마다 저장합니다. 저장할 경로를 설정하시면 됩니다.\n",
    "save_transfer = model_path.joinpath('./model_transfer.pt')\n",
    "#model_transfer.load_state_dict(torch.load(save_transfer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1 \tTraining Loss: 0.752286 \tValidation Loss: 2.161103\n",
      "Validation loss decreased (inf --> 2.161103).  Saving model ...\n",
      "Epoch: 2 \tTraining Loss: 0.704960 \tValidation Loss: 1.911734\n",
      "Validation loss decreased (2.161103 --> 1.911734).  Saving model ...\n",
      "Epoch: 3 \tTraining Loss: 0.740378 \tValidation Loss: 0.900781\n",
      "Validation loss decreased (1.911734 --> 0.900781).  Saving model ...\n",
      "Epoch: 4 \tTraining Loss: 0.664329 \tValidation Loss: 1.146669\n",
      "Epoch: 5 \tTraining Loss: 0.675426 \tValidation Loss: 1.573162\n",
      "Epoch: 6 \tTraining Loss: 0.735799 \tValidation Loss: 1.097833\n",
      "Epoch: 7 \tTraining Loss: 0.686110 \tValidation Loss: 0.735544\n",
      "Validation loss decreased (0.900781 --> 0.735544).  Saving model ...\n",
      "Epoch: 8 \tTraining Loss: 0.656851 \tValidation Loss: 0.860300\n",
      "Epoch: 9 \tTraining Loss: 0.777158 \tValidation Loss: 1.122247\n",
      "Epoch: 10 \tTraining Loss: 0.644194 \tValidation Loss: 1.610301\n",
      "Epoch: 11 \tTraining Loss: 0.697258 \tValidation Loss: 1.755479\n",
      "Epoch: 12 \tTraining Loss: 0.679093 \tValidation Loss: 0.694252\n",
      "Validation loss decreased (0.735544 --> 0.694252).  Saving model ...\n",
      "Epoch: 13 \tTraining Loss: 0.656819 \tValidation Loss: 1.342222\n",
      "Epoch: 14 \tTraining Loss: 0.643677 \tValidation Loss: 0.631101\n",
      "Validation loss decreased (0.694252 --> 0.631101).  Saving model ...\n",
      "Epoch: 15 \tTraining Loss: 0.637521 \tValidation Loss: 0.995262\n",
      "Epoch: 16 \tTraining Loss: 0.641333 \tValidation Loss: 0.684173\n",
      "Epoch: 17 \tTraining Loss: 0.654399 \tValidation Loss: 0.751532\n",
      "Epoch: 18 \tTraining Loss: 0.586685 \tValidation Loss: 0.626446\n",
      "Validation loss decreased (0.631101 --> 0.626446).  Saving model ...\n",
      "Epoch: 19 \tTraining Loss: 0.676905 \tValidation Loss: 0.635163\n",
      "Epoch: 20 \tTraining Loss: 0.622326 \tValidation Loss: 0.618151\n",
      "Validation loss decreased (0.626446 --> 0.618151).  Saving model ...\n",
      "Epoch: 21 \tTraining Loss: 0.673939 \tValidation Loss: 1.104696\n",
      "Epoch: 22 \tTraining Loss: 0.570650 \tValidation Loss: 0.760718\n",
      "Epoch: 23 \tTraining Loss: 0.605163 \tValidation Loss: 0.794066\n",
      "Epoch: 24 \tTraining Loss: 0.620601 \tValidation Loss: 1.073107\n",
      "Epoch: 25 \tTraining Loss: 0.609917 \tValidation Loss: 0.928474\n",
      "Epoch: 26 \tTraining Loss: 0.574884 \tValidation Loss: 0.619374\n",
      "Epoch: 27 \tTraining Loss: 0.628416 \tValidation Loss: 0.637645\n",
      "Epoch: 28 \tTraining Loss: 0.439923 \tValidation Loss: 0.692946\n",
      "Epoch: 29 \tTraining Loss: 0.611369 \tValidation Loss: 0.687612\n",
      "Epoch: 30 \tTraining Loss: 0.621153 \tValidation Loss: 0.622924\n",
      "Epoch: 31 \tTraining Loss: 0.646370 \tValidation Loss: 0.736333\n",
      "Epoch: 32 \tTraining Loss: 0.553752 \tValidation Loss: 0.639347\n",
      "Epoch: 33 \tTraining Loss: 0.555126 \tValidation Loss: 1.176605\n",
      "Epoch: 34 \tTraining Loss: 0.662863 \tValidation Loss: 0.801239\n",
      "Epoch: 35 \tTraining Loss: 0.544803 \tValidation Loss: 1.362530\n",
      "Epoch: 36 \tTraining Loss: 0.745734 \tValidation Loss: 0.892074\n",
      "Epoch: 37 \tTraining Loss: 0.620714 \tValidation Loss: 0.816176\n",
      "Epoch: 38 \tTraining Loss: 0.567181 \tValidation Loss: 0.693424\n",
      "Epoch: 39 \tTraining Loss: 0.480939 \tValidation Loss: 0.685196\n",
      "Epoch: 40 \tTraining Loss: 0.618250 \tValidation Loss: 0.700729\n",
      "Epoch: 41 \tTraining Loss: 0.601563 \tValidation Loss: 0.649277\n",
      "Epoch: 42 \tTraining Loss: 0.557138 \tValidation Loss: 0.621613\n",
      "Epoch: 43 \tTraining Loss: 0.580919 \tValidation Loss: 0.608457\n",
      "Validation loss decreased (0.618151 --> 0.608457).  Saving model ...\n",
      "Epoch: 44 \tTraining Loss: 0.609704 \tValidation Loss: 0.630279\n",
      "Epoch: 45 \tTraining Loss: 0.547288 \tValidation Loss: 0.575432\n",
      "Validation loss decreased (0.608457 --> 0.575432).  Saving model ...\n",
      "Epoch: 46 \tTraining Loss: 0.616664 \tValidation Loss: 0.573065\n",
      "Validation loss decreased (0.575432 --> 0.573065).  Saving model ...\n",
      "Epoch: 47 \tTraining Loss: 0.561601 \tValidation Loss: 0.554465\n",
      "Validation loss decreased (0.573065 --> 0.554465).  Saving model ...\n",
      "Epoch: 48 \tTraining Loss: 0.551412 \tValidation Loss: 0.544884\n",
      "Validation loss decreased (0.554465 --> 0.544884).  Saving model ...\n",
      "Epoch: 49 \tTraining Loss: 0.526145 \tValidation Loss: 0.578953\n",
      "Epoch: 50 \tTraining Loss: 0.532916 \tValidation Loss: 1.401526\n",
      "Epoch: 51 \tTraining Loss: 0.598935 \tValidation Loss: 0.663306\n",
      "Epoch: 52 \tTraining Loss: 0.575420 \tValidation Loss: 0.585286\n",
      "Epoch: 53 \tTraining Loss: 0.521126 \tValidation Loss: 0.741635\n",
      "Epoch: 54 \tTraining Loss: 0.613871 \tValidation Loss: 0.810710\n",
      "Epoch: 55 \tTraining Loss: 0.743244 \tValidation Loss: 1.025599\n",
      "Epoch: 56 \tTraining Loss: 0.579560 \tValidation Loss: 0.639268\n",
      "Epoch: 57 \tTraining Loss: 0.565208 \tValidation Loss: 0.639229\n",
      "Epoch: 58 \tTraining Loss: 0.604984 \tValidation Loss: 0.658125\n",
      "Epoch: 59 \tTraining Loss: 0.749593 \tValidation Loss: 0.642841\n",
      "Epoch: 60 \tTraining Loss: 0.551704 \tValidation Loss: 0.661633\n",
      "Epoch: 61 \tTraining Loss: 0.550330 \tValidation Loss: 0.612604\n",
      "Epoch: 62 \tTraining Loss: 0.588314 \tValidation Loss: 0.556123\n",
      "Epoch: 63 \tTraining Loss: 0.486929 \tValidation Loss: 0.563137\n",
      "Epoch: 64 \tTraining Loss: 0.550229 \tValidation Loss: 0.548847\n",
      "Epoch: 65 \tTraining Loss: 0.515821 \tValidation Loss: 0.610834\n",
      "Epoch: 66 \tTraining Loss: 0.511752 \tValidation Loss: 0.646117\n",
      "Epoch: 67 \tTraining Loss: 0.428122 \tValidation Loss: 0.889242\n",
      "Epoch: 68 \tTraining Loss: 0.676475 \tValidation Loss: 0.697348\n",
      "Epoch: 69 \tTraining Loss: 0.615660 \tValidation Loss: 0.616187\n",
      "Epoch: 70 \tTraining Loss: 0.654586 \tValidation Loss: 0.959747\n",
      "Epoch: 71 \tTraining Loss: 0.573494 \tValidation Loss: 0.644020\n",
      "Epoch: 72 \tTraining Loss: 0.504115 \tValidation Loss: 0.763354\n",
      "Epoch: 73 \tTraining Loss: 0.516086 \tValidation Loss: 0.834978\n",
      "Epoch: 74 \tTraining Loss: 0.801390 \tValidation Loss: 0.756555\n",
      "Epoch: 75 \tTraining Loss: 0.617519 \tValidation Loss: 0.649158\n",
      "Epoch: 76 \tTraining Loss: 0.606116 \tValidation Loss: 0.694542\n",
      "Epoch: 77 \tTraining Loss: 0.610615 \tValidation Loss: 0.640446\n",
      "Epoch: 78 \tTraining Loss: 0.594071 \tValidation Loss: 1.258338\n",
      "Epoch: 79 \tTraining Loss: 0.574739 \tValidation Loss: 0.589237\n",
      "Epoch: 80 \tTraining Loss: 0.562741 \tValidation Loss: 0.563867\n",
      "Epoch: 81 \tTraining Loss: 0.492364 \tValidation Loss: 0.659060\n",
      "Epoch: 82 \tTraining Loss: 0.524239 \tValidation Loss: 0.640575\n",
      "Epoch: 83 \tTraining Loss: 0.507763 \tValidation Loss: 0.868351\n",
      "Epoch: 84 \tTraining Loss: 0.584447 \tValidation Loss: 0.610588\n",
      "Epoch: 85 \tTraining Loss: 0.515756 \tValidation Loss: 0.608176\n",
      "Epoch: 86 \tTraining Loss: 0.477449 \tValidation Loss: 0.680882\n",
      "Epoch: 87 \tTraining Loss: 0.434115 \tValidation Loss: 0.986817\n",
      "Epoch: 88 \tTraining Loss: 0.532444 \tValidation Loss: 0.874711\n",
      "Epoch: 89 \tTraining Loss: 0.539883 \tValidation Loss: 0.797593\n",
      "Epoch: 90 \tTraining Loss: 0.532850 \tValidation Loss: 0.698334\n",
      "Epoch: 91 \tTraining Loss: 0.492298 \tValidation Loss: 0.619505\n",
      "Epoch: 92 \tTraining Loss: 0.502549 \tValidation Loss: 0.592478\n",
      "Epoch: 93 \tTraining Loss: 0.487305 \tValidation Loss: 0.680732\n",
      "Epoch: 94 \tTraining Loss: 0.598156 \tValidation Loss: 1.072205\n",
      "Epoch: 95 \tTraining Loss: 0.495977 \tValidation Loss: 0.609225\n",
      "Epoch: 96 \tTraining Loss: 0.471210 \tValidation Loss: 0.696052\n",
      "Epoch: 97 \tTraining Loss: 0.512074 \tValidation Loss: 0.632391\n",
      "Epoch: 98 \tTraining Loss: 0.439858 \tValidation Loss: 0.602273\n",
      "Epoch: 99 \tTraining Loss: 0.417936 \tValidation Loss: 0.784980\n",
      "Epoch: 100 \tTraining Loss: 0.324573 \tValidation Loss: 0.699189\n",
      "Epoch: 101 \tTraining Loss: 0.535180 \tValidation Loss: 5.164576\n",
      "Epoch: 102 \tTraining Loss: 0.594147 \tValidation Loss: 2.377486\n",
      "Epoch: 103 \tTraining Loss: 0.805616 \tValidation Loss: 0.621648\n",
      "Epoch: 104 \tTraining Loss: 0.678348 \tValidation Loss: 0.837105\n",
      "Epoch: 105 \tTraining Loss: 0.571576 \tValidation Loss: 0.643700\n",
      "Epoch: 106 \tTraining Loss: 0.591272 \tValidation Loss: 0.797965\n",
      "Epoch: 107 \tTraining Loss: 0.521702 \tValidation Loss: 0.820419\n",
      "Epoch: 108 \tTraining Loss: 0.633897 \tValidation Loss: 1.188936\n",
      "Epoch: 109 \tTraining Loss: 0.466902 \tValidation Loss: 0.905962\n",
      "Epoch: 110 \tTraining Loss: 0.524844 \tValidation Loss: 0.550034\n",
      "Epoch: 111 \tTraining Loss: 0.521311 \tValidation Loss: 0.553371\n",
      "Epoch: 112 \tTraining Loss: 0.553669 \tValidation Loss: 0.769245\n",
      "Epoch: 113 \tTraining Loss: 0.440071 \tValidation Loss: 0.656002\n",
      "Epoch: 114 \tTraining Loss: 0.441598 \tValidation Loss: 0.811512\n",
      "Epoch: 115 \tTraining Loss: 0.451021 \tValidation Loss: 0.763764\n",
      "Epoch: 116 \tTraining Loss: 0.538352 \tValidation Loss: 0.580177\n",
      "Epoch: 117 \tTraining Loss: 0.418261 \tValidation Loss: 0.861546\n",
      "Epoch: 118 \tTraining Loss: 0.495027 \tValidation Loss: 0.681099\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 119 \tTraining Loss: 0.416524 \tValidation Loss: 0.842327\n",
      "Epoch: 120 \tTraining Loss: 0.498830 \tValidation Loss: 0.888571\n",
      "Epoch: 121 \tTraining Loss: 0.458427 \tValidation Loss: 1.270118\n",
      "Epoch: 122 \tTraining Loss: 0.483028 \tValidation Loss: 1.740788\n",
      "Epoch: 123 \tTraining Loss: 0.555363 \tValidation Loss: 0.713756\n",
      "Epoch: 124 \tTraining Loss: 0.393785 \tValidation Loss: 1.157196\n",
      "Epoch: 125 \tTraining Loss: 0.409888 \tValidation Loss: 0.765983\n",
      "Epoch: 126 \tTraining Loss: 0.505122 \tValidation Loss: 0.896469\n",
      "Epoch: 127 \tTraining Loss: 0.488153 \tValidation Loss: 0.954187\n",
      "Epoch: 128 \tTraining Loss: 0.398243 \tValidation Loss: 0.751598\n",
      "Epoch: 129 \tTraining Loss: 0.552294 \tValidation Loss: 2.092679\n",
      "Epoch: 130 \tTraining Loss: 0.515651 \tValidation Loss: 0.654260\n",
      "Epoch: 131 \tTraining Loss: 0.396567 \tValidation Loss: 0.750711\n",
      "Epoch: 132 \tTraining Loss: 0.384348 \tValidation Loss: 0.796742\n",
      "Epoch: 133 \tTraining Loss: 0.342905 \tValidation Loss: 0.724424\n",
      "Epoch: 134 \tTraining Loss: 0.364284 \tValidation Loss: 0.932183\n",
      "Epoch: 135 \tTraining Loss: 0.608037 \tValidation Loss: 0.655229\n",
      "Epoch: 136 \tTraining Loss: 0.535597 \tValidation Loss: 0.957854\n",
      "Epoch: 137 \tTraining Loss: 0.444846 \tValidation Loss: 0.673798\n",
      "Epoch: 138 \tTraining Loss: 0.469618 \tValidation Loss: 1.582010\n",
      "Epoch: 139 \tTraining Loss: 0.390262 \tValidation Loss: 0.641621\n",
      "Epoch: 140 \tTraining Loss: 0.570249 \tValidation Loss: 0.538242\n",
      "Validation loss decreased (0.544884 --> 0.538242).  Saving model ...\n",
      "Epoch: 141 \tTraining Loss: 0.410029 \tValidation Loss: 0.718905\n",
      "Epoch: 142 \tTraining Loss: 0.446675 \tValidation Loss: 1.007890\n",
      "Epoch: 143 \tTraining Loss: 0.368973 \tValidation Loss: 0.865939\n",
      "Epoch: 144 \tTraining Loss: 0.458025 \tValidation Loss: 0.833478\n",
      "Epoch: 145 \tTraining Loss: 0.314027 \tValidation Loss: 5.415879\n",
      "Epoch: 146 \tTraining Loss: 0.486107 \tValidation Loss: 0.820522\n",
      "Epoch: 147 \tTraining Loss: 0.423962 \tValidation Loss: 1.044576\n",
      "Epoch: 148 \tTraining Loss: 0.292220 \tValidation Loss: 0.766435\n",
      "Epoch: 149 \tTraining Loss: 0.493006 \tValidation Loss: 1.129296\n",
      "Epoch: 150 \tTraining Loss: 0.351955 \tValidation Loss: 0.962763\n",
      "Epoch: 151 \tTraining Loss: 0.354234 \tValidation Loss: 0.793631\n",
      "Epoch: 152 \tTraining Loss: 0.408378 \tValidation Loss: 0.744346\n",
      "Epoch: 153 \tTraining Loss: 0.408578 \tValidation Loss: 0.795156\n",
      "Epoch: 154 \tTraining Loss: 0.313109 \tValidation Loss: 1.138438\n",
      "Epoch: 155 \tTraining Loss: 0.285428 \tValidation Loss: 1.858604\n",
      "Epoch: 156 \tTraining Loss: 0.502448 \tValidation Loss: 1.087801\n",
      "Epoch: 157 \tTraining Loss: 0.394644 \tValidation Loss: 0.830709\n",
      "Epoch: 158 \tTraining Loss: 0.425556 \tValidation Loss: 0.840766\n",
      "Epoch: 159 \tTraining Loss: 0.350655 \tValidation Loss: 0.916702\n",
      "Epoch: 160 \tTraining Loss: 0.520617 \tValidation Loss: 1.340223\n",
      "Epoch: 161 \tTraining Loss: 0.368137 \tValidation Loss: 0.895312\n",
      "Epoch: 162 \tTraining Loss: 0.423460 \tValidation Loss: 1.077627\n",
      "Epoch: 163 \tTraining Loss: 0.393371 \tValidation Loss: 1.101519\n",
      "Epoch: 164 \tTraining Loss: 0.358125 \tValidation Loss: 0.881051\n",
      "Epoch: 165 \tTraining Loss: 0.290350 \tValidation Loss: 0.993387\n",
      "Epoch: 166 \tTraining Loss: 0.239050 \tValidation Loss: 1.129505\n",
      "Epoch: 167 \tTraining Loss: 0.297731 \tValidation Loss: 1.229556\n",
      "Epoch: 168 \tTraining Loss: 0.497054 \tValidation Loss: 1.669476\n",
      "Epoch: 169 \tTraining Loss: 0.372514 \tValidation Loss: 1.070548\n",
      "Epoch: 170 \tTraining Loss: 0.245406 \tValidation Loss: 0.809245\n",
      "Epoch: 171 \tTraining Loss: 0.505427 \tValidation Loss: 1.167840\n",
      "Epoch: 172 \tTraining Loss: 0.316087 \tValidation Loss: 0.757623\n",
      "Epoch: 173 \tTraining Loss: 0.261297 \tValidation Loss: 0.758217\n",
      "Epoch: 174 \tTraining Loss: 0.334818 \tValidation Loss: 0.965950\n",
      "Epoch: 175 \tTraining Loss: 0.361944 \tValidation Loss: 0.659865\n",
      "Epoch: 176 \tTraining Loss: 0.574418 \tValidation Loss: 0.710652\n",
      "Epoch: 177 \tTraining Loss: 0.300568 \tValidation Loss: 0.751714\n",
      "Epoch: 178 \tTraining Loss: 0.238611 \tValidation Loss: 0.695610\n",
      "Epoch: 179 \tTraining Loss: 0.242962 \tValidation Loss: 1.468021\n",
      "Epoch: 180 \tTraining Loss: 0.465524 \tValidation Loss: 1.094010\n",
      "Epoch: 181 \tTraining Loss: 0.316748 \tValidation Loss: 0.723118\n",
      "Epoch: 182 \tTraining Loss: 0.363363 \tValidation Loss: 0.728179\n",
      "Epoch: 183 \tTraining Loss: 0.306111 \tValidation Loss: 0.880092\n",
      "Epoch: 184 \tTraining Loss: 0.327285 \tValidation Loss: 1.521361\n",
      "Epoch: 185 \tTraining Loss: 0.419201 \tValidation Loss: 0.939198\n",
      "Epoch: 186 \tTraining Loss: 0.198039 \tValidation Loss: 1.636109\n",
      "Epoch: 187 \tTraining Loss: 0.328520 \tValidation Loss: 0.742029\n",
      "Epoch: 188 \tTraining Loss: 0.285769 \tValidation Loss: 0.953227\n",
      "Epoch: 189 \tTraining Loss: 0.256513 \tValidation Loss: 1.140858\n",
      "Epoch: 190 \tTraining Loss: 0.188246 \tValidation Loss: 1.390344\n",
      "Epoch: 191 \tTraining Loss: 0.284230 \tValidation Loss: 1.772303\n",
      "Epoch: 192 \tTraining Loss: 0.273753 \tValidation Loss: 0.865978\n",
      "Epoch: 193 \tTraining Loss: 0.240417 \tValidation Loss: 1.200694\n",
      "Epoch: 194 \tTraining Loss: 0.194030 \tValidation Loss: 1.365679\n",
      "Epoch: 195 \tTraining Loss: 0.296106 \tValidation Loss: 0.767091\n",
      "Epoch: 196 \tTraining Loss: 0.230578 \tValidation Loss: 1.080941\n",
      "Epoch: 197 \tTraining Loss: 0.221630 \tValidation Loss: 0.882082\n",
      "Epoch: 198 \tTraining Loss: 0.313021 \tValidation Loss: 1.250742\n",
      "Epoch: 199 \tTraining Loss: 0.321176 \tValidation Loss: 1.022374\n",
      "Epoch: 200 \tTraining Loss: 0.428833 \tValidation Loss: 1.006869\n",
      "Epoch: 201 \tTraining Loss: 0.284382 \tValidation Loss: 0.610847\n",
      "Epoch: 202 \tTraining Loss: 0.285184 \tValidation Loss: 0.846988\n",
      "Epoch: 203 \tTraining Loss: 0.181643 \tValidation Loss: 0.811691\n",
      "Epoch: 204 \tTraining Loss: 0.138537 \tValidation Loss: 0.748827\n",
      "Epoch: 205 \tTraining Loss: 0.310813 \tValidation Loss: 0.707748\n",
      "Epoch: 206 \tTraining Loss: 0.405187 \tValidation Loss: 0.895702\n",
      "Epoch: 207 \tTraining Loss: 0.216705 \tValidation Loss: 0.880265\n",
      "Epoch: 208 \tTraining Loss: 0.239203 \tValidation Loss: 0.691002\n",
      "Epoch: 209 \tTraining Loss: 0.219796 \tValidation Loss: 0.913164\n",
      "Epoch: 210 \tTraining Loss: 0.365983 \tValidation Loss: 0.808189\n",
      "Epoch: 211 \tTraining Loss: 0.267409 \tValidation Loss: 0.846403\n",
      "Epoch: 212 \tTraining Loss: 0.294015 \tValidation Loss: 0.773097\n",
      "Epoch: 213 \tTraining Loss: 0.359563 \tValidation Loss: 0.896461\n",
      "Epoch: 214 \tTraining Loss: 0.182292 \tValidation Loss: 1.184853\n",
      "Epoch: 215 \tTraining Loss: 0.234694 \tValidation Loss: 1.156086\n",
      "Epoch: 216 \tTraining Loss: 0.247339 \tValidation Loss: 1.188638\n",
      "Epoch: 217 \tTraining Loss: 0.438750 \tValidation Loss: 1.166483\n",
      "Epoch: 218 \tTraining Loss: 0.325743 \tValidation Loss: 0.936566\n",
      "Epoch: 219 \tTraining Loss: 0.296391 \tValidation Loss: 0.774128\n",
      "Epoch: 220 \tTraining Loss: 0.415117 \tValidation Loss: 1.282271\n",
      "Epoch: 221 \tTraining Loss: 0.281291 \tValidation Loss: 0.981366\n",
      "Epoch: 222 \tTraining Loss: 0.209149 \tValidation Loss: 0.848127\n",
      "Epoch: 223 \tTraining Loss: 0.363767 \tValidation Loss: 0.792943\n",
      "Epoch: 224 \tTraining Loss: 0.472885 \tValidation Loss: 0.649305\n",
      "Epoch: 225 \tTraining Loss: 0.340839 \tValidation Loss: 0.809213\n",
      "Epoch: 226 \tTraining Loss: 0.251030 \tValidation Loss: 1.221674\n",
      "Epoch: 227 \tTraining Loss: 0.312635 \tValidation Loss: 0.991779\n",
      "Epoch: 228 \tTraining Loss: 0.268842 \tValidation Loss: 0.714366\n",
      "Epoch: 229 \tTraining Loss: 0.247693 \tValidation Loss: 1.103302\n",
      "Epoch: 230 \tTraining Loss: 0.290032 \tValidation Loss: 0.536906\n",
      "Validation loss decreased (0.538242 --> 0.536906).  Saving model ...\n",
      "Epoch: 231 \tTraining Loss: 0.248987 \tValidation Loss: 0.789763\n",
      "Epoch: 232 \tTraining Loss: 0.110836 \tValidation Loss: 0.730115\n",
      "Epoch: 233 \tTraining Loss: 0.274468 \tValidation Loss: 0.693705\n",
      "Epoch: 234 \tTraining Loss: 0.325265 \tValidation Loss: 8.369341\n",
      "Epoch: 235 \tTraining Loss: 0.924992 \tValidation Loss: 16.733659\n",
      "Epoch: 236 \tTraining Loss: 0.450321 \tValidation Loss: 1.634646\n",
      "Epoch: 237 \tTraining Loss: 0.431653 \tValidation Loss: 0.514153\n",
      "Validation loss decreased (0.536906 --> 0.514153).  Saving model ...\n",
      "Epoch: 238 \tTraining Loss: 0.256953 \tValidation Loss: 0.787671\n",
      "Epoch: 239 \tTraining Loss: 0.176525 \tValidation Loss: 1.518760\n",
      "Epoch: 240 \tTraining Loss: 0.262836 \tValidation Loss: 0.688157\n",
      "Epoch: 241 \tTraining Loss: 0.196050 \tValidation Loss: 1.489672\n",
      "Epoch: 242 \tTraining Loss: 0.162435 \tValidation Loss: 1.214966\n",
      "Epoch: 243 \tTraining Loss: 0.462858 \tValidation Loss: 1.118945\n",
      "Epoch: 244 \tTraining Loss: 0.580422 \tValidation Loss: 1.650684\n",
      "Epoch: 245 \tTraining Loss: 0.330685 \tValidation Loss: 0.923797\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 246 \tTraining Loss: 0.300014 \tValidation Loss: 1.143833\n",
      "Epoch: 247 \tTraining Loss: 0.291995 \tValidation Loss: 1.196704\n",
      "Epoch: 248 \tTraining Loss: 0.302772 \tValidation Loss: 0.940138\n",
      "Epoch: 249 \tTraining Loss: 0.371506 \tValidation Loss: 0.711057\n",
      "Epoch: 250 \tTraining Loss: 0.237833 \tValidation Loss: 1.116549\n",
      "Epoch: 251 \tTraining Loss: 0.137300 \tValidation Loss: 1.198765\n",
      "Epoch: 252 \tTraining Loss: 0.243053 \tValidation Loss: 1.769161\n",
      "Epoch: 253 \tTraining Loss: 0.223306 \tValidation Loss: 1.845138\n",
      "Epoch: 254 \tTraining Loss: 0.444515 \tValidation Loss: 1.147933\n",
      "Epoch: 255 \tTraining Loss: 0.291419 \tValidation Loss: 1.293828\n",
      "Epoch: 256 \tTraining Loss: 0.385892 \tValidation Loss: 1.605728\n",
      "Epoch: 257 \tTraining Loss: 0.330346 \tValidation Loss: 1.528466\n",
      "Epoch: 258 \tTraining Loss: 0.225195 \tValidation Loss: 0.911062\n",
      "Epoch: 259 \tTraining Loss: 0.352340 \tValidation Loss: 1.572830\n",
      "Epoch: 260 \tTraining Loss: 0.167434 \tValidation Loss: 1.354494\n",
      "Epoch: 261 \tTraining Loss: 0.406740 \tValidation Loss: 0.979259\n",
      "Epoch: 262 \tTraining Loss: 0.288159 \tValidation Loss: 1.568030\n",
      "Epoch: 263 \tTraining Loss: 0.312404 \tValidation Loss: 0.723958\n",
      "Epoch: 264 \tTraining Loss: 0.284630 \tValidation Loss: 1.397983\n",
      "Epoch: 265 \tTraining Loss: 0.120333 \tValidation Loss: 1.174523\n",
      "Epoch: 266 \tTraining Loss: 0.135104 \tValidation Loss: 1.458526\n",
      "Epoch: 267 \tTraining Loss: 0.118791 \tValidation Loss: 1.451498\n",
      "Epoch: 268 \tTraining Loss: 0.211575 \tValidation Loss: 1.086303\n",
      "Epoch: 269 \tTraining Loss: 0.279068 \tValidation Loss: 1.195645\n",
      "Epoch: 270 \tTraining Loss: 0.426901 \tValidation Loss: 1.297107\n",
      "Epoch: 271 \tTraining Loss: 0.491637 \tValidation Loss: 1.033960\n",
      "Epoch: 272 \tTraining Loss: 0.285076 \tValidation Loss: 1.399050\n",
      "Epoch: 273 \tTraining Loss: 0.248247 \tValidation Loss: 0.896078\n",
      "Epoch: 274 \tTraining Loss: 0.283133 \tValidation Loss: 1.387160\n",
      "Epoch: 275 \tTraining Loss: 0.094007 \tValidation Loss: 1.454388\n",
      "Epoch: 276 \tTraining Loss: 0.295341 \tValidation Loss: 1.634594\n",
      "Epoch: 277 \tTraining Loss: 0.261289 \tValidation Loss: 1.733335\n",
      "Epoch: 278 \tTraining Loss: 0.221490 \tValidation Loss: 1.582297\n",
      "Epoch: 279 \tTraining Loss: 0.270271 \tValidation Loss: 0.721366\n",
      "Epoch: 280 \tTraining Loss: 0.281738 \tValidation Loss: 0.886500\n",
      "Epoch: 281 \tTraining Loss: 0.366773 \tValidation Loss: 1.114390\n",
      "Epoch: 282 \tTraining Loss: 0.121377 \tValidation Loss: 1.542691\n",
      "Epoch: 283 \tTraining Loss: 0.130953 \tValidation Loss: 0.886391\n",
      "Epoch: 284 \tTraining Loss: 0.271027 \tValidation Loss: 1.782030\n",
      "Epoch: 285 \tTraining Loss: 0.244541 \tValidation Loss: 0.751898\n",
      "Epoch: 286 \tTraining Loss: 0.390162 \tValidation Loss: 1.004321\n",
      "Epoch: 287 \tTraining Loss: 0.447342 \tValidation Loss: 1.035524\n",
      "Epoch: 288 \tTraining Loss: 0.198914 \tValidation Loss: 0.782234\n",
      "Epoch: 289 \tTraining Loss: 0.189073 \tValidation Loss: 1.209404\n",
      "Epoch: 290 \tTraining Loss: 0.136788 \tValidation Loss: 1.099947\n",
      "Epoch: 291 \tTraining Loss: 0.228937 \tValidation Loss: 0.993281\n",
      "Epoch: 292 \tTraining Loss: 0.225304 \tValidation Loss: 1.591948\n",
      "Epoch: 293 \tTraining Loss: 0.186956 \tValidation Loss: 1.500049\n",
      "Epoch: 294 \tTraining Loss: 0.271650 \tValidation Loss: 1.075498\n",
      "Epoch: 295 \tTraining Loss: 0.423026 \tValidation Loss: 1.128494\n",
      "Epoch: 296 \tTraining Loss: 0.311154 \tValidation Loss: 1.052074\n",
      "Epoch: 297 \tTraining Loss: 0.219708 \tValidation Loss: 2.070612\n",
      "Epoch: 298 \tTraining Loss: 0.253909 \tValidation Loss: 0.999577\n",
      "Epoch: 299 \tTraining Loss: 0.248097 \tValidation Loss: 1.420362\n",
      "Epoch: 300 \tTraining Loss: 0.332299 \tValidation Loss: 1.140853\n",
      "Epoch: 301 \tTraining Loss: 0.224652 \tValidation Loss: 1.119381\n",
      "Epoch: 302 \tTraining Loss: 0.140244 \tValidation Loss: 1.070211\n",
      "Epoch: 303 \tTraining Loss: 0.175972 \tValidation Loss: 0.763919\n",
      "Epoch: 304 \tTraining Loss: 0.298425 \tValidation Loss: 2.163156\n",
      "Epoch: 305 \tTraining Loss: 0.338461 \tValidation Loss: 0.984814\n",
      "Epoch: 306 \tTraining Loss: 0.388076 \tValidation Loss: 1.513059\n",
      "Epoch: 307 \tTraining Loss: 0.370123 \tValidation Loss: 0.680150\n",
      "Epoch: 308 \tTraining Loss: 0.346715 \tValidation Loss: 1.095587\n",
      "Epoch: 309 \tTraining Loss: 0.238733 \tValidation Loss: 0.522007\n",
      "Epoch: 310 \tTraining Loss: 0.131193 \tValidation Loss: 0.736662\n",
      "Epoch: 311 \tTraining Loss: 0.377549 \tValidation Loss: 1.128278\n",
      "Epoch: 312 \tTraining Loss: 0.319284 \tValidation Loss: 1.460986\n",
      "Epoch: 313 \tTraining Loss: 0.405781 \tValidation Loss: 1.122884\n",
      "Epoch: 314 \tTraining Loss: 0.270330 \tValidation Loss: 0.843627\n",
      "Epoch: 315 \tTraining Loss: 0.360958 \tValidation Loss: 1.554518\n",
      "Epoch: 316 \tTraining Loss: 0.376935 \tValidation Loss: 1.594751\n",
      "Epoch: 317 \tTraining Loss: 0.234306 \tValidation Loss: 0.949107\n",
      "Epoch: 318 \tTraining Loss: 0.254980 \tValidation Loss: 1.128315\n",
      "Epoch: 319 \tTraining Loss: 0.192610 \tValidation Loss: 1.167089\n",
      "Epoch: 320 \tTraining Loss: 0.102147 \tValidation Loss: 1.046163\n",
      "Epoch: 321 \tTraining Loss: 0.455864 \tValidation Loss: 1.777928\n",
      "Epoch: 322 \tTraining Loss: 0.344368 \tValidation Loss: 0.737403\n",
      "Epoch: 323 \tTraining Loss: 0.274924 \tValidation Loss: 0.925953\n",
      "Epoch: 324 \tTraining Loss: 0.225837 \tValidation Loss: 0.582307\n",
      "Epoch: 325 \tTraining Loss: 0.201957 \tValidation Loss: 0.670045\n",
      "Epoch: 326 \tTraining Loss: 0.240411 \tValidation Loss: 0.961669\n",
      "Epoch: 327 \tTraining Loss: 0.269392 \tValidation Loss: 0.764797\n",
      "Epoch: 328 \tTraining Loss: 0.398999 \tValidation Loss: 1.241249\n",
      "Epoch: 329 \tTraining Loss: 0.165178 \tValidation Loss: 0.933500\n",
      "Epoch: 330 \tTraining Loss: 0.136770 \tValidation Loss: 1.171381\n",
      "Epoch: 331 \tTraining Loss: 0.127670 \tValidation Loss: 0.634063\n",
      "Epoch: 332 \tTraining Loss: 0.325013 \tValidation Loss: 0.653599\n",
      "Epoch: 333 \tTraining Loss: 0.208963 \tValidation Loss: 1.284386\n",
      "Epoch: 334 \tTraining Loss: 0.136078 \tValidation Loss: 0.990126\n",
      "Epoch: 335 \tTraining Loss: 0.118623 \tValidation Loss: 1.237938\n",
      "Epoch: 336 \tTraining Loss: 0.104750 \tValidation Loss: 0.845691\n",
      "Epoch: 337 \tTraining Loss: 0.166383 \tValidation Loss: 1.007682\n",
      "Epoch: 338 \tTraining Loss: 0.150069 \tValidation Loss: 1.025598\n",
      "Epoch: 339 \tTraining Loss: 0.293278 \tValidation Loss: 0.869091\n",
      "Epoch: 340 \tTraining Loss: 0.177406 \tValidation Loss: 2.501468\n",
      "Epoch: 341 \tTraining Loss: 0.436538 \tValidation Loss: 1.246765\n",
      "Epoch: 342 \tTraining Loss: 0.261730 \tValidation Loss: 0.701559\n",
      "Epoch: 343 \tTraining Loss: 0.186531 \tValidation Loss: 0.663344\n",
      "Epoch: 344 \tTraining Loss: 0.240890 \tValidation Loss: 0.939728\n",
      "Epoch: 345 \tTraining Loss: 0.313202 \tValidation Loss: 0.787214\n",
      "Epoch: 346 \tTraining Loss: 0.163538 \tValidation Loss: 0.739712\n",
      "Epoch: 347 \tTraining Loss: 0.202924 \tValidation Loss: 1.056925\n",
      "Epoch: 348 \tTraining Loss: 0.159418 \tValidation Loss: 0.972690\n",
      "Epoch: 349 \tTraining Loss: 0.144269 \tValidation Loss: 1.041678\n",
      "Epoch: 350 \tTraining Loss: 0.114016 \tValidation Loss: 0.895151\n",
      "Epoch: 351 \tTraining Loss: 0.242700 \tValidation Loss: 1.037335\n",
      "Epoch: 352 \tTraining Loss: 0.077725 \tValidation Loss: 0.619205\n",
      "Epoch: 353 \tTraining Loss: 0.132016 \tValidation Loss: 1.247661\n",
      "Epoch: 354 \tTraining Loss: 0.074254 \tValidation Loss: 0.795617\n",
      "Epoch: 355 \tTraining Loss: 0.130258 \tValidation Loss: 0.851081\n",
      "Epoch: 356 \tTraining Loss: 0.285900 \tValidation Loss: 2.046051\n",
      "Epoch: 357 \tTraining Loss: 0.222402 \tValidation Loss: 1.186182\n",
      "Epoch: 358 \tTraining Loss: 0.127128 \tValidation Loss: 1.109539\n",
      "Epoch: 359 \tTraining Loss: 0.129529 \tValidation Loss: 2.141040\n",
      "Epoch: 360 \tTraining Loss: 0.118155 \tValidation Loss: 1.075958\n",
      "Epoch: 361 \tTraining Loss: 0.234186 \tValidation Loss: 0.905765\n",
      "Epoch: 362 \tTraining Loss: 0.158942 \tValidation Loss: 0.705371\n",
      "Epoch: 363 \tTraining Loss: 0.239396 \tValidation Loss: 1.245333\n",
      "Epoch: 364 \tTraining Loss: 0.193079 \tValidation Loss: 0.680863\n",
      "Epoch: 365 \tTraining Loss: 0.208785 \tValidation Loss: 1.342095\n",
      "Epoch: 366 \tTraining Loss: 0.160365 \tValidation Loss: 0.738343\n",
      "Epoch: 367 \tTraining Loss: 0.138824 \tValidation Loss: 0.592498\n",
      "Epoch: 368 \tTraining Loss: 0.117538 \tValidation Loss: 0.802206\n",
      "Epoch: 369 \tTraining Loss: 0.275788 \tValidation Loss: 1.011081\n",
      "Epoch: 370 \tTraining Loss: 0.096078 \tValidation Loss: 0.725976\n",
      "Epoch: 371 \tTraining Loss: 0.058502 \tValidation Loss: 1.181686\n",
      "Epoch: 372 \tTraining Loss: 0.094140 \tValidation Loss: 1.288960\n",
      "Epoch: 373 \tTraining Loss: 0.117133 \tValidation Loss: 0.886458\n",
      "Epoch: 374 \tTraining Loss: 0.369609 \tValidation Loss: 1.113239\n",
      "Epoch: 375 \tTraining Loss: 0.260498 \tValidation Loss: 1.159536\n",
      "Epoch: 376 \tTraining Loss: 0.083223 \tValidation Loss: 0.790287\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 377 \tTraining Loss: 0.134905 \tValidation Loss: 0.806895\n",
      "Epoch: 378 \tTraining Loss: 0.144743 \tValidation Loss: 0.993096\n",
      "Epoch: 379 \tTraining Loss: 0.063769 \tValidation Loss: 0.841393\n",
      "Epoch: 380 \tTraining Loss: 0.079392 \tValidation Loss: 0.690205\n",
      "Epoch: 381 \tTraining Loss: 0.206144 \tValidation Loss: 1.120032\n",
      "Epoch: 382 \tTraining Loss: 0.248461 \tValidation Loss: 1.635587\n",
      "Epoch: 383 \tTraining Loss: 0.123970 \tValidation Loss: 1.184070\n",
      "Epoch: 384 \tTraining Loss: 0.196461 \tValidation Loss: 0.909032\n",
      "Epoch: 385 \tTraining Loss: 0.088802 \tValidation Loss: 0.832869\n",
      "Epoch: 386 \tTraining Loss: 0.259477 \tValidation Loss: 1.391576\n",
      "Epoch: 387 \tTraining Loss: 0.218462 \tValidation Loss: 1.486196\n",
      "Epoch: 388 \tTraining Loss: 0.149740 \tValidation Loss: 1.125030\n",
      "Epoch: 389 \tTraining Loss: 0.150529 \tValidation Loss: 1.162808\n",
      "Epoch: 390 \tTraining Loss: 0.132458 \tValidation Loss: 1.078884\n",
      "Epoch: 391 \tTraining Loss: 0.098134 \tValidation Loss: 0.916174\n",
      "Epoch: 392 \tTraining Loss: 0.082627 \tValidation Loss: 0.815518\n",
      "Epoch: 393 \tTraining Loss: 0.204928 \tValidation Loss: 2.074984\n",
      "Epoch: 394 \tTraining Loss: 0.189474 \tValidation Loss: 1.550710\n",
      "Epoch: 395 \tTraining Loss: 0.201393 \tValidation Loss: 1.167763\n",
      "Epoch: 396 \tTraining Loss: 0.347577 \tValidation Loss: 0.757558\n",
      "Epoch: 397 \tTraining Loss: 0.126444 \tValidation Loss: 1.710450\n",
      "Epoch: 398 \tTraining Loss: 0.170404 \tValidation Loss: 29.495192\n",
      "Epoch: 399 \tTraining Loss: 0.202103 \tValidation Loss: 0.844297\n",
      "Epoch: 400 \tTraining Loss: 0.159580 \tValidation Loss: 1.314276\n",
      "Epoch: 401 \tTraining Loss: 0.142127 \tValidation Loss: 3.602940\n",
      "Epoch: 402 \tTraining Loss: 0.332977 \tValidation Loss: 0.884524\n",
      "Epoch: 403 \tTraining Loss: 0.089488 \tValidation Loss: 0.837062\n",
      "Epoch: 404 \tTraining Loss: 0.088421 \tValidation Loss: 1.836751\n",
      "Epoch: 405 \tTraining Loss: 0.109773 \tValidation Loss: 1.314649\n",
      "Epoch: 406 \tTraining Loss: 0.120076 \tValidation Loss: 1.333321\n",
      "Epoch: 407 \tTraining Loss: 0.133911 \tValidation Loss: 2.613942\n",
      "Epoch: 408 \tTraining Loss: 0.183859 \tValidation Loss: 2.029609\n",
      "Epoch: 409 \tTraining Loss: 0.100816 \tValidation Loss: 1.416606\n",
      "Epoch: 410 \tTraining Loss: 0.132274 \tValidation Loss: 1.431086\n",
      "Epoch: 411 \tTraining Loss: 0.045570 \tValidation Loss: 1.037898\n",
      "Epoch: 412 \tTraining Loss: 0.060998 \tValidation Loss: 2.922828\n",
      "Epoch: 413 \tTraining Loss: 0.222967 \tValidation Loss: 0.646949\n",
      "Epoch: 414 \tTraining Loss: 0.134679 \tValidation Loss: 0.582970\n",
      "Epoch: 415 \tTraining Loss: 0.108350 \tValidation Loss: 1.027748\n",
      "Epoch: 416 \tTraining Loss: 0.073534 \tValidation Loss: 0.655237\n",
      "Epoch: 417 \tTraining Loss: 0.168958 \tValidation Loss: 0.872983\n",
      "Epoch: 418 \tTraining Loss: 0.218033 \tValidation Loss: 0.870509\n",
      "Epoch: 419 \tTraining Loss: 0.094138 \tValidation Loss: 0.765957\n",
      "Epoch: 420 \tTraining Loss: 0.088376 \tValidation Loss: 1.439079\n",
      "Epoch: 421 \tTraining Loss: 0.308619 \tValidation Loss: 3.927723\n",
      "Epoch: 422 \tTraining Loss: 0.232168 \tValidation Loss: 1.387595\n",
      "Epoch: 423 \tTraining Loss: 0.207013 \tValidation Loss: 2.008251\n",
      "Epoch: 424 \tTraining Loss: 0.135226 \tValidation Loss: 1.470221\n",
      "Epoch: 425 \tTraining Loss: 0.108291 \tValidation Loss: 1.204182\n",
      "Epoch: 426 \tTraining Loss: 0.098872 \tValidation Loss: 0.766080\n",
      "Epoch: 427 \tTraining Loss: 0.056692 \tValidation Loss: 1.332441\n",
      "Epoch: 428 \tTraining Loss: 0.031003 \tValidation Loss: 1.037407\n",
      "Epoch: 429 \tTraining Loss: 0.035135 \tValidation Loss: 0.925838\n",
      "Epoch: 430 \tTraining Loss: 0.159634 \tValidation Loss: 0.607228\n",
      "Epoch: 431 \tTraining Loss: 0.054241 \tValidation Loss: 0.813334\n",
      "Epoch: 432 \tTraining Loss: 0.122888 \tValidation Loss: 1.951515\n",
      "Epoch: 433 \tTraining Loss: 0.082729 \tValidation Loss: 2.656391\n",
      "Epoch: 434 \tTraining Loss: 0.121145 \tValidation Loss: 1.002553\n",
      "Epoch: 435 \tTraining Loss: 0.054670 \tValidation Loss: 2.287902\n",
      "Epoch: 436 \tTraining Loss: 0.107459 \tValidation Loss: 1.087986\n",
      "Epoch: 437 \tTraining Loss: 0.070626 \tValidation Loss: 1.121340\n",
      "Epoch: 438 \tTraining Loss: 0.055576 \tValidation Loss: 1.226253\n",
      "Epoch: 439 \tTraining Loss: 0.062799 \tValidation Loss: 0.893569\n",
      "Epoch: 440 \tTraining Loss: 0.197837 \tValidation Loss: 0.983826\n",
      "Epoch: 441 \tTraining Loss: 0.050781 \tValidation Loss: 1.235253\n",
      "Epoch: 442 \tTraining Loss: 0.185793 \tValidation Loss: 1.547949\n",
      "Epoch: 443 \tTraining Loss: 0.313834 \tValidation Loss: 2.126914\n",
      "Epoch: 444 \tTraining Loss: 0.237564 \tValidation Loss: 1.099951\n",
      "Epoch: 445 \tTraining Loss: 0.122597 \tValidation Loss: 0.867090\n",
      "Epoch: 446 \tTraining Loss: 0.039026 \tValidation Loss: 0.853111\n",
      "Epoch: 447 \tTraining Loss: 0.137820 \tValidation Loss: 1.351780\n",
      "Epoch: 448 \tTraining Loss: 0.082897 \tValidation Loss: 0.718698\n",
      "Epoch: 449 \tTraining Loss: 0.036640 \tValidation Loss: 0.625307\n",
      "Epoch: 450 \tTraining Loss: 0.115977 \tValidation Loss: 1.309637\n",
      "Epoch: 451 \tTraining Loss: 0.183613 \tValidation Loss: 1.615402\n",
      "Epoch: 452 \tTraining Loss: 0.124097 \tValidation Loss: 1.747121\n",
      "Epoch: 453 \tTraining Loss: 0.283572 \tValidation Loss: 1.375775\n",
      "Epoch: 454 \tTraining Loss: 0.240305 \tValidation Loss: 0.968379\n",
      "Epoch: 455 \tTraining Loss: 0.171620 \tValidation Loss: 0.708330\n",
      "Epoch: 456 \tTraining Loss: 0.093252 \tValidation Loss: 0.985113\n",
      "Epoch: 457 \tTraining Loss: 0.060291 \tValidation Loss: 0.894294\n",
      "Epoch: 458 \tTraining Loss: 0.013508 \tValidation Loss: 0.972400\n",
      "Epoch: 459 \tTraining Loss: 0.070701 \tValidation Loss: 0.884370\n",
      "Epoch: 460 \tTraining Loss: 0.038503 \tValidation Loss: 0.990065\n",
      "Epoch: 461 \tTraining Loss: 0.015932 \tValidation Loss: 0.998126\n",
      "Epoch: 462 \tTraining Loss: 0.064707 \tValidation Loss: 0.984839\n",
      "Epoch: 463 \tTraining Loss: 0.018001 \tValidation Loss: 0.980228\n",
      "Epoch: 464 \tTraining Loss: 0.077205 \tValidation Loss: 1.437963\n",
      "Epoch: 465 \tTraining Loss: 0.120056 \tValidation Loss: 1.831203\n",
      "Epoch: 466 \tTraining Loss: 0.122601 \tValidation Loss: 0.598425\n",
      "Epoch: 467 \tTraining Loss: 0.129832 \tValidation Loss: 0.638583\n",
      "Epoch: 468 \tTraining Loss: 0.149422 \tValidation Loss: 0.870795\n",
      "Epoch: 469 \tTraining Loss: 0.028136 \tValidation Loss: 0.913433\n",
      "Epoch: 470 \tTraining Loss: 0.036429 \tValidation Loss: 1.102762\n",
      "Epoch: 471 \tTraining Loss: 0.031405 \tValidation Loss: 1.112340\n",
      "Epoch: 472 \tTraining Loss: 0.042210 \tValidation Loss: 1.028494\n",
      "Epoch: 473 \tTraining Loss: 0.030721 \tValidation Loss: 0.906165\n",
      "Epoch: 474 \tTraining Loss: 0.050130 \tValidation Loss: 0.857881\n",
      "Epoch: 475 \tTraining Loss: 0.099404 \tValidation Loss: 0.721136\n",
      "Epoch: 476 \tTraining Loss: 0.145341 \tValidation Loss: 1.040772\n",
      "Epoch: 477 \tTraining Loss: 0.068615 \tValidation Loss: 1.239305\n",
      "Epoch: 478 \tTraining Loss: 0.034399 \tValidation Loss: 1.231079\n",
      "Epoch: 479 \tTraining Loss: 0.070631 \tValidation Loss: 2.845908\n",
      "Epoch: 480 \tTraining Loss: 0.270452 \tValidation Loss: 2.470978\n",
      "Epoch: 481 \tTraining Loss: 0.364359 \tValidation Loss: 3.978725\n",
      "Epoch: 482 \tTraining Loss: 0.625286 \tValidation Loss: 2.185737\n",
      "Epoch: 483 \tTraining Loss: 0.249554 \tValidation Loss: 1.199125\n",
      "Epoch: 484 \tTraining Loss: 0.176661 \tValidation Loss: 1.069256\n",
      "Epoch: 485 \tTraining Loss: 0.214776 \tValidation Loss: 0.958363\n",
      "Epoch: 486 \tTraining Loss: 0.213214 \tValidation Loss: 0.707518\n",
      "Epoch: 487 \tTraining Loss: 0.181202 \tValidation Loss: 0.777099\n",
      "Epoch: 488 \tTraining Loss: 0.111651 \tValidation Loss: 0.693035\n",
      "Epoch: 489 \tTraining Loss: 0.365278 \tValidation Loss: 1.060867\n",
      "Epoch: 490 \tTraining Loss: 0.238687 \tValidation Loss: 1.149306\n",
      "Epoch: 491 \tTraining Loss: 0.228024 \tValidation Loss: 0.928960\n",
      "Epoch: 492 \tTraining Loss: 0.164904 \tValidation Loss: 1.053283\n",
      "Epoch: 493 \tTraining Loss: 0.153548 \tValidation Loss: 0.684928\n",
      "Epoch: 494 \tTraining Loss: 0.284402 \tValidation Loss: 0.831562\n",
      "Epoch: 495 \tTraining Loss: 0.087863 \tValidation Loss: 0.880306\n",
      "Epoch: 496 \tTraining Loss: 0.180775 \tValidation Loss: 0.864598\n",
      "Epoch: 497 \tTraining Loss: 0.074066 \tValidation Loss: 0.763811\n",
      "Epoch: 498 \tTraining Loss: 0.213671 \tValidation Loss: 0.956127\n",
      "Epoch: 499 \tTraining Loss: 0.276963 \tValidation Loss: 0.911261\n",
      "Epoch: 500 \tTraining Loss: 0.214154 \tValidation Loss: 0.954634\n",
      "Epoch: 501 \tTraining Loss: 0.251584 \tValidation Loss: 0.717771\n",
      "Epoch: 502 \tTraining Loss: 0.177472 \tValidation Loss: 0.807388\n",
      "Epoch: 503 \tTraining Loss: 0.171564 \tValidation Loss: 1.509690\n",
      "Epoch: 504 \tTraining Loss: 0.056500 \tValidation Loss: 1.277463\n",
      "Epoch: 505 \tTraining Loss: 0.034861 \tValidation Loss: 1.141779\n",
      "Epoch: 506 \tTraining Loss: 0.057140 \tValidation Loss: 0.776161\n",
      "Epoch: 507 \tTraining Loss: 0.037258 \tValidation Loss: 0.781737\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 508 \tTraining Loss: 0.292137 \tValidation Loss: 0.818074\n",
      "Epoch: 509 \tTraining Loss: 0.159110 \tValidation Loss: 0.546175\n",
      "Epoch: 510 \tTraining Loss: 0.049467 \tValidation Loss: 1.020678\n",
      "Epoch: 511 \tTraining Loss: 0.048622 \tValidation Loss: 1.162481\n",
      "Epoch: 512 \tTraining Loss: 0.075926 \tValidation Loss: 1.210207\n",
      "Epoch: 513 \tTraining Loss: 0.150501 \tValidation Loss: 1.069312\n",
      "Epoch: 514 \tTraining Loss: 0.163307 \tValidation Loss: 0.778576\n",
      "Epoch: 515 \tTraining Loss: 0.055060 \tValidation Loss: 0.870551\n",
      "Epoch: 516 \tTraining Loss: 0.175387 \tValidation Loss: 2.323535\n",
      "Epoch: 517 \tTraining Loss: 0.262549 \tValidation Loss: 1.017840\n",
      "Epoch: 518 \tTraining Loss: 0.067814 \tValidation Loss: 0.980699\n",
      "Epoch: 519 \tTraining Loss: 0.053061 \tValidation Loss: 1.074512\n",
      "Epoch: 520 \tTraining Loss: 0.147077 \tValidation Loss: 8.594288\n",
      "Epoch: 521 \tTraining Loss: 0.122105 \tValidation Loss: 1.178915\n",
      "Epoch: 522 \tTraining Loss: 0.170337 \tValidation Loss: 1.369128\n",
      "Epoch: 523 \tTraining Loss: 0.111512 \tValidation Loss: 0.858807\n",
      "Epoch: 524 \tTraining Loss: 0.040586 \tValidation Loss: 0.954354\n",
      "Epoch: 525 \tTraining Loss: 0.065621 \tValidation Loss: 0.922669\n",
      "Epoch: 526 \tTraining Loss: 0.116182 \tValidation Loss: 0.892479\n",
      "Epoch: 527 \tTraining Loss: 0.074155 \tValidation Loss: 1.297465\n",
      "Epoch: 528 \tTraining Loss: 0.156509 \tValidation Loss: 1.582883\n",
      "Epoch: 529 \tTraining Loss: 0.086441 \tValidation Loss: 1.109182\n",
      "Epoch: 530 \tTraining Loss: 0.124289 \tValidation Loss: 1.329118\n",
      "Epoch: 531 \tTraining Loss: 0.033251 \tValidation Loss: 1.216394\n",
      "Epoch: 532 \tTraining Loss: 0.032705 \tValidation Loss: 1.027661\n",
      "Epoch: 533 \tTraining Loss: 0.020195 \tValidation Loss: 1.158901\n",
      "Epoch: 534 \tTraining Loss: 0.023069 \tValidation Loss: 1.457070\n",
      "Epoch: 535 \tTraining Loss: 0.012547 \tValidation Loss: 1.557852\n",
      "Epoch: 536 \tTraining Loss: 0.027355 \tValidation Loss: 0.959461\n",
      "Epoch: 537 \tTraining Loss: 0.027114 \tValidation Loss: 1.128771\n",
      "Epoch: 538 \tTraining Loss: 0.051581 \tValidation Loss: 1.466032\n",
      "Epoch: 539 \tTraining Loss: 0.053330 \tValidation Loss: 1.266270\n",
      "Epoch: 540 \tTraining Loss: 0.040290 \tValidation Loss: 1.312755\n",
      "Epoch: 541 \tTraining Loss: 0.143425 \tValidation Loss: 1.465020\n",
      "Epoch: 542 \tTraining Loss: 0.088471 \tValidation Loss: 1.213904\n",
      "Epoch: 543 \tTraining Loss: 0.047984 \tValidation Loss: 0.943226\n",
      "Epoch: 544 \tTraining Loss: 0.057396 \tValidation Loss: 0.782744\n",
      "Epoch: 545 \tTraining Loss: 0.081134 \tValidation Loss: 1.456079\n",
      "Epoch: 546 \tTraining Loss: 0.310534 \tValidation Loss: 1.221798\n",
      "Epoch: 547 \tTraining Loss: 0.242210 \tValidation Loss: 0.990796\n",
      "Epoch: 548 \tTraining Loss: 0.081339 \tValidation Loss: 0.933092\n",
      "Epoch: 549 \tTraining Loss: 0.124980 \tValidation Loss: 1.001134\n",
      "Epoch: 550 \tTraining Loss: 0.172756 \tValidation Loss: 1.194604\n",
      "Epoch: 551 \tTraining Loss: 0.096011 \tValidation Loss: 0.840517\n",
      "Epoch: 552 \tTraining Loss: 0.055013 \tValidation Loss: 0.788677\n",
      "Epoch: 553 \tTraining Loss: 0.171485 \tValidation Loss: 2.038118\n",
      "Epoch: 554 \tTraining Loss: 0.080082 \tValidation Loss: 0.968708\n",
      "Epoch: 555 \tTraining Loss: 0.089921 \tValidation Loss: 1.183333\n",
      "Epoch: 556 \tTraining Loss: 0.044366 \tValidation Loss: 1.208816\n",
      "Epoch: 557 \tTraining Loss: 0.046775 \tValidation Loss: 1.297549\n",
      "Epoch: 558 \tTraining Loss: 0.048732 \tValidation Loss: 1.624504\n",
      "Epoch: 559 \tTraining Loss: 0.032866 \tValidation Loss: 1.190303\n",
      "Epoch: 560 \tTraining Loss: 0.033115 \tValidation Loss: 0.772119\n",
      "Epoch: 561 \tTraining Loss: 0.155724 \tValidation Loss: 1.076770\n",
      "Epoch: 562 \tTraining Loss: 0.076300 \tValidation Loss: 1.263153\n",
      "Epoch: 563 \tTraining Loss: 0.345729 \tValidation Loss: 2.180799\n",
      "Epoch: 564 \tTraining Loss: 0.378456 \tValidation Loss: 1.504548\n",
      "Epoch: 565 \tTraining Loss: 0.145346 \tValidation Loss: 1.297181\n",
      "Epoch: 566 \tTraining Loss: 0.155652 \tValidation Loss: 3.281393\n",
      "Epoch: 567 \tTraining Loss: 0.197371 \tValidation Loss: 1.744812\n",
      "Epoch: 568 \tTraining Loss: 0.129285 \tValidation Loss: 0.760826\n",
      "Epoch: 569 \tTraining Loss: 0.124062 \tValidation Loss: 1.219078\n",
      "Epoch: 570 \tTraining Loss: 0.293116 \tValidation Loss: 2.245298\n",
      "Epoch: 571 \tTraining Loss: 0.320914 \tValidation Loss: 2.757617\n",
      "Epoch: 572 \tTraining Loss: 0.077154 \tValidation Loss: 1.607693\n",
      "Epoch: 573 \tTraining Loss: 0.141628 \tValidation Loss: 1.610231\n",
      "Epoch: 574 \tTraining Loss: 0.113303 \tValidation Loss: 1.475520\n",
      "Epoch: 575 \tTraining Loss: 0.062672 \tValidation Loss: 1.382071\n",
      "Epoch: 576 \tTraining Loss: 0.024976 \tValidation Loss: 1.155708\n",
      "Epoch: 577 \tTraining Loss: 0.040080 \tValidation Loss: 0.764025\n",
      "Epoch: 578 \tTraining Loss: 0.023511 \tValidation Loss: 1.092977\n",
      "Epoch: 579 \tTraining Loss: 0.043570 \tValidation Loss: 1.189742\n",
      "Epoch: 580 \tTraining Loss: 0.027874 \tValidation Loss: 1.169827\n",
      "Epoch: 581 \tTraining Loss: 0.059159 \tValidation Loss: 0.945172\n",
      "Epoch: 582 \tTraining Loss: 0.069808 \tValidation Loss: 1.923178\n",
      "Epoch: 583 \tTraining Loss: 0.024574 \tValidation Loss: 1.375454\n",
      "Epoch: 584 \tTraining Loss: 0.005991 \tValidation Loss: 1.772967\n",
      "Epoch: 585 \tTraining Loss: 0.018171 \tValidation Loss: 1.182043\n",
      "Epoch: 586 \tTraining Loss: 0.075207 \tValidation Loss: 0.926030\n",
      "Epoch: 587 \tTraining Loss: 0.182688 \tValidation Loss: 1.221483\n",
      "Epoch: 588 \tTraining Loss: 0.063675 \tValidation Loss: 1.169143\n",
      "Epoch: 589 \tTraining Loss: 0.052295 \tValidation Loss: 1.844163\n",
      "Epoch: 590 \tTraining Loss: 0.080105 \tValidation Loss: 0.804857\n",
      "Epoch: 591 \tTraining Loss: 0.018287 \tValidation Loss: 0.869356\n",
      "Epoch: 592 \tTraining Loss: 0.029974 \tValidation Loss: 0.788149\n",
      "Epoch: 593 \tTraining Loss: 0.085046 \tValidation Loss: 0.911857\n",
      "Epoch: 594 \tTraining Loss: 0.059894 \tValidation Loss: 1.287471\n",
      "Epoch: 595 \tTraining Loss: 0.029386 \tValidation Loss: 0.699808\n",
      "Epoch: 596 \tTraining Loss: 0.104583 \tValidation Loss: 1.217335\n",
      "Epoch: 597 \tTraining Loss: 0.051922 \tValidation Loss: 0.991326\n",
      "Epoch: 598 \tTraining Loss: 0.025421 \tValidation Loss: 1.161075\n",
      "Epoch: 599 \tTraining Loss: 0.027564 \tValidation Loss: 1.069553\n",
      "Epoch: 600 \tTraining Loss: 0.019189 \tValidation Loss: 1.493361\n",
      "Epoch: 601 \tTraining Loss: 0.011886 \tValidation Loss: 1.793867\n",
      "Epoch: 602 \tTraining Loss: 0.034915 \tValidation Loss: 1.778499\n",
      "Epoch: 603 \tTraining Loss: 0.007739 \tValidation Loss: 1.186404\n",
      "Epoch: 604 \tTraining Loss: 0.030075 \tValidation Loss: 1.372166\n",
      "Epoch: 605 \tTraining Loss: 0.330299 \tValidation Loss: 1.155507\n",
      "Epoch: 606 \tTraining Loss: 0.435150 \tValidation Loss: 0.920072\n",
      "Epoch: 607 \tTraining Loss: 0.336095 \tValidation Loss: 0.967744\n",
      "Epoch: 608 \tTraining Loss: 0.252293 \tValidation Loss: 1.201640\n",
      "Epoch: 609 \tTraining Loss: 0.270513 \tValidation Loss: 1.142619\n",
      "Epoch: 610 \tTraining Loss: 0.161081 \tValidation Loss: 1.173964\n",
      "Epoch: 611 \tTraining Loss: 0.149001 \tValidation Loss: 0.921544\n",
      "Epoch: 612 \tTraining Loss: 0.235661 \tValidation Loss: 0.929291\n",
      "Epoch: 613 \tTraining Loss: 0.100918 \tValidation Loss: 0.934108\n",
      "Epoch: 614 \tTraining Loss: 0.084445 \tValidation Loss: 0.817325\n",
      "Epoch: 615 \tTraining Loss: 0.106611 \tValidation Loss: 0.780220\n",
      "Epoch: 616 \tTraining Loss: 0.027411 \tValidation Loss: 1.174075\n",
      "Epoch: 617 \tTraining Loss: 0.041769 \tValidation Loss: 1.171763\n",
      "Epoch: 618 \tTraining Loss: 0.009254 \tValidation Loss: 1.311761\n",
      "Epoch: 619 \tTraining Loss: 0.156345 \tValidation Loss: 1.389285\n",
      "Epoch: 620 \tTraining Loss: 0.324576 \tValidation Loss: 1.061767\n",
      "Epoch: 621 \tTraining Loss: 0.305978 \tValidation Loss: 5.889311\n",
      "Epoch: 622 \tTraining Loss: 0.405324 \tValidation Loss: 0.672886\n",
      "Epoch: 623 \tTraining Loss: 0.132833 \tValidation Loss: 0.713806\n",
      "Epoch: 624 \tTraining Loss: 0.067963 \tValidation Loss: 0.834659\n",
      "Epoch: 625 \tTraining Loss: 0.168339 \tValidation Loss: 0.553130\n",
      "Epoch: 626 \tTraining Loss: 0.104704 \tValidation Loss: 0.862048\n",
      "Epoch: 627 \tTraining Loss: 0.069553 \tValidation Loss: 0.991855\n",
      "Epoch: 628 \tTraining Loss: 0.014012 \tValidation Loss: 1.151029\n",
      "Epoch: 629 \tTraining Loss: 0.038978 \tValidation Loss: 2.030885\n",
      "Epoch: 630 \tTraining Loss: 0.265202 \tValidation Loss: 1.415464\n",
      "Epoch: 631 \tTraining Loss: 0.319470 \tValidation Loss: 3.142767\n",
      "Epoch: 632 \tTraining Loss: 0.162086 \tValidation Loss: 1.106733\n",
      "Epoch: 633 \tTraining Loss: 0.169616 \tValidation Loss: 1.003098\n",
      "Epoch: 634 \tTraining Loss: 0.148961 \tValidation Loss: 0.873951\n",
      "Epoch: 635 \tTraining Loss: 0.153757 \tValidation Loss: 1.322117\n",
      "Epoch: 636 \tTraining Loss: 0.111735 \tValidation Loss: 0.874516\n",
      "Epoch: 637 \tTraining Loss: 0.082110 \tValidation Loss: 0.727713\n",
      "Epoch: 638 \tTraining Loss: 0.056317 \tValidation Loss: 1.051282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 639 \tTraining Loss: 0.087480 \tValidation Loss: 1.159373\n",
      "Epoch: 640 \tTraining Loss: 0.154524 \tValidation Loss: 0.885129\n",
      "Epoch: 641 \tTraining Loss: 0.034443 \tValidation Loss: 0.822838\n",
      "Epoch: 642 \tTraining Loss: 0.021933 \tValidation Loss: 0.752636\n",
      "Epoch: 643 \tTraining Loss: 0.020697 \tValidation Loss: 0.695475\n",
      "Epoch: 644 \tTraining Loss: 0.022421 \tValidation Loss: 0.740190\n",
      "Epoch: 645 \tTraining Loss: 0.014798 \tValidation Loss: 0.829657\n",
      "Epoch: 646 \tTraining Loss: 0.015030 \tValidation Loss: 0.900629\n",
      "Epoch: 647 \tTraining Loss: 0.012261 \tValidation Loss: 0.964263\n",
      "Epoch: 648 \tTraining Loss: 0.076032 \tValidation Loss: 1.411273\n",
      "Epoch: 649 \tTraining Loss: 0.260636 \tValidation Loss: 0.757656\n",
      "Epoch: 650 \tTraining Loss: 0.108371 \tValidation Loss: 0.851688\n",
      "Epoch: 651 \tTraining Loss: 0.140548 \tValidation Loss: 1.367595\n",
      "Epoch: 652 \tTraining Loss: 0.103010 \tValidation Loss: 0.811131\n",
      "Epoch: 653 \tTraining Loss: 0.132004 \tValidation Loss: 0.677031\n",
      "Epoch: 654 \tTraining Loss: 0.032739 \tValidation Loss: 1.000162\n",
      "Epoch: 655 \tTraining Loss: 0.029122 \tValidation Loss: 1.112822\n",
      "Epoch: 656 \tTraining Loss: 0.235564 \tValidation Loss: 1.326419\n",
      "Epoch: 657 \tTraining Loss: 0.233128 \tValidation Loss: 0.813044\n",
      "Epoch: 658 \tTraining Loss: 0.081500 \tValidation Loss: 0.756733\n",
      "Epoch: 659 \tTraining Loss: 0.112833 \tValidation Loss: 0.736330\n",
      "Epoch: 660 \tTraining Loss: 0.117693 \tValidation Loss: 0.741011\n",
      "Epoch: 661 \tTraining Loss: 0.040312 \tValidation Loss: 1.246561\n",
      "Epoch: 662 \tTraining Loss: 0.091818 \tValidation Loss: 1.049662\n",
      "Epoch: 663 \tTraining Loss: 0.054793 \tValidation Loss: 1.029384\n",
      "Epoch: 664 \tTraining Loss: 0.046221 \tValidation Loss: 1.037997\n",
      "Epoch: 665 \tTraining Loss: 0.065097 \tValidation Loss: 0.753559\n",
      "Epoch: 666 \tTraining Loss: 0.094958 \tValidation Loss: 1.144465\n",
      "Epoch: 667 \tTraining Loss: 0.056014 \tValidation Loss: 0.608862\n",
      "Epoch: 668 \tTraining Loss: 0.038603 \tValidation Loss: 0.605201\n",
      "Epoch: 669 \tTraining Loss: 0.073403 \tValidation Loss: 0.800698\n",
      "Epoch: 670 \tTraining Loss: 0.088958 \tValidation Loss: 1.201820\n",
      "Epoch: 671 \tTraining Loss: 0.026329 \tValidation Loss: 0.903391\n",
      "Epoch: 672 \tTraining Loss: 0.159383 \tValidation Loss: 1.220575\n",
      "Epoch: 673 \tTraining Loss: 0.424273 \tValidation Loss: 0.800732\n",
      "Epoch: 674 \tTraining Loss: 0.399696 \tValidation Loss: 0.858810\n",
      "Epoch: 675 \tTraining Loss: 0.255522 \tValidation Loss: 0.671038\n",
      "Epoch: 676 \tTraining Loss: 0.130663 \tValidation Loss: 0.708154\n",
      "Epoch: 677 \tTraining Loss: 0.054247 \tValidation Loss: 0.693727\n",
      "Epoch: 678 \tTraining Loss: 0.041003 \tValidation Loss: 0.888268\n",
      "Epoch: 679 \tTraining Loss: 0.201250 \tValidation Loss: 1.742492\n",
      "Epoch: 680 \tTraining Loss: 0.140698 \tValidation Loss: 1.142019\n",
      "Epoch: 681 \tTraining Loss: 0.206741 \tValidation Loss: 1.117884\n",
      "Epoch: 682 \tTraining Loss: 0.100728 \tValidation Loss: 1.137967\n",
      "Epoch: 683 \tTraining Loss: 0.253164 \tValidation Loss: 1.281197\n",
      "Epoch: 684 \tTraining Loss: 0.144515 \tValidation Loss: 0.759742\n",
      "Epoch: 685 \tTraining Loss: 0.053942 \tValidation Loss: 1.017207\n",
      "Epoch: 686 \tTraining Loss: 0.136219 \tValidation Loss: 1.376507\n",
      "Epoch: 687 \tTraining Loss: 0.156269 \tValidation Loss: 0.736919\n",
      "Epoch: 688 \tTraining Loss: 0.179844 \tValidation Loss: 1.278179\n",
      "Epoch: 689 \tTraining Loss: 0.073986 \tValidation Loss: 1.079784\n",
      "Epoch: 690 \tTraining Loss: 0.029071 \tValidation Loss: 1.241074\n",
      "Epoch: 691 \tTraining Loss: 0.187322 \tValidation Loss: 0.857324\n",
      "Epoch: 692 \tTraining Loss: 0.044343 \tValidation Loss: 1.324467\n",
      "Epoch: 693 \tTraining Loss: 0.064265 \tValidation Loss: 0.898297\n",
      "Epoch: 694 \tTraining Loss: 0.034052 \tValidation Loss: 0.930991\n",
      "Epoch: 695 \tTraining Loss: 0.028577 \tValidation Loss: 0.956811\n",
      "Epoch: 696 \tTraining Loss: 0.030601 \tValidation Loss: 1.277523\n",
      "Epoch: 697 \tTraining Loss: 0.022763 \tValidation Loss: 1.043215\n",
      "Epoch: 698 \tTraining Loss: 0.262217 \tValidation Loss: 4.287855\n",
      "Epoch: 699 \tTraining Loss: 0.082730 \tValidation Loss: 2.261529\n",
      "Epoch: 700 \tTraining Loss: 0.105845 \tValidation Loss: 2.073191\n",
      "Epoch: 701 \tTraining Loss: 0.116951 \tValidation Loss: 1.098531\n",
      "Epoch: 702 \tTraining Loss: 0.063003 \tValidation Loss: 0.910091\n",
      "Epoch: 703 \tTraining Loss: 0.030207 \tValidation Loss: 0.738990\n",
      "Epoch: 704 \tTraining Loss: 0.084403 \tValidation Loss: 1.057924\n",
      "Epoch: 705 \tTraining Loss: 0.103298 \tValidation Loss: 1.048841\n",
      "Epoch: 706 \tTraining Loss: 0.143083 \tValidation Loss: 2.275230\n",
      "Epoch: 707 \tTraining Loss: 0.103637 \tValidation Loss: 0.861477\n",
      "Epoch: 708 \tTraining Loss: 0.113601 \tValidation Loss: 1.128986\n",
      "Epoch: 709 \tTraining Loss: 0.022937 \tValidation Loss: 1.001164\n",
      "Epoch: 710 \tTraining Loss: 0.007982 \tValidation Loss: 1.068197\n",
      "Epoch: 711 \tTraining Loss: 0.004247 \tValidation Loss: 1.198078\n",
      "Epoch: 712 \tTraining Loss: 0.010697 \tValidation Loss: 1.324682\n",
      "Epoch: 713 \tTraining Loss: 0.037697 \tValidation Loss: 3.079688\n",
      "Epoch: 714 \tTraining Loss: 0.039473 \tValidation Loss: 1.590192\n",
      "Epoch: 715 \tTraining Loss: 0.053040 \tValidation Loss: 0.859210\n",
      "Epoch: 716 \tTraining Loss: 0.092890 \tValidation Loss: 1.029803\n",
      "Epoch: 717 \tTraining Loss: 0.040462 \tValidation Loss: 1.273268\n",
      "Epoch: 718 \tTraining Loss: 0.012899 \tValidation Loss: 1.314402\n",
      "Epoch: 719 \tTraining Loss: 0.011835 \tValidation Loss: 1.151225\n",
      "Epoch: 720 \tTraining Loss: 0.016296 \tValidation Loss: 1.250688\n",
      "Epoch: 721 \tTraining Loss: 0.007099 \tValidation Loss: 1.269577\n",
      "Epoch: 722 \tTraining Loss: 0.004997 \tValidation Loss: 1.549114\n",
      "Epoch: 723 \tTraining Loss: 0.005805 \tValidation Loss: 1.533512\n",
      "Epoch: 724 \tTraining Loss: 0.006864 \tValidation Loss: 1.696151\n",
      "Epoch: 725 \tTraining Loss: 0.003443 \tValidation Loss: 1.569351\n",
      "Epoch: 726 \tTraining Loss: 0.002509 \tValidation Loss: 1.588439\n",
      "Epoch: 727 \tTraining Loss: 0.115399 \tValidation Loss: 1.415632\n",
      "Epoch: 728 \tTraining Loss: 0.043731 \tValidation Loss: 1.640015\n",
      "Epoch: 729 \tTraining Loss: 0.038320 \tValidation Loss: 1.185646\n",
      "Epoch: 730 \tTraining Loss: 0.029154 \tValidation Loss: 1.314592\n",
      "Epoch: 731 \tTraining Loss: 0.022093 \tValidation Loss: 1.328382\n",
      "Epoch: 732 \tTraining Loss: 0.013843 \tValidation Loss: 1.443899\n",
      "Epoch: 733 \tTraining Loss: 0.028682 \tValidation Loss: 1.307474\n",
      "Epoch: 734 \tTraining Loss: 0.028986 \tValidation Loss: 1.699652\n",
      "Epoch: 735 \tTraining Loss: 0.012577 \tValidation Loss: 1.504150\n",
      "Epoch: 736 \tTraining Loss: 0.105544 \tValidation Loss: 0.887350\n",
      "Epoch: 737 \tTraining Loss: 0.058679 \tValidation Loss: 1.746305\n",
      "Epoch: 738 \tTraining Loss: 0.057342 \tValidation Loss: 1.067353\n",
      "Epoch: 739 \tTraining Loss: 0.017834 \tValidation Loss: 1.380149\n",
      "Epoch: 740 \tTraining Loss: 0.029442 \tValidation Loss: 1.057766\n",
      "Epoch: 741 \tTraining Loss: 0.019871 \tValidation Loss: 1.003900\n",
      "Epoch: 742 \tTraining Loss: 0.010809 \tValidation Loss: 1.101349\n",
      "Epoch: 743 \tTraining Loss: 0.003434 \tValidation Loss: 1.350566\n",
      "Epoch: 744 \tTraining Loss: 0.006893 \tValidation Loss: 1.283747\n",
      "Epoch: 745 \tTraining Loss: 0.019774 \tValidation Loss: 1.641499\n",
      "Epoch: 746 \tTraining Loss: 0.014663 \tValidation Loss: 1.713773\n",
      "Epoch: 747 \tTraining Loss: 0.109500 \tValidation Loss: 2.080879\n",
      "Epoch: 748 \tTraining Loss: 0.042244 \tValidation Loss: 1.026677\n",
      "Epoch: 749 \tTraining Loss: 0.016743 \tValidation Loss: 1.151129\n",
      "Epoch: 750 \tTraining Loss: 0.011564 \tValidation Loss: 1.327351\n",
      "Epoch: 751 \tTraining Loss: 0.009480 \tValidation Loss: 1.556227\n",
      "Epoch: 752 \tTraining Loss: 0.014101 \tValidation Loss: 1.245173\n",
      "Epoch: 753 \tTraining Loss: 0.012461 \tValidation Loss: 1.352282\n",
      "Epoch: 754 \tTraining Loss: 0.025785 \tValidation Loss: 1.375240\n",
      "Epoch: 755 \tTraining Loss: 0.044983 \tValidation Loss: 0.863301\n",
      "Epoch: 756 \tTraining Loss: 0.153227 \tValidation Loss: 1.134846\n",
      "Epoch: 757 \tTraining Loss: 0.062834 \tValidation Loss: 0.921134\n",
      "Epoch: 758 \tTraining Loss: 0.028517 \tValidation Loss: 1.519165\n",
      "Epoch: 759 \tTraining Loss: 0.017964 \tValidation Loss: 1.073064\n",
      "Epoch: 760 \tTraining Loss: 0.009893 \tValidation Loss: 0.959042\n",
      "Epoch: 761 \tTraining Loss: 0.022592 \tValidation Loss: 1.211310\n",
      "Epoch: 762 \tTraining Loss: 0.032845 \tValidation Loss: 1.162542\n",
      "Epoch: 763 \tTraining Loss: 0.029325 \tValidation Loss: 1.536967\n",
      "Epoch: 764 \tTraining Loss: 0.097760 \tValidation Loss: 0.830569\n",
      "Epoch: 765 \tTraining Loss: 0.197162 \tValidation Loss: 0.786465\n",
      "Epoch: 766 \tTraining Loss: 0.274799 \tValidation Loss: 3.960220\n",
      "Epoch: 767 \tTraining Loss: 0.239699 \tValidation Loss: 1.670767\n",
      "Epoch: 768 \tTraining Loss: 0.322916 \tValidation Loss: 1.004986\n",
      "Epoch: 769 \tTraining Loss: 0.253432 \tValidation Loss: 1.061425\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 770 \tTraining Loss: 0.197883 \tValidation Loss: 0.942722\n",
      "Epoch: 771 \tTraining Loss: 0.174968 \tValidation Loss: 1.041680\n",
      "Epoch: 772 \tTraining Loss: 0.184408 \tValidation Loss: 0.798637\n",
      "Epoch: 773 \tTraining Loss: 0.114863 \tValidation Loss: 0.796875\n",
      "Epoch: 774 \tTraining Loss: 0.067996 \tValidation Loss: 0.875958\n",
      "Epoch: 775 \tTraining Loss: 0.050791 \tValidation Loss: 0.986225\n",
      "Epoch: 776 \tTraining Loss: 0.112806 \tValidation Loss: 0.770631\n",
      "Epoch: 777 \tTraining Loss: 0.050674 \tValidation Loss: 0.854980\n",
      "Epoch: 778 \tTraining Loss: 0.037101 \tValidation Loss: 0.849629\n",
      "Epoch: 779 \tTraining Loss: 0.030885 \tValidation Loss: 0.594601\n",
      "Epoch: 780 \tTraining Loss: 0.019475 \tValidation Loss: 0.672033\n",
      "Epoch: 781 \tTraining Loss: 0.124801 \tValidation Loss: 0.478779\n",
      "Validation loss decreased (0.514153 --> 0.478779).  Saving model ...\n",
      "Epoch: 782 \tTraining Loss: 0.647870 \tValidation Loss: 2.330870\n",
      "Epoch: 783 \tTraining Loss: 0.272681 \tValidation Loss: 1.021559\n",
      "Epoch: 784 \tTraining Loss: 0.316160 \tValidation Loss: 0.609881\n",
      "Epoch: 785 \tTraining Loss: 0.264761 \tValidation Loss: 0.773861\n",
      "Epoch: 786 \tTraining Loss: 0.197237 \tValidation Loss: 0.593095\n",
      "Epoch: 787 \tTraining Loss: 0.139072 \tValidation Loss: 0.648566\n",
      "Epoch: 788 \tTraining Loss: 0.148910 \tValidation Loss: 0.717796\n",
      "Epoch: 789 \tTraining Loss: 0.251712 \tValidation Loss: 0.485508\n",
      "Epoch: 790 \tTraining Loss: 0.110870 \tValidation Loss: 0.530905\n",
      "Epoch: 791 \tTraining Loss: 0.070970 \tValidation Loss: 0.689161\n",
      "Epoch: 792 \tTraining Loss: 0.044825 \tValidation Loss: 0.791534\n",
      "Epoch: 793 \tTraining Loss: 0.104951 \tValidation Loss: 0.907154\n",
      "Epoch: 794 \tTraining Loss: 0.153368 \tValidation Loss: 0.804268\n",
      "Epoch: 795 \tTraining Loss: 0.074886 \tValidation Loss: 0.740674\n",
      "Epoch: 796 \tTraining Loss: 0.072520 \tValidation Loss: 1.201231\n",
      "Epoch: 797 \tTraining Loss: 0.047393 \tValidation Loss: 1.361404\n",
      "Epoch: 798 \tTraining Loss: 0.045992 \tValidation Loss: 1.012190\n",
      "Epoch: 799 \tTraining Loss: 0.035142 \tValidation Loss: 0.871767\n",
      "Epoch: 800 \tTraining Loss: 0.014003 \tValidation Loss: 0.715207\n",
      "Epoch: 801 \tTraining Loss: 0.009259 \tValidation Loss: 0.814869\n",
      "Epoch: 802 \tTraining Loss: 0.006635 \tValidation Loss: 0.875599\n",
      "Epoch: 803 \tTraining Loss: 0.021716 \tValidation Loss: 1.194186\n",
      "Epoch: 804 \tTraining Loss: 0.011611 \tValidation Loss: 0.921307\n",
      "Epoch: 805 \tTraining Loss: 0.005952 \tValidation Loss: 0.987697\n",
      "Epoch: 806 \tTraining Loss: 0.005793 \tValidation Loss: 0.885789\n",
      "Epoch: 807 \tTraining Loss: 0.002343 \tValidation Loss: 0.929051\n",
      "Epoch: 808 \tTraining Loss: 0.034356 \tValidation Loss: 0.636057\n",
      "Epoch: 809 \tTraining Loss: 0.124999 \tValidation Loss: 0.610945\n",
      "Epoch: 810 \tTraining Loss: 0.040584 \tValidation Loss: 0.776732\n",
      "Epoch: 811 \tTraining Loss: 0.061740 \tValidation Loss: 0.891269\n",
      "Epoch: 812 \tTraining Loss: 0.066422 \tValidation Loss: 1.090358\n",
      "Epoch: 813 \tTraining Loss: 0.060610 \tValidation Loss: 0.845396\n",
      "Epoch: 814 \tTraining Loss: 0.033787 \tValidation Loss: 0.838353\n",
      "Epoch: 815 \tTraining Loss: 0.015285 \tValidation Loss: 0.918126\n",
      "Epoch: 816 \tTraining Loss: 0.007994 \tValidation Loss: 1.144847\n",
      "Epoch: 817 \tTraining Loss: 0.029437 \tValidation Loss: 1.117280\n",
      "Epoch: 818 \tTraining Loss: 0.014801 \tValidation Loss: 1.052789\n",
      "Epoch: 819 \tTraining Loss: 0.160666 \tValidation Loss: 1.145469\n",
      "Epoch: 820 \tTraining Loss: 0.116828 \tValidation Loss: 0.817408\n",
      "Epoch: 821 \tTraining Loss: 0.065931 \tValidation Loss: 1.849606\n",
      "Epoch: 822 \tTraining Loss: 0.071256 \tValidation Loss: 0.950138\n",
      "Epoch: 823 \tTraining Loss: 0.030929 \tValidation Loss: 1.132302\n",
      "Epoch: 824 \tTraining Loss: 0.093438 \tValidation Loss: 0.569672\n",
      "Epoch: 825 \tTraining Loss: 0.094791 \tValidation Loss: 0.796518\n",
      "Epoch: 826 \tTraining Loss: 0.041853 \tValidation Loss: 0.974598\n",
      "Epoch: 827 \tTraining Loss: 0.026184 \tValidation Loss: 1.171825\n",
      "Epoch: 828 \tTraining Loss: 0.026092 \tValidation Loss: 1.283738\n",
      "Epoch: 829 \tTraining Loss: 0.043142 \tValidation Loss: 1.296154\n",
      "Epoch: 830 \tTraining Loss: 0.043515 \tValidation Loss: 1.348438\n",
      "Epoch: 831 \tTraining Loss: 0.170415 \tValidation Loss: 1.151230\n",
      "Epoch: 832 \tTraining Loss: 0.098610 \tValidation Loss: 1.178985\n",
      "Epoch: 833 \tTraining Loss: 0.243794 \tValidation Loss: 1.951981\n",
      "Epoch: 834 \tTraining Loss: 0.072368 \tValidation Loss: 1.054646\n",
      "Epoch: 835 \tTraining Loss: 0.157526 \tValidation Loss: 1.085970\n",
      "Epoch: 836 \tTraining Loss: 0.067608 \tValidation Loss: 1.278413\n",
      "Epoch: 837 \tTraining Loss: 0.081000 \tValidation Loss: 0.831072\n",
      "Epoch: 838 \tTraining Loss: 0.065305 \tValidation Loss: 1.219667\n",
      "Epoch: 839 \tTraining Loss: 0.076288 \tValidation Loss: 0.797336\n",
      "Epoch: 840 \tTraining Loss: 0.054253 \tValidation Loss: 0.843867\n",
      "Epoch: 841 \tTraining Loss: 0.078136 \tValidation Loss: 0.811036\n",
      "Epoch: 842 \tTraining Loss: 0.057512 \tValidation Loss: 0.877591\n",
      "Epoch: 843 \tTraining Loss: 0.026467 \tValidation Loss: 1.200978\n",
      "Epoch: 844 \tTraining Loss: 0.040466 \tValidation Loss: 1.300930\n",
      "Epoch: 845 \tTraining Loss: 0.081525 \tValidation Loss: 1.045164\n",
      "Epoch: 846 \tTraining Loss: 0.033687 \tValidation Loss: 0.944793\n",
      "Epoch: 847 \tTraining Loss: 0.030681 \tValidation Loss: 0.984925\n",
      "Epoch: 848 \tTraining Loss: 0.016111 \tValidation Loss: 1.067403\n",
      "Epoch: 849 \tTraining Loss: 0.006434 \tValidation Loss: 1.062863\n",
      "Epoch: 850 \tTraining Loss: 0.006240 \tValidation Loss: 1.108643\n",
      "Epoch: 851 \tTraining Loss: 0.005124 \tValidation Loss: 1.090671\n",
      "Epoch: 852 \tTraining Loss: 0.003832 \tValidation Loss: 1.130080\n",
      "Epoch: 853 \tTraining Loss: 0.003684 \tValidation Loss: 1.080824\n",
      "Epoch: 854 \tTraining Loss: 0.004871 \tValidation Loss: 1.088226\n",
      "Epoch: 855 \tTraining Loss: 0.003317 \tValidation Loss: 1.072346\n",
      "Epoch: 856 \tTraining Loss: 0.010852 \tValidation Loss: 1.141827\n",
      "Epoch: 857 \tTraining Loss: 0.017479 \tValidation Loss: 1.047413\n",
      "Epoch: 858 \tTraining Loss: 0.007304 \tValidation Loss: 1.156989\n",
      "Epoch: 859 \tTraining Loss: 0.005175 \tValidation Loss: 1.251880\n",
      "Epoch: 860 \tTraining Loss: 0.008542 \tValidation Loss: 1.171044\n",
      "Epoch: 861 \tTraining Loss: 0.012073 \tValidation Loss: 0.913546\n",
      "Epoch: 862 \tTraining Loss: 0.023353 \tValidation Loss: 1.193038\n",
      "Epoch: 863 \tTraining Loss: 0.013814 \tValidation Loss: 1.064267\n",
      "Epoch: 864 \tTraining Loss: 0.004992 \tValidation Loss: 1.124179\n",
      "Epoch: 865 \tTraining Loss: 0.002963 \tValidation Loss: 1.208634\n",
      "Epoch: 866 \tTraining Loss: 0.002948 \tValidation Loss: 1.197234\n",
      "Epoch: 867 \tTraining Loss: 0.002482 \tValidation Loss: 1.293675\n",
      "Epoch: 868 \tTraining Loss: 0.003537 \tValidation Loss: 1.271812\n",
      "Epoch: 869 \tTraining Loss: 0.006073 \tValidation Loss: 1.153015\n",
      "Epoch: 870 \tTraining Loss: 0.003750 \tValidation Loss: 1.365590\n",
      "Epoch: 871 \tTraining Loss: 0.006933 \tValidation Loss: 1.139814\n",
      "Epoch: 872 \tTraining Loss: 0.005413 \tValidation Loss: 1.159870\n",
      "Epoch: 873 \tTraining Loss: 0.001304 \tValidation Loss: 1.148608\n",
      "Epoch: 874 \tTraining Loss: 0.000979 \tValidation Loss: 1.210319\n",
      "Epoch: 875 \tTraining Loss: 0.005819 \tValidation Loss: 1.379361\n",
      "Epoch: 876 \tTraining Loss: 0.002966 \tValidation Loss: 1.375826\n",
      "Epoch: 877 \tTraining Loss: 0.012865 \tValidation Loss: 1.281409\n",
      "Epoch: 878 \tTraining Loss: 0.004420 \tValidation Loss: 1.078645\n",
      "Epoch: 879 \tTraining Loss: 0.013739 \tValidation Loss: 2.127676\n",
      "Epoch: 880 \tTraining Loss: 0.048195 \tValidation Loss: 1.633568\n",
      "Epoch: 881 \tTraining Loss: 0.061813 \tValidation Loss: 1.015530\n",
      "Epoch: 882 \tTraining Loss: 0.046506 \tValidation Loss: 0.689379\n",
      "Epoch: 883 \tTraining Loss: 0.092512 \tValidation Loss: 0.788916\n",
      "Epoch: 884 \tTraining Loss: 0.017703 \tValidation Loss: 0.826385\n",
      "Epoch: 885 \tTraining Loss: 0.021056 \tValidation Loss: 1.064767\n",
      "Epoch: 886 \tTraining Loss: 0.006815 \tValidation Loss: 1.120703\n",
      "Epoch: 887 \tTraining Loss: 0.004077 \tValidation Loss: 1.079073\n",
      "Epoch: 888 \tTraining Loss: 0.002674 \tValidation Loss: 0.982283\n",
      "Epoch: 889 \tTraining Loss: 0.002974 \tValidation Loss: 0.945699\n",
      "Epoch: 890 \tTraining Loss: 0.004006 \tValidation Loss: 0.971114\n",
      "Epoch: 891 \tTraining Loss: 0.018286 \tValidation Loss: 1.236608\n",
      "Epoch: 892 \tTraining Loss: 0.031884 \tValidation Loss: 1.783790\n",
      "Epoch: 893 \tTraining Loss: 0.058121 \tValidation Loss: 1.206629\n",
      "Epoch: 894 \tTraining Loss: 0.058965 \tValidation Loss: 2.549350\n",
      "Epoch: 895 \tTraining Loss: 0.086540 \tValidation Loss: 3.585260\n",
      "Epoch: 896 \tTraining Loss: 0.139472 \tValidation Loss: 2.432451\n",
      "Epoch: 897 \tTraining Loss: 0.171194 \tValidation Loss: 0.937277\n",
      "Epoch: 898 \tTraining Loss: 0.183639 \tValidation Loss: 2.474734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 899 \tTraining Loss: 0.556309 \tValidation Loss: 0.928166\n",
      "Epoch: 900 \tTraining Loss: 0.184804 \tValidation Loss: 1.273255\n",
      "Epoch: 901 \tTraining Loss: 0.176815 \tValidation Loss: 0.789853\n",
      "Epoch: 902 \tTraining Loss: 0.094616 \tValidation Loss: 1.321502\n",
      "Epoch: 903 \tTraining Loss: 0.054661 \tValidation Loss: 1.211729\n",
      "Epoch: 904 \tTraining Loss: 0.054412 \tValidation Loss: 1.138685\n",
      "Epoch: 905 \tTraining Loss: 0.029314 \tValidation Loss: 0.803909\n",
      "Epoch: 906 \tTraining Loss: 0.065739 \tValidation Loss: 0.683871\n",
      "Epoch: 907 \tTraining Loss: 0.042489 \tValidation Loss: 0.629575\n",
      "Epoch: 908 \tTraining Loss: 0.139562 \tValidation Loss: 1.460459\n",
      "Epoch: 909 \tTraining Loss: 0.044560 \tValidation Loss: 1.055997\n",
      "Epoch: 910 \tTraining Loss: 0.137313 \tValidation Loss: 1.822733\n",
      "Epoch: 911 \tTraining Loss: 0.078767 \tValidation Loss: 0.587551\n",
      "Epoch: 912 \tTraining Loss: 0.022523 \tValidation Loss: 0.728309\n",
      "Epoch: 913 \tTraining Loss: 0.113768 \tValidation Loss: 1.089153\n",
      "Epoch: 914 \tTraining Loss: 0.047853 \tValidation Loss: 0.766234\n",
      "Epoch: 915 \tTraining Loss: 0.075077 \tValidation Loss: 0.762510\n",
      "Epoch: 916 \tTraining Loss: 0.038555 \tValidation Loss: 0.962659\n",
      "Epoch: 917 \tTraining Loss: 0.023648 \tValidation Loss: 1.005107\n",
      "Epoch: 918 \tTraining Loss: 0.026073 \tValidation Loss: 1.088120\n",
      "Epoch: 919 \tTraining Loss: 0.020441 \tValidation Loss: 1.144648\n",
      "Epoch: 920 \tTraining Loss: 0.012960 \tValidation Loss: 1.233913\n",
      "Epoch: 921 \tTraining Loss: 0.010381 \tValidation Loss: 1.357633\n",
      "Epoch: 922 \tTraining Loss: 0.013578 \tValidation Loss: 1.250162\n",
      "Epoch: 923 \tTraining Loss: 0.012339 \tValidation Loss: 1.186376\n",
      "Epoch: 924 \tTraining Loss: 0.006436 \tValidation Loss: 1.222401\n",
      "Epoch: 925 \tTraining Loss: 0.004359 \tValidation Loss: 1.309953\n",
      "Epoch: 926 \tTraining Loss: 0.005726 \tValidation Loss: 1.182830\n",
      "Epoch: 927 \tTraining Loss: 0.006456 \tValidation Loss: 1.196818\n",
      "Epoch: 928 \tTraining Loss: 0.014169 \tValidation Loss: 1.323237\n",
      "Epoch: 929 \tTraining Loss: 0.021067 \tValidation Loss: 0.779106\n",
      "Epoch: 930 \tTraining Loss: 0.016410 \tValidation Loss: 1.396594\n",
      "Epoch: 931 \tTraining Loss: 0.042108 \tValidation Loss: 0.933651\n",
      "Epoch: 932 \tTraining Loss: 0.183396 \tValidation Loss: 1.583912\n",
      "Epoch: 933 \tTraining Loss: 0.231735 \tValidation Loss: 0.661226\n",
      "Epoch: 934 \tTraining Loss: 0.130722 \tValidation Loss: 0.774854\n",
      "Epoch: 935 \tTraining Loss: 0.102224 \tValidation Loss: 0.935843\n",
      "Epoch: 936 \tTraining Loss: 0.038148 \tValidation Loss: 0.911611\n",
      "Epoch: 937 \tTraining Loss: 0.081902 \tValidation Loss: 1.118811\n",
      "Epoch: 938 \tTraining Loss: 0.035746 \tValidation Loss: 0.749588\n",
      "Epoch: 939 \tTraining Loss: 0.013021 \tValidation Loss: 0.811881\n",
      "Epoch: 940 \tTraining Loss: 0.036969 \tValidation Loss: 1.451180\n",
      "Epoch: 941 \tTraining Loss: 0.028578 \tValidation Loss: 1.387245\n",
      "Epoch: 942 \tTraining Loss: 0.018278 \tValidation Loss: 1.506556\n",
      "Epoch: 943 \tTraining Loss: 0.010215 \tValidation Loss: 1.734070\n",
      "Epoch: 944 \tTraining Loss: 0.018994 \tValidation Loss: 1.931430\n",
      "Epoch: 945 \tTraining Loss: 0.043197 \tValidation Loss: 0.910792\n",
      "Epoch: 946 \tTraining Loss: 0.116561 \tValidation Loss: 1.114128\n",
      "Epoch: 947 \tTraining Loss: 0.049556 \tValidation Loss: 1.511004\n",
      "Epoch: 948 \tTraining Loss: 0.070938 \tValidation Loss: 2.343941\n",
      "Epoch: 949 \tTraining Loss: 0.058706 \tValidation Loss: 1.764900\n",
      "Epoch: 950 \tTraining Loss: 0.013028 \tValidation Loss: 1.577962\n",
      "Epoch: 951 \tTraining Loss: 0.011466 \tValidation Loss: 1.584937\n",
      "Epoch: 952 \tTraining Loss: 0.006176 \tValidation Loss: 1.441327\n",
      "Epoch: 953 \tTraining Loss: 0.004327 \tValidation Loss: 1.478388\n",
      "Epoch: 954 \tTraining Loss: 0.005994 \tValidation Loss: 1.343224\n",
      "Epoch: 955 \tTraining Loss: 0.002284 \tValidation Loss: 1.297366\n",
      "Epoch: 956 \tTraining Loss: 0.003871 \tValidation Loss: 1.374928\n",
      "Epoch: 957 \tTraining Loss: 0.003403 \tValidation Loss: 1.399309\n",
      "Epoch: 958 \tTraining Loss: 0.002703 \tValidation Loss: 1.414111\n",
      "Epoch: 959 \tTraining Loss: 0.004647 \tValidation Loss: 1.499755\n",
      "Epoch: 960 \tTraining Loss: 0.004705 \tValidation Loss: 1.473588\n",
      "Epoch: 961 \tTraining Loss: 0.003025 \tValidation Loss: 1.538334\n",
      "Epoch: 962 \tTraining Loss: 0.002517 \tValidation Loss: 1.520262\n",
      "Epoch: 963 \tTraining Loss: 0.002552 \tValidation Loss: 1.409422\n",
      "Epoch: 964 \tTraining Loss: 0.001907 \tValidation Loss: 1.449329\n",
      "Epoch: 965 \tTraining Loss: 0.012809 \tValidation Loss: 1.219016\n",
      "Epoch: 966 \tTraining Loss: 0.020529 \tValidation Loss: 1.220604\n",
      "Epoch: 967 \tTraining Loss: 0.005029 \tValidation Loss: 1.194646\n",
      "Epoch: 968 \tTraining Loss: 0.001724 \tValidation Loss: 1.578245\n",
      "Epoch: 969 \tTraining Loss: 0.003008 \tValidation Loss: 1.767883\n",
      "Epoch: 970 \tTraining Loss: 0.002196 \tValidation Loss: 1.905251\n",
      "Epoch: 971 \tTraining Loss: 0.003328 \tValidation Loss: 1.651042\n",
      "Epoch: 972 \tTraining Loss: 0.000929 \tValidation Loss: 1.688402\n",
      "Epoch: 973 \tTraining Loss: 0.006002 \tValidation Loss: 1.783382\n",
      "Epoch: 974 \tTraining Loss: 0.058482 \tValidation Loss: 2.161001\n",
      "Epoch: 975 \tTraining Loss: 0.219176 \tValidation Loss: 1.846261\n",
      "Epoch: 976 \tTraining Loss: 0.199102 \tValidation Loss: 0.953920\n",
      "Epoch: 977 \tTraining Loss: 0.096997 \tValidation Loss: 1.490752\n",
      "Epoch: 978 \tTraining Loss: 0.067954 \tValidation Loss: 0.914028\n",
      "Epoch: 979 \tTraining Loss: 0.083731 \tValidation Loss: 1.114205\n",
      "Epoch: 980 \tTraining Loss: 0.143831 \tValidation Loss: 1.082280\n",
      "Epoch: 981 \tTraining Loss: 0.117983 \tValidation Loss: 1.321944\n",
      "Epoch: 982 \tTraining Loss: 0.137113 \tValidation Loss: 1.335808\n",
      "Epoch: 983 \tTraining Loss: 0.064789 \tValidation Loss: 0.737753\n",
      "Epoch: 984 \tTraining Loss: 0.033992 \tValidation Loss: 0.980694\n",
      "Epoch: 985 \tTraining Loss: 0.019238 \tValidation Loss: 1.303281\n",
      "Epoch: 986 \tTraining Loss: 0.040174 \tValidation Loss: 1.317028\n",
      "Epoch: 987 \tTraining Loss: 0.030930 \tValidation Loss: 1.221870\n",
      "Epoch: 988 \tTraining Loss: 0.019069 \tValidation Loss: 1.084160\n",
      "Epoch: 989 \tTraining Loss: 0.009811 \tValidation Loss: 1.242583\n",
      "Epoch: 990 \tTraining Loss: 0.008418 \tValidation Loss: 1.283372\n",
      "Epoch: 991 \tTraining Loss: 0.005426 \tValidation Loss: 1.279141\n",
      "Epoch: 992 \tTraining Loss: 0.012270 \tValidation Loss: 1.317571\n",
      "Epoch: 993 \tTraining Loss: 0.029202 \tValidation Loss: 1.453565\n",
      "Epoch: 994 \tTraining Loss: 0.070895 \tValidation Loss: 2.118196\n",
      "Epoch: 995 \tTraining Loss: 0.116783 \tValidation Loss: 2.421943\n",
      "Epoch: 996 \tTraining Loss: 0.029558 \tValidation Loss: 1.151878\n",
      "Epoch: 997 \tTraining Loss: 0.023240 \tValidation Loss: 1.132177\n",
      "Epoch: 998 \tTraining Loss: 0.015551 \tValidation Loss: 1.432551\n",
      "Epoch: 999 \tTraining Loss: 0.039882 \tValidation Loss: 0.828222\n",
      "Epoch: 1000 \tTraining Loss: 0.192995 \tValidation Loss: 1.161168\n",
      "Epoch: 1001 \tTraining Loss: 0.110030 \tValidation Loss: 1.346205\n",
      "Epoch: 1002 \tTraining Loss: 0.055002 \tValidation Loss: 1.047353\n",
      "Epoch: 1003 \tTraining Loss: 0.026359 \tValidation Loss: 1.303193\n",
      "Epoch: 1004 \tTraining Loss: 0.027900 \tValidation Loss: 1.314554\n",
      "Epoch: 1005 \tTraining Loss: 0.012597 \tValidation Loss: 0.896594\n",
      "Epoch: 1006 \tTraining Loss: 0.007824 \tValidation Loss: 1.105547\n",
      "Epoch: 1007 \tTraining Loss: 0.004664 \tValidation Loss: 1.158529\n",
      "Epoch: 1008 \tTraining Loss: 0.004222 \tValidation Loss: 1.043371\n",
      "Epoch: 1009 \tTraining Loss: 0.007994 \tValidation Loss: 1.264453\n",
      "Epoch: 1010 \tTraining Loss: 0.006689 \tValidation Loss: 1.242142\n",
      "Epoch: 1011 \tTraining Loss: 0.004101 \tValidation Loss: 1.254000\n",
      "Epoch: 1012 \tTraining Loss: 0.002660 \tValidation Loss: 1.168161\n",
      "Epoch: 1013 \tTraining Loss: 0.005068 \tValidation Loss: 1.326295\n",
      "Epoch: 1014 \tTraining Loss: 0.004301 \tValidation Loss: 1.185431\n",
      "Epoch: 1015 \tTraining Loss: 0.014707 \tValidation Loss: 1.426639\n",
      "Epoch: 1016 \tTraining Loss: 0.019709 \tValidation Loss: 0.914558\n",
      "Epoch: 1017 \tTraining Loss: 0.036240 \tValidation Loss: 1.076968\n",
      "Epoch: 1018 \tTraining Loss: 0.008132 \tValidation Loss: 1.246040\n",
      "Epoch: 1019 \tTraining Loss: 0.002461 \tValidation Loss: 1.401782\n",
      "Epoch: 1020 \tTraining Loss: 0.006077 \tValidation Loss: 1.490955\n",
      "Epoch: 1021 \tTraining Loss: 0.025177 \tValidation Loss: 0.580666\n",
      "Epoch: 1022 \tTraining Loss: 0.021200 \tValidation Loss: 1.120941\n",
      "Epoch: 1023 \tTraining Loss: 0.003492 \tValidation Loss: 1.057543\n",
      "Epoch: 1024 \tTraining Loss: 0.007602 \tValidation Loss: 1.058855\n",
      "Epoch: 1025 \tTraining Loss: 0.010526 \tValidation Loss: 1.184528\n",
      "Epoch: 1026 \tTraining Loss: 0.124263 \tValidation Loss: 1.134882\n",
      "Epoch: 1027 \tTraining Loss: 0.100860 \tValidation Loss: 1.519877\n",
      "Epoch: 1028 \tTraining Loss: 0.090336 \tValidation Loss: 1.028899\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1029 \tTraining Loss: 0.039140 \tValidation Loss: 0.798143\n",
      "Epoch: 1030 \tTraining Loss: 0.144088 \tValidation Loss: 0.993112\n",
      "Epoch: 1031 \tTraining Loss: 0.046341 \tValidation Loss: 1.327663\n",
      "Epoch: 1032 \tTraining Loss: 0.061796 \tValidation Loss: 0.938200\n",
      "Epoch: 1033 \tTraining Loss: 0.257303 \tValidation Loss: 1.594387\n",
      "Epoch: 1034 \tTraining Loss: 0.398533 \tValidation Loss: 5.426483\n",
      "Epoch: 1035 \tTraining Loss: 0.280915 \tValidation Loss: 1.009381\n",
      "Epoch: 1036 \tTraining Loss: 0.205596 \tValidation Loss: 0.772114\n",
      "Epoch: 1037 \tTraining Loss: 0.163276 \tValidation Loss: 0.841038\n",
      "Epoch: 1038 \tTraining Loss: 0.069014 \tValidation Loss: 0.976431\n",
      "Epoch: 1039 \tTraining Loss: 0.050204 \tValidation Loss: 1.078345\n",
      "Epoch: 1040 \tTraining Loss: 0.029915 \tValidation Loss: 1.063670\n",
      "Epoch: 1041 \tTraining Loss: 0.027248 \tValidation Loss: 1.354824\n",
      "Epoch: 1042 \tTraining Loss: 0.040470 \tValidation Loss: 0.856896\n",
      "Epoch: 1043 \tTraining Loss: 0.019971 \tValidation Loss: 0.669127\n",
      "Epoch: 1044 \tTraining Loss: 0.038267 \tValidation Loss: 0.960157\n",
      "Epoch: 1045 \tTraining Loss: 0.055699 \tValidation Loss: 1.532557\n",
      "Epoch: 1046 \tTraining Loss: 0.196112 \tValidation Loss: 0.969283\n",
      "Epoch: 1047 \tTraining Loss: 0.157717 \tValidation Loss: 2.114488\n",
      "Epoch: 1048 \tTraining Loss: 0.084913 \tValidation Loss: 1.000118\n",
      "Epoch: 1049 \tTraining Loss: 0.028737 \tValidation Loss: 0.628532\n",
      "Epoch: 1050 \tTraining Loss: 0.110101 \tValidation Loss: 0.694551\n",
      "Epoch: 1051 \tTraining Loss: 0.047255 \tValidation Loss: 0.932698\n",
      "Epoch: 1052 \tTraining Loss: 0.048572 \tValidation Loss: 1.037805\n",
      "Epoch: 1053 \tTraining Loss: 0.018167 \tValidation Loss: 1.123943\n",
      "Epoch: 1054 \tTraining Loss: 0.012934 \tValidation Loss: 1.177542\n",
      "Epoch: 1055 \tTraining Loss: 0.009670 \tValidation Loss: 1.189537\n",
      "Epoch: 1056 \tTraining Loss: 0.004312 \tValidation Loss: 1.300356\n",
      "Epoch: 1057 \tTraining Loss: 0.010815 \tValidation Loss: 1.635120\n",
      "Epoch: 1058 \tTraining Loss: 0.006928 \tValidation Loss: 1.279371\n",
      "Epoch: 1059 \tTraining Loss: 0.005915 \tValidation Loss: 1.078090\n",
      "Epoch: 1060 \tTraining Loss: 0.003858 \tValidation Loss: 1.064417\n",
      "Epoch: 1061 \tTraining Loss: 0.004341 \tValidation Loss: 1.068547\n",
      "Epoch: 1062 \tTraining Loss: 0.005439 \tValidation Loss: 1.140648\n",
      "Epoch: 1063 \tTraining Loss: 0.007598 \tValidation Loss: 1.116748\n",
      "Epoch: 1064 \tTraining Loss: 0.001515 \tValidation Loss: 1.138161\n",
      "Epoch: 1065 \tTraining Loss: 0.004243 \tValidation Loss: 1.171037\n",
      "Epoch: 1066 \tTraining Loss: 0.002470 \tValidation Loss: 1.118493\n",
      "Epoch: 1067 \tTraining Loss: 0.009770 \tValidation Loss: 1.188678\n",
      "Epoch: 1068 \tTraining Loss: 0.010140 \tValidation Loss: 1.412435\n",
      "Epoch: 1069 \tTraining Loss: 0.002519 \tValidation Loss: 1.159145\n",
      "Epoch: 1070 \tTraining Loss: 0.121833 \tValidation Loss: 1.025348\n",
      "Epoch: 1071 \tTraining Loss: 0.260884 \tValidation Loss: 1.489774\n",
      "Epoch: 1072 \tTraining Loss: 0.329340 \tValidation Loss: 1.373997\n",
      "Epoch: 1073 \tTraining Loss: 0.068778 \tValidation Loss: 1.098127\n",
      "Epoch: 1074 \tTraining Loss: 0.109049 \tValidation Loss: 0.900400\n",
      "Epoch: 1075 \tTraining Loss: 0.080002 \tValidation Loss: 1.285290\n",
      "Epoch: 1076 \tTraining Loss: 0.463375 \tValidation Loss: 1.726515\n",
      "Epoch: 1077 \tTraining Loss: 0.197699 \tValidation Loss: 0.579553\n",
      "Epoch: 1078 \tTraining Loss: 0.351772 \tValidation Loss: 0.771575\n",
      "Epoch: 1079 \tTraining Loss: 0.191573 \tValidation Loss: 1.000399\n",
      "Epoch: 1080 \tTraining Loss: 0.171840 \tValidation Loss: 0.842866\n",
      "Epoch: 1081 \tTraining Loss: 0.099982 \tValidation Loss: 0.794356\n",
      "Epoch: 1082 \tTraining Loss: 0.071739 \tValidation Loss: 0.852662\n",
      "Epoch: 1083 \tTraining Loss: 0.027341 \tValidation Loss: 0.981451\n",
      "Epoch: 1084 \tTraining Loss: 0.139050 \tValidation Loss: 0.670449\n",
      "Epoch: 1085 \tTraining Loss: 0.058124 \tValidation Loss: 0.654543\n",
      "Epoch: 1086 \tTraining Loss: 0.025932 \tValidation Loss: 0.804364\n",
      "Epoch: 1087 \tTraining Loss: 0.009480 \tValidation Loss: 0.814561\n",
      "Epoch: 1088 \tTraining Loss: 0.022957 \tValidation Loss: 1.150761\n",
      "Epoch: 1089 \tTraining Loss: 0.018405 \tValidation Loss: 1.090011\n",
      "Epoch: 1090 \tTraining Loss: 0.011589 \tValidation Loss: 0.837895\n",
      "Epoch: 1091 \tTraining Loss: 0.025228 \tValidation Loss: 0.855585\n",
      "Epoch: 1092 \tTraining Loss: 0.008845 \tValidation Loss: 0.930956\n",
      "Epoch: 1093 \tTraining Loss: 0.005957 \tValidation Loss: 0.921706\n",
      "Epoch: 1094 \tTraining Loss: 0.007099 \tValidation Loss: 0.960601\n",
      "Epoch: 1095 \tTraining Loss: 0.005716 \tValidation Loss: 0.852114\n",
      "Epoch: 1096 \tTraining Loss: 0.008166 \tValidation Loss: 0.970560\n",
      "Epoch: 1097 \tTraining Loss: 0.019819 \tValidation Loss: 1.360733\n",
      "Epoch: 1098 \tTraining Loss: 0.004716 \tValidation Loss: 1.031383\n",
      "Epoch: 1099 \tTraining Loss: 0.005144 \tValidation Loss: 1.143265\n",
      "Epoch: 1100 \tTraining Loss: 0.007164 \tValidation Loss: 1.054879\n",
      "Epoch: 1101 \tTraining Loss: 0.003460 \tValidation Loss: 1.195679\n",
      "Epoch: 1102 \tTraining Loss: 0.007710 \tValidation Loss: 1.014609\n",
      "Epoch: 1103 \tTraining Loss: 0.002946 \tValidation Loss: 1.004700\n",
      "Epoch: 1104 \tTraining Loss: 0.004185 \tValidation Loss: 1.188030\n",
      "Epoch: 1105 \tTraining Loss: 0.006595 \tValidation Loss: 1.026375\n",
      "Epoch: 1106 \tTraining Loss: 0.002911 \tValidation Loss: 1.062017\n",
      "Epoch: 1107 \tTraining Loss: 0.002089 \tValidation Loss: 1.129200\n",
      "Epoch: 1108 \tTraining Loss: 0.002959 \tValidation Loss: 1.223549\n",
      "Epoch: 1109 \tTraining Loss: 0.002391 \tValidation Loss: 1.280303\n",
      "Epoch: 1110 \tTraining Loss: 0.001931 \tValidation Loss: 1.244618\n",
      "Epoch: 1111 \tTraining Loss: 0.003417 \tValidation Loss: 1.178596\n",
      "Epoch: 1112 \tTraining Loss: 0.001951 \tValidation Loss: 1.107860\n",
      "Epoch: 1113 \tTraining Loss: 0.002017 \tValidation Loss: 1.025868\n",
      "Epoch: 1114 \tTraining Loss: 0.007038 \tValidation Loss: 1.112569\n",
      "Epoch: 1115 \tTraining Loss: 0.005476 \tValidation Loss: 1.167792\n",
      "Epoch: 1116 \tTraining Loss: 0.006522 \tValidation Loss: 0.784113\n",
      "Epoch: 1117 \tTraining Loss: 0.022427 \tValidation Loss: 1.064625\n",
      "Epoch: 1118 \tTraining Loss: 0.005273 \tValidation Loss: 0.877092\n",
      "Epoch: 1119 \tTraining Loss: 0.002799 \tValidation Loss: 1.020208\n",
      "Epoch: 1120 \tTraining Loss: 0.006529 \tValidation Loss: 0.875763\n",
      "Epoch: 1121 \tTraining Loss: 0.001937 \tValidation Loss: 0.939452\n",
      "Epoch: 1122 \tTraining Loss: 0.006052 \tValidation Loss: 0.826974\n",
      "Epoch: 1123 \tTraining Loss: 0.008191 \tValidation Loss: 0.946566\n",
      "Epoch: 1124 \tTraining Loss: 0.002117 \tValidation Loss: 1.334884\n",
      "Epoch: 1125 \tTraining Loss: 0.003852 \tValidation Loss: 1.107967\n",
      "Epoch: 1126 \tTraining Loss: 0.002444 \tValidation Loss: 1.192898\n",
      "Epoch: 1127 \tTraining Loss: 0.003819 \tValidation Loss: 1.083958\n",
      "Epoch: 1128 \tTraining Loss: 0.001147 \tValidation Loss: 0.916421\n",
      "Epoch: 1129 \tTraining Loss: 0.002216 \tValidation Loss: 1.052771\n",
      "Epoch: 1130 \tTraining Loss: 0.001884 \tValidation Loss: 1.137858\n",
      "Epoch: 1131 \tTraining Loss: 0.000548 \tValidation Loss: 1.208936\n",
      "Epoch: 1132 \tTraining Loss: 0.002015 \tValidation Loss: 1.242479\n",
      "Epoch: 1133 \tTraining Loss: 0.001041 \tValidation Loss: 1.274443\n",
      "Epoch: 1134 \tTraining Loss: 0.001130 \tValidation Loss: 1.261793\n",
      "Epoch: 1135 \tTraining Loss: 0.000734 \tValidation Loss: 1.287981\n",
      "Epoch: 1136 \tTraining Loss: 0.001108 \tValidation Loss: 1.265849\n",
      "Epoch: 1137 \tTraining Loss: 0.001200 \tValidation Loss: 1.450137\n",
      "Epoch: 1138 \tTraining Loss: 0.001168 \tValidation Loss: 1.411500\n",
      "Epoch: 1139 \tTraining Loss: 0.000681 \tValidation Loss: 1.285486\n",
      "Epoch: 1140 \tTraining Loss: 0.000508 \tValidation Loss: 1.244227\n",
      "Epoch: 1141 \tTraining Loss: 0.003015 \tValidation Loss: 1.340172\n",
      "Epoch: 1142 \tTraining Loss: 0.001197 \tValidation Loss: 1.097662\n",
      "Epoch: 1143 \tTraining Loss: 0.001942 \tValidation Loss: 1.311203\n",
      "Epoch: 1144 \tTraining Loss: 0.001309 \tValidation Loss: 1.345128\n",
      "Epoch: 1145 \tTraining Loss: 0.002019 \tValidation Loss: 1.232618\n",
      "Epoch: 1146 \tTraining Loss: 0.000896 \tValidation Loss: 1.239304\n",
      "Epoch: 1147 \tTraining Loss: 0.011034 \tValidation Loss: 1.461468\n",
      "Epoch: 1148 \tTraining Loss: 0.032203 \tValidation Loss: 1.516047\n",
      "Epoch: 1149 \tTraining Loss: 0.007542 \tValidation Loss: 1.667974\n",
      "Epoch: 1150 \tTraining Loss: 0.003116 \tValidation Loss: 1.848108\n",
      "Epoch: 1151 \tTraining Loss: 0.051430 \tValidation Loss: 1.416120\n",
      "Epoch: 1152 \tTraining Loss: 0.100379 \tValidation Loss: 2.152029\n",
      "Epoch: 1153 \tTraining Loss: 0.150684 \tValidation Loss: 1.626498\n",
      "Epoch: 1154 \tTraining Loss: 0.345413 \tValidation Loss: 0.893541\n",
      "Epoch: 1155 \tTraining Loss: 0.678304 \tValidation Loss: 1.436612\n",
      "Epoch: 1156 \tTraining Loss: 0.278241 \tValidation Loss: 0.984966\n",
      "Epoch: 1157 \tTraining Loss: 0.175477 \tValidation Loss: 0.958885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1158 \tTraining Loss: 0.095411 \tValidation Loss: 0.871683\n",
      "Epoch: 1159 \tTraining Loss: 0.221045 \tValidation Loss: 1.707843\n",
      "Epoch: 1160 \tTraining Loss: 0.086245 \tValidation Loss: 0.902333\n",
      "Epoch: 1161 \tTraining Loss: 0.382046 \tValidation Loss: 1.527630\n",
      "Epoch: 1162 \tTraining Loss: 0.156546 \tValidation Loss: 0.930814\n",
      "Epoch: 1163 \tTraining Loss: 0.144065 \tValidation Loss: 0.965899\n",
      "Epoch: 1164 \tTraining Loss: 0.107944 \tValidation Loss: 1.074440\n",
      "Epoch: 1165 \tTraining Loss: 0.065429 \tValidation Loss: 1.124534\n",
      "Epoch: 1166 \tTraining Loss: 0.029633 \tValidation Loss: 0.937217\n",
      "Epoch: 1167 \tTraining Loss: 0.026568 \tValidation Loss: 0.978833\n",
      "Epoch: 1168 \tTraining Loss: 0.083081 \tValidation Loss: 1.266991\n",
      "Epoch: 1169 \tTraining Loss: 0.177937 \tValidation Loss: 1.220501\n",
      "Epoch: 1170 \tTraining Loss: 0.181158 \tValidation Loss: 0.932940\n",
      "Epoch: 1171 \tTraining Loss: 0.068708 \tValidation Loss: 1.075800\n",
      "Epoch: 1172 \tTraining Loss: 0.036626 \tValidation Loss: 0.675210\n",
      "Epoch: 1173 \tTraining Loss: 0.028650 \tValidation Loss: 0.597816\n",
      "Epoch: 1174 \tTraining Loss: 0.055928 \tValidation Loss: 0.981777\n",
      "Epoch: 1175 \tTraining Loss: 0.068766 \tValidation Loss: 1.108825\n",
      "Epoch: 1176 \tTraining Loss: 0.028414 \tValidation Loss: 0.973995\n",
      "Epoch: 1177 \tTraining Loss: 0.018122 \tValidation Loss: 0.945907\n",
      "Epoch: 1178 \tTraining Loss: 0.023651 \tValidation Loss: 0.880104\n",
      "Epoch: 1179 \tTraining Loss: 0.007076 \tValidation Loss: 0.876756\n",
      "Epoch: 1180 \tTraining Loss: 0.006102 \tValidation Loss: 0.890978\n",
      "Epoch: 1181 \tTraining Loss: 0.007385 \tValidation Loss: 0.935965\n",
      "Epoch: 1182 \tTraining Loss: 0.007171 \tValidation Loss: 0.873279\n",
      "Epoch: 1183 \tTraining Loss: 0.006394 \tValidation Loss: 0.872923\n",
      "Epoch: 1184 \tTraining Loss: 0.003767 \tValidation Loss: 0.882371\n",
      "Epoch: 1185 \tTraining Loss: 0.005225 \tValidation Loss: 0.974027\n",
      "Epoch: 1186 \tTraining Loss: 0.007011 \tValidation Loss: 0.993820\n",
      "Epoch: 1187 \tTraining Loss: 0.003198 \tValidation Loss: 0.903155\n",
      "Epoch: 1188 \tTraining Loss: 0.007785 \tValidation Loss: 1.176308\n",
      "Epoch: 1189 \tTraining Loss: 0.011817 \tValidation Loss: 1.039968\n",
      "Epoch: 1190 \tTraining Loss: 0.161319 \tValidation Loss: 1.407670\n",
      "Epoch: 1191 \tTraining Loss: 0.047871 \tValidation Loss: 0.667885\n",
      "Epoch: 1192 \tTraining Loss: 0.018358 \tValidation Loss: 1.723081\n",
      "Epoch: 1193 \tTraining Loss: 0.426423 \tValidation Loss: 1.708811\n",
      "Epoch: 1194 \tTraining Loss: 0.411105 \tValidation Loss: 2.758757\n",
      "Epoch: 1195 \tTraining Loss: 0.262611 \tValidation Loss: 0.967308\n",
      "Epoch: 1196 \tTraining Loss: 0.313508 \tValidation Loss: 1.002572\n",
      "Epoch: 1197 \tTraining Loss: 0.251713 \tValidation Loss: 1.117613\n",
      "Epoch: 1198 \tTraining Loss: 0.188954 \tValidation Loss: 0.809152\n",
      "Epoch: 1199 \tTraining Loss: 0.185501 \tValidation Loss: 0.794237\n",
      "Epoch: 1200 \tTraining Loss: 0.133041 \tValidation Loss: 0.831009\n",
      "Epoch: 1201 \tTraining Loss: 0.326837 \tValidation Loss: 1.133683\n",
      "Epoch: 1202 \tTraining Loss: 0.147237 \tValidation Loss: 1.119285\n",
      "Epoch: 1203 \tTraining Loss: 0.233386 \tValidation Loss: 0.798273\n",
      "Epoch: 1204 \tTraining Loss: 0.104557 \tValidation Loss: 0.670134\n",
      "Epoch: 1205 \tTraining Loss: 0.072470 \tValidation Loss: 0.806338\n",
      "Epoch: 1206 \tTraining Loss: 0.023757 \tValidation Loss: 0.957629\n",
      "Epoch: 1207 \tTraining Loss: 0.015115 \tValidation Loss: 1.020091\n",
      "Epoch: 1208 \tTraining Loss: 0.015349 \tValidation Loss: 0.941031\n",
      "Epoch: 1209 \tTraining Loss: 0.020789 \tValidation Loss: 0.887084\n",
      "Epoch: 1210 \tTraining Loss: 0.015666 \tValidation Loss: 0.937586\n",
      "Epoch: 1211 \tTraining Loss: 0.059694 \tValidation Loss: 0.900760\n",
      "Epoch: 1212 \tTraining Loss: 0.032542 \tValidation Loss: 0.670867\n",
      "Epoch: 1213 \tTraining Loss: 0.014958 \tValidation Loss: 0.662512\n",
      "Epoch: 1214 \tTraining Loss: 0.007439 \tValidation Loss: 0.694708\n",
      "Epoch: 1215 \tTraining Loss: 0.008569 \tValidation Loss: 0.777075\n",
      "Epoch: 1216 \tTraining Loss: 0.017631 \tValidation Loss: 0.759933\n",
      "Epoch: 1217 \tTraining Loss: 0.011451 \tValidation Loss: 0.815476\n",
      "Epoch: 1218 \tTraining Loss: 0.005335 \tValidation Loss: 0.890690\n",
      "Epoch: 1219 \tTraining Loss: 0.096419 \tValidation Loss: 0.992422\n",
      "Epoch: 1220 \tTraining Loss: 0.384835 \tValidation Loss: 1.188153\n",
      "Epoch: 1221 \tTraining Loss: 0.423814 \tValidation Loss: 1.950998\n",
      "Epoch: 1222 \tTraining Loss: 0.378238 \tValidation Loss: 2.654602\n",
      "Epoch: 1223 \tTraining Loss: 0.400260 \tValidation Loss: 1.186369\n",
      "Epoch: 1224 \tTraining Loss: 0.263787 \tValidation Loss: 1.912082\n",
      "Epoch: 1225 \tTraining Loss: 0.203242 \tValidation Loss: 1.581509\n",
      "Epoch: 1226 \tTraining Loss: 0.196147 \tValidation Loss: 0.934721\n",
      "Epoch: 1227 \tTraining Loss: 0.263433 \tValidation Loss: 1.273214\n",
      "Epoch: 1228 \tTraining Loss: 0.157975 \tValidation Loss: 1.063748\n",
      "Epoch: 1229 \tTraining Loss: 0.112351 \tValidation Loss: 0.886381\n",
      "Epoch: 1230 \tTraining Loss: 0.116016 \tValidation Loss: 0.723309\n",
      "Epoch: 1231 \tTraining Loss: 0.099181 \tValidation Loss: 0.689536\n",
      "Epoch: 1232 \tTraining Loss: 0.091806 \tValidation Loss: 0.823615\n",
      "Epoch: 1233 \tTraining Loss: 0.035984 \tValidation Loss: 1.027580\n",
      "Epoch: 1234 \tTraining Loss: 0.034718 \tValidation Loss: 1.191936\n",
      "Epoch: 1235 \tTraining Loss: 0.012744 \tValidation Loss: 1.226984\n",
      "Epoch: 1236 \tTraining Loss: 0.027433 \tValidation Loss: 1.284800\n",
      "Epoch: 1237 \tTraining Loss: 0.018902 \tValidation Loss: 1.056277\n",
      "Epoch: 1238 \tTraining Loss: 0.027038 \tValidation Loss: 1.241549\n",
      "Epoch: 1239 \tTraining Loss: 0.111444 \tValidation Loss: 0.990261\n",
      "Epoch: 1240 \tTraining Loss: 0.119696 \tValidation Loss: 1.110097\n",
      "Epoch: 1241 \tTraining Loss: 0.089970 \tValidation Loss: 0.976649\n",
      "Epoch: 1242 \tTraining Loss: 0.072016 \tValidation Loss: 1.090595\n",
      "Epoch: 1243 \tTraining Loss: 0.134056 \tValidation Loss: 1.338184\n",
      "Epoch: 1244 \tTraining Loss: 0.097996 \tValidation Loss: 1.582108\n",
      "Epoch: 1245 \tTraining Loss: 0.066317 \tValidation Loss: 1.336128\n",
      "Epoch: 1246 \tTraining Loss: 0.058948 \tValidation Loss: 1.091078\n",
      "Epoch: 1247 \tTraining Loss: 0.022725 \tValidation Loss: 1.238508\n",
      "Epoch: 1248 \tTraining Loss: 0.020425 \tValidation Loss: 1.345627\n",
      "Epoch: 1249 \tTraining Loss: 0.016424 \tValidation Loss: 1.394145\n",
      "Epoch: 1250 \tTraining Loss: 0.014138 \tValidation Loss: 1.481298\n",
      "Epoch: 1251 \tTraining Loss: 0.030188 \tValidation Loss: 1.322221\n",
      "Epoch: 1252 \tTraining Loss: 0.038949 \tValidation Loss: 1.355644\n",
      "Epoch: 1253 \tTraining Loss: 0.017507 \tValidation Loss: 1.350097\n",
      "Epoch: 1254 \tTraining Loss: 0.030310 \tValidation Loss: 1.426780\n",
      "Epoch: 1255 \tTraining Loss: 0.027082 \tValidation Loss: 1.188292\n",
      "Epoch: 1256 \tTraining Loss: 0.032686 \tValidation Loss: 1.205939\n",
      "Epoch: 1257 \tTraining Loss: 0.007969 \tValidation Loss: 1.387491\n",
      "Epoch: 1258 \tTraining Loss: 0.019287 \tValidation Loss: 1.151910\n",
      "Epoch: 1259 \tTraining Loss: 0.160010 \tValidation Loss: 1.350355\n",
      "Epoch: 1260 \tTraining Loss: 0.047118 \tValidation Loss: 1.221111\n",
      "Epoch: 1261 \tTraining Loss: 0.017842 \tValidation Loss: 1.295532\n",
      "Epoch: 1262 \tTraining Loss: 0.028490 \tValidation Loss: 1.425732\n",
      "Epoch: 1263 \tTraining Loss: 0.006963 \tValidation Loss: 1.564581\n",
      "Epoch: 1264 \tTraining Loss: 0.029993 \tValidation Loss: 1.353866\n",
      "Epoch: 1265 \tTraining Loss: 0.016496 \tValidation Loss: 1.106782\n",
      "Epoch: 1266 \tTraining Loss: 0.031207 \tValidation Loss: 1.434027\n",
      "Epoch: 1267 \tTraining Loss: 0.011310 \tValidation Loss: 1.365223\n",
      "Epoch: 1268 \tTraining Loss: 0.003669 \tValidation Loss: 1.461533\n",
      "Epoch: 1269 \tTraining Loss: 0.005781 \tValidation Loss: 1.497824\n",
      "Epoch: 1270 \tTraining Loss: 0.004964 \tValidation Loss: 1.533980\n",
      "Epoch: 1271 \tTraining Loss: 0.008416 \tValidation Loss: 1.443658\n",
      "Epoch: 1272 \tTraining Loss: 0.007284 \tValidation Loss: 1.712330\n",
      "Epoch: 1273 \tTraining Loss: 0.006587 \tValidation Loss: 1.735835\n",
      "Epoch: 1274 \tTraining Loss: 0.005132 \tValidation Loss: 1.680349\n",
      "Epoch: 1275 \tTraining Loss: 0.007914 \tValidation Loss: 1.667402\n",
      "Epoch: 1276 \tTraining Loss: 0.007362 \tValidation Loss: 1.562695\n",
      "Epoch: 1277 \tTraining Loss: 0.006216 \tValidation Loss: 1.615147\n",
      "Epoch: 1278 \tTraining Loss: 0.008412 \tValidation Loss: 1.674245\n",
      "Epoch: 1279 \tTraining Loss: 0.002246 \tValidation Loss: 1.632148\n",
      "Epoch: 1280 \tTraining Loss: 0.004109 \tValidation Loss: 1.644993\n",
      "Epoch: 1281 \tTraining Loss: 0.002014 \tValidation Loss: 1.747904\n",
      "Epoch: 1282 \tTraining Loss: 0.002970 \tValidation Loss: 1.873522\n",
      "Epoch: 1283 \tTraining Loss: 0.003156 \tValidation Loss: 1.803284\n",
      "Epoch: 1284 \tTraining Loss: 0.004013 \tValidation Loss: 1.770059\n",
      "Epoch: 1285 \tTraining Loss: 0.003777 \tValidation Loss: 1.725016\n",
      "Epoch: 1286 \tTraining Loss: 0.002717 \tValidation Loss: 1.868555\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1287 \tTraining Loss: 0.003549 \tValidation Loss: 1.867394\n",
      "Epoch: 1288 \tTraining Loss: 0.002642 \tValidation Loss: 2.007659\n",
      "Epoch: 1289 \tTraining Loss: 0.001939 \tValidation Loss: 1.965655\n",
      "Epoch: 1290 \tTraining Loss: 0.002661 \tValidation Loss: 2.012744\n",
      "Epoch: 1291 \tTraining Loss: 0.001715 \tValidation Loss: 1.957553\n",
      "Epoch: 1292 \tTraining Loss: 0.001803 \tValidation Loss: 1.899221\n",
      "Epoch: 1293 \tTraining Loss: 0.002699 \tValidation Loss: 1.810073\n",
      "Epoch: 1294 \tTraining Loss: 0.003490 \tValidation Loss: 1.658065\n",
      "Epoch: 1295 \tTraining Loss: 0.005839 \tValidation Loss: 1.694368\n",
      "Epoch: 1296 \tTraining Loss: 0.008583 \tValidation Loss: 1.480688\n",
      "Epoch: 1297 \tTraining Loss: 0.005430 \tValidation Loss: 1.670018\n",
      "Epoch: 1298 \tTraining Loss: 0.001281 \tValidation Loss: 1.762353\n",
      "Epoch: 1299 \tTraining Loss: 0.001001 \tValidation Loss: 1.989234\n",
      "Epoch: 1300 \tTraining Loss: 0.004613 \tValidation Loss: 1.834560\n",
      "Epoch: 1301 \tTraining Loss: 0.001618 \tValidation Loss: 1.745836\n",
      "Epoch: 1302 \tTraining Loss: 0.001429 \tValidation Loss: 1.799357\n",
      "Epoch: 1303 \tTraining Loss: 0.029547 \tValidation Loss: 1.455104\n",
      "Epoch: 1304 \tTraining Loss: 0.019674 \tValidation Loss: 1.605929\n",
      "Epoch: 1305 \tTraining Loss: 0.005012 \tValidation Loss: 1.736904\n",
      "Epoch: 1306 \tTraining Loss: 0.006669 \tValidation Loss: 1.209225\n",
      "Epoch: 1307 \tTraining Loss: 0.004484 \tValidation Loss: 1.445669\n",
      "Epoch: 1308 \tTraining Loss: 0.004421 \tValidation Loss: 2.283097\n",
      "Epoch: 1309 \tTraining Loss: 0.002214 \tValidation Loss: 1.891511\n",
      "Epoch: 1310 \tTraining Loss: 0.002448 \tValidation Loss: 1.898007\n",
      "Epoch: 1311 \tTraining Loss: 0.001410 \tValidation Loss: 2.094986\n",
      "Epoch: 1312 \tTraining Loss: 0.001524 \tValidation Loss: 2.191816\n",
      "Epoch: 1313 \tTraining Loss: 0.084041 \tValidation Loss: 2.080820\n",
      "Epoch: 1314 \tTraining Loss: 0.262186 \tValidation Loss: 2.319674\n",
      "Epoch: 1315 \tTraining Loss: 0.601464 \tValidation Loss: 2.644278\n",
      "Epoch: 1316 \tTraining Loss: 0.274713 \tValidation Loss: 1.050668\n",
      "Epoch: 1317 \tTraining Loss: 0.218433 \tValidation Loss: 1.391819\n",
      "Epoch: 1318 \tTraining Loss: 0.175179 \tValidation Loss: 0.999122\n",
      "Epoch: 1319 \tTraining Loss: 0.182112 \tValidation Loss: 0.726843\n",
      "Epoch: 1320 \tTraining Loss: 0.082832 \tValidation Loss: 1.258350\n",
      "Epoch: 1321 \tTraining Loss: 0.138155 \tValidation Loss: 0.894362\n",
      "Epoch: 1322 \tTraining Loss: 0.054497 \tValidation Loss: 0.854350\n",
      "Epoch: 1323 \tTraining Loss: 0.040186 \tValidation Loss: 0.966093\n",
      "Epoch: 1324 \tTraining Loss: 0.036753 \tValidation Loss: 0.940495\n",
      "Epoch: 1325 \tTraining Loss: 0.017755 \tValidation Loss: 1.019302\n",
      "Epoch: 1326 \tTraining Loss: 0.006694 \tValidation Loss: 0.986116\n",
      "Epoch: 1327 \tTraining Loss: 0.012212 \tValidation Loss: 1.052890\n",
      "Epoch: 1328 \tTraining Loss: 0.020321 \tValidation Loss: 1.086256\n",
      "Epoch: 1329 \tTraining Loss: 0.009389 \tValidation Loss: 1.086980\n",
      "Epoch: 1330 \tTraining Loss: 0.017838 \tValidation Loss: 1.184628\n",
      "Epoch: 1331 \tTraining Loss: 0.015077 \tValidation Loss: 1.061009\n",
      "Epoch: 1332 \tTraining Loss: 0.023011 \tValidation Loss: 1.059383\n",
      "Epoch: 1333 \tTraining Loss: 0.041999 \tValidation Loss: 1.336965\n",
      "Epoch: 1334 \tTraining Loss: 0.060479 \tValidation Loss: 1.305731\n",
      "Epoch: 1335 \tTraining Loss: 0.184081 \tValidation Loss: 0.896797\n",
      "Epoch: 1336 \tTraining Loss: 0.053875 \tValidation Loss: 0.733852\n",
      "Epoch: 1337 \tTraining Loss: 0.015657 \tValidation Loss: 0.860896\n",
      "Epoch: 1338 \tTraining Loss: 0.029948 \tValidation Loss: 0.956610\n",
      "Epoch: 1339 \tTraining Loss: 0.025745 \tValidation Loss: 1.079586\n",
      "Epoch: 1340 \tTraining Loss: 0.015482 \tValidation Loss: 1.021469\n",
      "Epoch: 1341 \tTraining Loss: 0.006993 \tValidation Loss: 1.040944\n",
      "Epoch: 1342 \tTraining Loss: 0.006131 \tValidation Loss: 1.032054\n",
      "Epoch: 1343 \tTraining Loss: 0.038623 \tValidation Loss: 0.997052\n",
      "Epoch: 1344 \tTraining Loss: 0.056557 \tValidation Loss: 2.211577\n",
      "Epoch: 1345 \tTraining Loss: 0.069332 \tValidation Loss: 1.313194\n",
      "Epoch: 1346 \tTraining Loss: 0.219408 \tValidation Loss: 0.955852\n",
      "Epoch: 1347 \tTraining Loss: 0.030662 \tValidation Loss: 0.766953\n",
      "Epoch: 1348 \tTraining Loss: 0.020782 \tValidation Loss: 1.066606\n",
      "Epoch: 1349 \tTraining Loss: 0.013624 \tValidation Loss: 0.948879\n",
      "Epoch: 1350 \tTraining Loss: 0.015916 \tValidation Loss: 1.038201\n",
      "Epoch: 1351 \tTraining Loss: 0.007131 \tValidation Loss: 1.159742\n",
      "Epoch: 1352 \tTraining Loss: 0.002942 \tValidation Loss: 1.331872\n",
      "Epoch: 1353 \tTraining Loss: 0.010633 \tValidation Loss: 1.392312\n",
      "Epoch: 1354 \tTraining Loss: 0.006998 \tValidation Loss: 1.508487\n",
      "Epoch: 1355 \tTraining Loss: 0.009855 \tValidation Loss: 1.337397\n",
      "Epoch: 1356 \tTraining Loss: 0.004894 \tValidation Loss: 1.193882\n",
      "Epoch: 1357 \tTraining Loss: 0.010965 \tValidation Loss: 1.117861\n",
      "Epoch: 1358 \tTraining Loss: 0.071954 \tValidation Loss: 1.291803\n",
      "Epoch: 1359 \tTraining Loss: 0.007745 \tValidation Loss: 1.027580\n",
      "Epoch: 1360 \tTraining Loss: 0.057900 \tValidation Loss: 1.398552\n",
      "Epoch: 1361 \tTraining Loss: 0.115261 \tValidation Loss: 1.323068\n",
      "Epoch: 1362 \tTraining Loss: 0.089709 \tValidation Loss: 2.407127\n",
      "Epoch: 1363 \tTraining Loss: 0.063313 \tValidation Loss: 1.834115\n",
      "Epoch: 1364 \tTraining Loss: 0.066204 \tValidation Loss: 1.798556\n",
      "Epoch: 1365 \tTraining Loss: 0.049208 \tValidation Loss: 1.217082\n",
      "Epoch: 1366 \tTraining Loss: 0.090015 \tValidation Loss: 1.036942\n",
      "Epoch: 1367 \tTraining Loss: 0.045377 \tValidation Loss: 0.806178\n",
      "Epoch: 1368 \tTraining Loss: 0.095661 \tValidation Loss: 0.810384\n",
      "Epoch: 1369 \tTraining Loss: 0.038585 \tValidation Loss: 0.831611\n",
      "Epoch: 1370 \tTraining Loss: 0.036298 \tValidation Loss: 0.754369\n",
      "Epoch: 1371 \tTraining Loss: 0.018090 \tValidation Loss: 0.762274\n",
      "Epoch: 1372 \tTraining Loss: 0.005657 \tValidation Loss: 0.768101\n",
      "Epoch: 1373 \tTraining Loss: 0.006470 \tValidation Loss: 0.904650\n",
      "Epoch: 1374 \tTraining Loss: 0.003784 \tValidation Loss: 0.872200\n",
      "Epoch: 1375 \tTraining Loss: 0.003730 \tValidation Loss: 1.008331\n",
      "Epoch: 1376 \tTraining Loss: 0.005832 \tValidation Loss: 0.945501\n",
      "Epoch: 1377 \tTraining Loss: 0.004230 \tValidation Loss: 0.892626\n",
      "Epoch: 1378 \tTraining Loss: 0.011656 \tValidation Loss: 0.716659\n",
      "Epoch: 1379 \tTraining Loss: 0.034229 \tValidation Loss: 0.793314\n",
      "Epoch: 1380 \tTraining Loss: 0.137186 \tValidation Loss: 1.147376\n",
      "Epoch: 1381 \tTraining Loss: 0.143636 \tValidation Loss: 1.411642\n",
      "Epoch: 1382 \tTraining Loss: 0.160610 \tValidation Loss: 1.147095\n",
      "Epoch: 1383 \tTraining Loss: 0.055021 \tValidation Loss: 1.007647\n",
      "Epoch: 1384 \tTraining Loss: 0.046546 \tValidation Loss: 0.779864\n",
      "Epoch: 1385 \tTraining Loss: 0.034780 \tValidation Loss: 0.753242\n",
      "Epoch: 1386 \tTraining Loss: 0.012717 \tValidation Loss: 0.948891\n",
      "Epoch: 1387 \tTraining Loss: 0.024780 \tValidation Loss: 1.116970\n",
      "Epoch: 1388 \tTraining Loss: 0.015326 \tValidation Loss: 0.990336\n",
      "Epoch: 1389 \tTraining Loss: 0.009028 \tValidation Loss: 1.031600\n",
      "Epoch: 1390 \tTraining Loss: 0.006778 \tValidation Loss: 1.077803\n",
      "Epoch: 1391 \tTraining Loss: 0.006558 \tValidation Loss: 1.138254\n",
      "Epoch: 1392 \tTraining Loss: 0.004558 \tValidation Loss: 1.133735\n",
      "Epoch: 1393 \tTraining Loss: 0.004482 \tValidation Loss: 1.109505\n",
      "Epoch: 1394 \tTraining Loss: 0.004259 \tValidation Loss: 1.143165\n",
      "Epoch: 1395 \tTraining Loss: 0.006855 \tValidation Loss: 1.025579\n",
      "Epoch: 1396 \tTraining Loss: 0.006708 \tValidation Loss: 1.056122\n",
      "Epoch: 1397 \tTraining Loss: 0.009794 \tValidation Loss: 1.082994\n",
      "Epoch: 1398 \tTraining Loss: 0.002601 \tValidation Loss: 1.004212\n",
      "Epoch: 1399 \tTraining Loss: 0.003209 \tValidation Loss: 0.995935\n",
      "Epoch: 1400 \tTraining Loss: 0.003136 \tValidation Loss: 1.032659\n",
      "Epoch: 1401 \tTraining Loss: 0.019317 \tValidation Loss: 1.098341\n",
      "Epoch: 1402 \tTraining Loss: 0.006305 \tValidation Loss: 1.335766\n",
      "Epoch: 1403 \tTraining Loss: 0.001933 \tValidation Loss: 1.384488\n",
      "Epoch: 1404 \tTraining Loss: 0.002601 \tValidation Loss: 1.307492\n",
      "Epoch: 1405 \tTraining Loss: 0.340643 \tValidation Loss: 3.220879\n",
      "Epoch: 1406 \tTraining Loss: 0.325701 \tValidation Loss: 0.942308\n",
      "Epoch: 1407 \tTraining Loss: 0.339331 \tValidation Loss: 0.600916\n",
      "Epoch: 1408 \tTraining Loss: 0.490590 \tValidation Loss: 0.908946\n",
      "Epoch: 1409 \tTraining Loss: 0.400536 \tValidation Loss: 0.730656\n",
      "Epoch: 1410 \tTraining Loss: 0.171229 \tValidation Loss: 0.764026\n",
      "Epoch: 1411 \tTraining Loss: 0.143444 \tValidation Loss: 0.953371\n",
      "Epoch: 1412 \tTraining Loss: 0.122550 \tValidation Loss: 1.018963\n",
      "Epoch: 1413 \tTraining Loss: 0.050194 \tValidation Loss: 0.796240\n",
      "Epoch: 1414 \tTraining Loss: 0.027343 \tValidation Loss: 1.000600\n",
      "Epoch: 1415 \tTraining Loss: 0.017598 \tValidation Loss: 1.036219\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1416 \tTraining Loss: 0.041476 \tValidation Loss: 0.750031\n",
      "Epoch: 1417 \tTraining Loss: 0.048045 \tValidation Loss: 0.954680\n",
      "Epoch: 1418 \tTraining Loss: 0.014621 \tValidation Loss: 1.156498\n",
      "Epoch: 1419 \tTraining Loss: 0.041567 \tValidation Loss: 0.895598\n",
      "Epoch: 1420 \tTraining Loss: 0.017775 \tValidation Loss: 1.193825\n",
      "Epoch: 1421 \tTraining Loss: 0.014783 \tValidation Loss: 1.234997\n",
      "Epoch: 1422 \tTraining Loss: 0.004495 \tValidation Loss: 1.251309\n",
      "Epoch: 1423 \tTraining Loss: 0.007387 \tValidation Loss: 1.375002\n",
      "Epoch: 1424 \tTraining Loss: 0.024807 \tValidation Loss: 1.291577\n",
      "Epoch: 1425 \tTraining Loss: 0.023695 \tValidation Loss: 1.141980\n",
      "Epoch: 1426 \tTraining Loss: 0.023586 \tValidation Loss: 1.009541\n",
      "Epoch: 1427 \tTraining Loss: 0.014869 \tValidation Loss: 1.105094\n",
      "Epoch: 1428 \tTraining Loss: 0.006574 \tValidation Loss: 1.196698\n",
      "Epoch: 1429 \tTraining Loss: 0.047737 \tValidation Loss: 1.226037\n",
      "Epoch: 1430 \tTraining Loss: 0.024251 \tValidation Loss: 1.282370\n",
      "Epoch: 1431 \tTraining Loss: 0.031863 \tValidation Loss: 1.054937\n",
      "Epoch: 1432 \tTraining Loss: 0.011579 \tValidation Loss: 1.085499\n",
      "Epoch: 1433 \tTraining Loss: 0.014717 \tValidation Loss: 1.080797\n",
      "Epoch: 1434 \tTraining Loss: 0.007152 \tValidation Loss: 1.036884\n",
      "Epoch: 1435 \tTraining Loss: 0.006771 \tValidation Loss: 0.988361\n",
      "Epoch: 1436 \tTraining Loss: 0.006619 \tValidation Loss: 0.995046\n",
      "Epoch: 1437 \tTraining Loss: 0.003409 \tValidation Loss: 1.040416\n",
      "Epoch: 1438 \tTraining Loss: 0.005911 \tValidation Loss: 1.048887\n",
      "Epoch: 1439 \tTraining Loss: 0.006714 \tValidation Loss: 1.017538\n",
      "Epoch: 1440 \tTraining Loss: 0.005569 \tValidation Loss: 1.094314\n",
      "Epoch: 1441 \tTraining Loss: 0.002886 \tValidation Loss: 1.146846\n",
      "Epoch: 1442 \tTraining Loss: 0.014561 \tValidation Loss: 1.392613\n",
      "Epoch: 1443 \tTraining Loss: 0.018358 \tValidation Loss: 1.347090\n",
      "Epoch: 1444 \tTraining Loss: 0.021999 \tValidation Loss: 1.648261\n",
      "Epoch: 1445 \tTraining Loss: 0.011167 \tValidation Loss: 1.406696\n",
      "Epoch: 1446 \tTraining Loss: 0.006016 \tValidation Loss: 1.410323\n",
      "Epoch: 1447 \tTraining Loss: 0.008345 \tValidation Loss: 1.420267\n",
      "Epoch: 1448 \tTraining Loss: 0.003858 \tValidation Loss: 1.573373\n",
      "Epoch: 1449 \tTraining Loss: 0.003375 \tValidation Loss: 1.488780\n",
      "Epoch: 1450 \tTraining Loss: 0.005970 \tValidation Loss: 1.308662\n",
      "Epoch: 1451 \tTraining Loss: 0.003174 \tValidation Loss: 1.311543\n",
      "Epoch: 1452 \tTraining Loss: 0.007466 \tValidation Loss: 1.266000\n",
      "Epoch: 1453 \tTraining Loss: 0.099483 \tValidation Loss: 1.185256\n",
      "Epoch: 1454 \tTraining Loss: 0.293256 \tValidation Loss: 1.650936\n",
      "Epoch: 1455 \tTraining Loss: 0.412484 \tValidation Loss: 0.651286\n",
      "Epoch: 1456 \tTraining Loss: 0.313052 \tValidation Loss: 0.635725\n",
      "Epoch: 1457 \tTraining Loss: 0.139509 \tValidation Loss: 1.142233\n",
      "Epoch: 1458 \tTraining Loss: 0.066977 \tValidation Loss: 1.317942\n",
      "Epoch: 1459 \tTraining Loss: 0.139407 \tValidation Loss: 1.367991\n",
      "Epoch: 1460 \tTraining Loss: 0.064723 \tValidation Loss: 1.241522\n",
      "Epoch: 1461 \tTraining Loss: 0.045768 \tValidation Loss: 1.546796\n",
      "Epoch: 1462 \tTraining Loss: 0.043836 \tValidation Loss: 1.362259\n",
      "Epoch: 1463 \tTraining Loss: 0.024813 \tValidation Loss: 1.537794\n",
      "Epoch: 1464 \tTraining Loss: 0.018207 \tValidation Loss: 1.633282\n",
      "Epoch: 1465 \tTraining Loss: 0.032458 \tValidation Loss: 1.776502\n",
      "Epoch: 1466 \tTraining Loss: 0.042805 \tValidation Loss: 1.820462\n",
      "Epoch: 1467 \tTraining Loss: 0.019754 \tValidation Loss: 1.458965\n",
      "Epoch: 1468 \tTraining Loss: 0.003755 \tValidation Loss: 1.719807\n",
      "Epoch: 1469 \tTraining Loss: 0.006239 \tValidation Loss: 1.747302\n",
      "Epoch: 1470 \tTraining Loss: 0.004824 \tValidation Loss: 1.904904\n",
      "Epoch: 1471 \tTraining Loss: 0.001450 \tValidation Loss: 1.925098\n",
      "Epoch: 1472 \tTraining Loss: 0.007454 \tValidation Loss: 1.963766\n",
      "Epoch: 1473 \tTraining Loss: 0.013298 \tValidation Loss: 1.686165\n",
      "Epoch: 1474 \tTraining Loss: 0.235057 \tValidation Loss: 1.122708\n",
      "Epoch: 1475 \tTraining Loss: 0.337514 \tValidation Loss: 1.968577\n",
      "Epoch: 1476 \tTraining Loss: 0.110138 \tValidation Loss: 1.271236\n",
      "Epoch: 1477 \tTraining Loss: 0.077691 \tValidation Loss: 1.304417\n",
      "Epoch: 1478 \tTraining Loss: 0.055136 \tValidation Loss: 1.239751\n",
      "Epoch: 1479 \tTraining Loss: 0.086949 \tValidation Loss: 0.804700\n",
      "Epoch: 1480 \tTraining Loss: 0.111047 \tValidation Loss: 0.858510\n",
      "Epoch: 1481 \tTraining Loss: 0.084023 \tValidation Loss: 0.874592\n",
      "Epoch: 1482 \tTraining Loss: 0.017588 \tValidation Loss: 0.744157\n",
      "Epoch: 1483 \tTraining Loss: 0.026103 \tValidation Loss: 0.756298\n",
      "Epoch: 1484 \tTraining Loss: 0.010069 \tValidation Loss: 0.797780\n",
      "Epoch: 1485 \tTraining Loss: 0.009309 \tValidation Loss: 0.787335\n",
      "Epoch: 1486 \tTraining Loss: 0.006396 \tValidation Loss: 0.825192\n",
      "Epoch: 1487 \tTraining Loss: 0.007275 \tValidation Loss: 0.813264\n",
      "Epoch: 1488 \tTraining Loss: 0.007706 \tValidation Loss: 0.856667\n",
      "Epoch: 1489 \tTraining Loss: 0.006671 \tValidation Loss: 0.880826\n",
      "Epoch: 1490 \tTraining Loss: 0.003835 \tValidation Loss: 0.891579\n",
      "Epoch: 1491 \tTraining Loss: 0.003586 \tValidation Loss: 0.889701\n",
      "Epoch: 1492 \tTraining Loss: 0.003941 \tValidation Loss: 0.885813\n",
      "Epoch: 1493 \tTraining Loss: 0.001810 \tValidation Loss: 0.906459\n",
      "Epoch: 1494 \tTraining Loss: 0.005623 \tValidation Loss: 0.962021\n",
      "Epoch: 1495 \tTraining Loss: 0.006751 \tValidation Loss: 0.945251\n",
      "Epoch: 1496 \tTraining Loss: 0.004408 \tValidation Loss: 0.970252\n",
      "Epoch: 1497 \tTraining Loss: 0.010210 \tValidation Loss: 1.096075\n",
      "Epoch: 1498 \tTraining Loss: 0.015958 \tValidation Loss: 1.039018\n",
      "Epoch: 1499 \tTraining Loss: 0.003875 \tValidation Loss: 0.936066\n",
      "Epoch: 1500 \tTraining Loss: 0.004913 \tValidation Loss: 0.989400\n",
      "Epoch: 1501 \tTraining Loss: 0.005568 \tValidation Loss: 0.955148\n",
      "Epoch: 1502 \tTraining Loss: 0.001819 \tValidation Loss: 0.957645\n",
      "Epoch: 1503 \tTraining Loss: 0.002362 \tValidation Loss: 0.992983\n",
      "Epoch: 1504 \tTraining Loss: 0.004903 \tValidation Loss: 1.037526\n",
      "Epoch: 1505 \tTraining Loss: 0.004111 \tValidation Loss: 1.035251\n",
      "Epoch: 1506 \tTraining Loss: 0.003958 \tValidation Loss: 1.084469\n",
      "Epoch: 1507 \tTraining Loss: 0.001874 \tValidation Loss: 1.030670\n",
      "Epoch: 1508 \tTraining Loss: 0.002540 \tValidation Loss: 1.069057\n",
      "Epoch: 1509 \tTraining Loss: 0.002138 \tValidation Loss: 1.046514\n",
      "Epoch: 1510 \tTraining Loss: 0.006595 \tValidation Loss: 1.036232\n",
      "Epoch: 1511 \tTraining Loss: 0.001747 \tValidation Loss: 1.028952\n",
      "Epoch: 1512 \tTraining Loss: 0.001681 \tValidation Loss: 1.060162\n",
      "Epoch: 1513 \tTraining Loss: 0.001976 \tValidation Loss: 1.068995\n",
      "Epoch: 1514 \tTraining Loss: 0.001617 \tValidation Loss: 1.053195\n",
      "Epoch: 1515 \tTraining Loss: 0.002284 \tValidation Loss: 1.073147\n",
      "Epoch: 1516 \tTraining Loss: 0.002196 \tValidation Loss: 1.137941\n",
      "Epoch: 1517 \tTraining Loss: 0.006010 \tValidation Loss: 1.145878\n",
      "Epoch: 1518 \tTraining Loss: 0.003117 \tValidation Loss: 1.014783\n",
      "Epoch: 1519 \tTraining Loss: 0.001866 \tValidation Loss: 0.988039\n",
      "Epoch: 1520 \tTraining Loss: 0.001849 \tValidation Loss: 1.024986\n",
      "Epoch: 1521 \tTraining Loss: 0.003699 \tValidation Loss: 1.059131\n",
      "Epoch: 1522 \tTraining Loss: 0.001779 \tValidation Loss: 1.061733\n",
      "Epoch: 1523 \tTraining Loss: 0.003146 \tValidation Loss: 1.046481\n",
      "Epoch: 1524 \tTraining Loss: 0.002657 \tValidation Loss: 1.074506\n",
      "Epoch: 1525 \tTraining Loss: 0.000933 \tValidation Loss: 1.120334\n",
      "Epoch: 1526 \tTraining Loss: 0.001788 \tValidation Loss: 1.173725\n",
      "Epoch: 1527 \tTraining Loss: 0.001095 \tValidation Loss: 1.062562\n",
      "Epoch: 1528 \tTraining Loss: 0.001388 \tValidation Loss: 1.105938\n",
      "Epoch: 1529 \tTraining Loss: 0.001007 \tValidation Loss: 1.097431\n",
      "Epoch: 1530 \tTraining Loss: 0.001386 \tValidation Loss: 1.111401\n",
      "Epoch: 1531 \tTraining Loss: 0.001623 \tValidation Loss: 1.077130\n",
      "Epoch: 1532 \tTraining Loss: 0.002046 \tValidation Loss: 1.188637\n",
      "Epoch: 1533 \tTraining Loss: 0.002460 \tValidation Loss: 1.217864\n",
      "Epoch: 1534 \tTraining Loss: 0.002061 \tValidation Loss: 1.206904\n",
      "Epoch: 1535 \tTraining Loss: 0.000777 \tValidation Loss: 1.214795\n",
      "Epoch: 1536 \tTraining Loss: 0.001095 \tValidation Loss: 1.188787\n",
      "Epoch: 1537 \tTraining Loss: 0.000981 \tValidation Loss: 1.251716\n",
      "Epoch: 1538 \tTraining Loss: 0.001903 \tValidation Loss: 1.205704\n",
      "Epoch: 1539 \tTraining Loss: 0.000634 \tValidation Loss: 1.161494\n",
      "Epoch: 1540 \tTraining Loss: 0.001273 \tValidation Loss: 1.212010\n",
      "Epoch: 1541 \tTraining Loss: 0.001148 \tValidation Loss: 1.252170\n",
      "Epoch: 1542 \tTraining Loss: 0.001337 \tValidation Loss: 1.305947\n",
      "Epoch: 1543 \tTraining Loss: 0.000738 \tValidation Loss: 1.321741\n",
      "Epoch: 1544 \tTraining Loss: 0.003155 \tValidation Loss: 1.202485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1545 \tTraining Loss: 0.001590 \tValidation Loss: 0.952400\n",
      "Epoch: 1546 \tTraining Loss: 0.015200 \tValidation Loss: 1.069325\n",
      "Epoch: 1547 \tTraining Loss: 0.030021 \tValidation Loss: 1.164844\n",
      "Epoch: 1548 \tTraining Loss: 0.004997 \tValidation Loss: 1.154454\n",
      "Epoch: 1549 \tTraining Loss: 0.003558 \tValidation Loss: 1.220006\n",
      "Epoch: 1550 \tTraining Loss: 0.008898 \tValidation Loss: 1.262477\n",
      "Epoch: 1551 \tTraining Loss: 0.024670 \tValidation Loss: 1.110449\n",
      "Epoch: 1552 \tTraining Loss: 0.012896 \tValidation Loss: 0.902855\n",
      "Epoch: 1553 \tTraining Loss: 0.005050 \tValidation Loss: 0.943821\n",
      "Epoch: 1554 \tTraining Loss: 0.004743 \tValidation Loss: 1.065048\n",
      "Epoch: 1555 \tTraining Loss: 0.004542 \tValidation Loss: 1.239255\n",
      "Epoch: 1556 \tTraining Loss: 0.013072 \tValidation Loss: 0.853749\n",
      "Epoch: 1557 \tTraining Loss: 0.007127 \tValidation Loss: 0.923251\n",
      "Epoch: 1558 \tTraining Loss: 0.015362 \tValidation Loss: 1.368961\n",
      "Epoch: 1559 \tTraining Loss: 0.031867 \tValidation Loss: 1.069681\n",
      "Epoch: 1560 \tTraining Loss: 0.050075 \tValidation Loss: 1.145970\n",
      "Epoch: 1561 \tTraining Loss: 0.021300 \tValidation Loss: 0.977574\n",
      "Epoch: 1562 \tTraining Loss: 0.053934 \tValidation Loss: 1.524270\n",
      "Epoch: 1563 \tTraining Loss: 0.044661 \tValidation Loss: 1.199298\n",
      "Epoch: 1564 \tTraining Loss: 0.070820 \tValidation Loss: 1.801403\n",
      "Epoch: 1565 \tTraining Loss: 0.131333 \tValidation Loss: 2.092899\n",
      "Epoch: 1566 \tTraining Loss: 0.105166 \tValidation Loss: 0.911634\n",
      "Epoch: 1567 \tTraining Loss: 0.274652 \tValidation Loss: 1.942474\n",
      "Epoch: 1568 \tTraining Loss: 0.693821 \tValidation Loss: 1.002401\n",
      "Epoch: 1569 \tTraining Loss: 0.122119 \tValidation Loss: 0.695886\n",
      "Epoch: 1570 \tTraining Loss: 0.171644 \tValidation Loss: 1.229005\n",
      "Epoch: 1571 \tTraining Loss: 0.126232 \tValidation Loss: 0.705023\n",
      "Epoch: 1572 \tTraining Loss: 0.120857 \tValidation Loss: 0.707961\n",
      "Epoch: 1573 \tTraining Loss: 0.044060 \tValidation Loss: 0.686882\n",
      "Epoch: 1574 \tTraining Loss: 0.035500 \tValidation Loss: 1.147254\n",
      "Epoch: 1575 \tTraining Loss: 0.022280 \tValidation Loss: 1.086472\n",
      "Epoch: 1576 \tTraining Loss: 0.024654 \tValidation Loss: 1.210092\n",
      "Epoch: 1577 \tTraining Loss: 0.078719 \tValidation Loss: 0.976339\n",
      "Epoch: 1578 \tTraining Loss: 0.124862 \tValidation Loss: 1.013813\n",
      "Epoch: 1579 \tTraining Loss: 0.237276 \tValidation Loss: 0.815779\n",
      "Epoch: 1580 \tTraining Loss: 0.095221 \tValidation Loss: 1.063947\n",
      "Epoch: 1581 \tTraining Loss: 0.040250 \tValidation Loss: 0.854525\n",
      "Epoch: 1582 \tTraining Loss: 0.013446 \tValidation Loss: 0.889850\n",
      "Epoch: 1583 \tTraining Loss: 0.028602 \tValidation Loss: 0.998289\n",
      "Epoch: 1584 \tTraining Loss: 0.012771 \tValidation Loss: 1.052984\n",
      "Epoch: 1585 \tTraining Loss: 0.013764 \tValidation Loss: 0.994121\n",
      "Epoch: 1586 \tTraining Loss: 0.021873 \tValidation Loss: 1.058619\n",
      "Epoch: 1587 \tTraining Loss: 0.008993 \tValidation Loss: 0.998371\n",
      "Epoch: 1588 \tTraining Loss: 0.004513 \tValidation Loss: 0.919929\n",
      "Epoch: 1589 \tTraining Loss: 0.011638 \tValidation Loss: 0.896443\n",
      "Epoch: 1590 \tTraining Loss: 0.008197 \tValidation Loss: 1.053462\n",
      "Epoch: 1591 \tTraining Loss: 0.006596 \tValidation Loss: 0.921736\n",
      "Epoch: 1592 \tTraining Loss: 0.006194 \tValidation Loss: 0.953857\n",
      "Epoch: 1593 \tTraining Loss: 0.003436 \tValidation Loss: 0.968131\n",
      "Epoch: 1594 \tTraining Loss: 0.002407 \tValidation Loss: 0.940078\n",
      "Epoch: 1595 \tTraining Loss: 0.003815 \tValidation Loss: 0.982413\n",
      "Epoch: 1596 \tTraining Loss: 0.010895 \tValidation Loss: 1.163550\n",
      "Epoch: 1597 \tTraining Loss: 0.005562 \tValidation Loss: 1.096179\n",
      "Epoch: 1598 \tTraining Loss: 0.003419 \tValidation Loss: 1.044793\n",
      "Epoch: 1599 \tTraining Loss: 0.004689 \tValidation Loss: 1.217899\n",
      "Epoch: 1600 \tTraining Loss: 0.005698 \tValidation Loss: 1.046036\n",
      "Epoch: 1601 \tTraining Loss: 0.001594 \tValidation Loss: 0.988961\n",
      "Epoch: 1602 \tTraining Loss: 0.008974 \tValidation Loss: 1.315496\n",
      "Epoch: 1603 \tTraining Loss: 0.007496 \tValidation Loss: 1.419394\n",
      "Epoch: 1604 \tTraining Loss: 0.084131 \tValidation Loss: 1.923751\n",
      "Epoch: 1605 \tTraining Loss: 0.322481 \tValidation Loss: 1.245325\n",
      "Epoch: 1606 \tTraining Loss: 0.294895 \tValidation Loss: 1.340814\n",
      "Epoch: 1607 \tTraining Loss: 0.177807 \tValidation Loss: 0.973040\n",
      "Epoch: 1608 \tTraining Loss: 0.079971 \tValidation Loss: 0.991868\n",
      "Epoch: 1609 \tTraining Loss: 0.048294 \tValidation Loss: 0.721910\n",
      "Epoch: 1610 \tTraining Loss: 0.087457 \tValidation Loss: 1.249022\n",
      "Epoch: 1611 \tTraining Loss: 0.066101 \tValidation Loss: 1.041472\n",
      "Epoch: 1612 \tTraining Loss: 0.282142 \tValidation Loss: 2.406827\n",
      "Epoch: 1613 \tTraining Loss: 0.394521 \tValidation Loss: 0.895355\n",
      "Epoch: 1614 \tTraining Loss: 0.180814 \tValidation Loss: 0.875230\n",
      "Epoch: 1615 \tTraining Loss: 0.139708 \tValidation Loss: 0.884169\n",
      "Epoch: 1616 \tTraining Loss: 0.037411 \tValidation Loss: 0.874325\n",
      "Epoch: 1617 \tTraining Loss: 0.035691 \tValidation Loss: 0.890955\n",
      "Epoch: 1618 \tTraining Loss: 0.048503 \tValidation Loss: 1.345790\n",
      "Epoch: 1619 \tTraining Loss: 0.074248 \tValidation Loss: 0.726946\n",
      "Epoch: 1620 \tTraining Loss: 0.105346 \tValidation Loss: 0.653883\n",
      "Epoch: 1621 \tTraining Loss: 0.043657 \tValidation Loss: 0.750413\n",
      "Epoch: 1622 \tTraining Loss: 0.072220 \tValidation Loss: 0.774289\n",
      "Epoch: 1623 \tTraining Loss: 0.034122 \tValidation Loss: 0.890412\n",
      "Epoch: 1624 \tTraining Loss: 0.024301 \tValidation Loss: 1.042475\n",
      "Epoch: 1625 \tTraining Loss: 0.008920 \tValidation Loss: 0.978166\n",
      "Epoch: 1626 \tTraining Loss: 0.007614 \tValidation Loss: 0.978691\n",
      "Epoch: 1627 \tTraining Loss: 0.015561 \tValidation Loss: 1.187616\n",
      "Epoch: 1628 \tTraining Loss: 0.004431 \tValidation Loss: 1.222402\n",
      "Epoch: 1629 \tTraining Loss: 0.008024 \tValidation Loss: 1.128901\n",
      "Epoch: 1630 \tTraining Loss: 0.005456 \tValidation Loss: 1.150024\n",
      "Epoch: 1631 \tTraining Loss: 0.007301 \tValidation Loss: 1.032958\n",
      "Epoch: 1632 \tTraining Loss: 0.008334 \tValidation Loss: 0.988230\n",
      "Epoch: 1633 \tTraining Loss: 0.004526 \tValidation Loss: 1.022490\n",
      "Epoch: 1634 \tTraining Loss: 0.009225 \tValidation Loss: 1.113181\n",
      "Epoch: 1635 \tTraining Loss: 0.003908 \tValidation Loss: 1.156369\n",
      "Epoch: 1636 \tTraining Loss: 0.002484 \tValidation Loss: 1.142031\n",
      "Epoch: 1637 \tTraining Loss: 0.004060 \tValidation Loss: 1.004654\n",
      "Epoch: 1638 \tTraining Loss: 0.006671 \tValidation Loss: 0.930374\n",
      "Epoch: 1639 \tTraining Loss: 0.021127 \tValidation Loss: 0.727305\n",
      "Epoch: 1640 \tTraining Loss: 0.031313 \tValidation Loss: 0.701076\n",
      "Epoch: 1641 \tTraining Loss: 0.013757 \tValidation Loss: 0.907330\n",
      "Epoch: 1642 \tTraining Loss: 0.005894 \tValidation Loss: 0.990980\n",
      "Epoch: 1643 \tTraining Loss: 0.012101 \tValidation Loss: 1.214993\n",
      "Epoch: 1644 \tTraining Loss: 0.003204 \tValidation Loss: 1.186463\n",
      "Epoch: 1645 \tTraining Loss: 0.002524 \tValidation Loss: 1.126215\n",
      "Epoch: 1646 \tTraining Loss: 0.003750 \tValidation Loss: 1.149951\n",
      "Epoch: 1647 \tTraining Loss: 0.001490 \tValidation Loss: 1.073450\n",
      "Epoch: 1648 \tTraining Loss: 0.007072 \tValidation Loss: 0.898430\n",
      "Epoch: 1649 \tTraining Loss: 0.002443 \tValidation Loss: 0.847184\n",
      "Epoch: 1650 \tTraining Loss: 0.002649 \tValidation Loss: 0.917209\n",
      "Epoch: 1651 \tTraining Loss: 0.009096 \tValidation Loss: 1.421417\n",
      "Epoch: 1652 \tTraining Loss: 0.006026 \tValidation Loss: 1.338615\n",
      "Epoch: 1653 \tTraining Loss: 0.002773 \tValidation Loss: 0.934692\n",
      "Epoch: 1654 \tTraining Loss: 0.002935 \tValidation Loss: 1.094937\n",
      "Epoch: 1655 \tTraining Loss: 0.003558 \tValidation Loss: 0.948522\n",
      "Epoch: 1656 \tTraining Loss: 0.001661 \tValidation Loss: 1.003333\n",
      "Epoch: 1657 \tTraining Loss: 0.002794 \tValidation Loss: 1.111481\n",
      "Epoch: 1658 \tTraining Loss: 0.003744 \tValidation Loss: 1.146095\n",
      "Epoch: 1659 \tTraining Loss: 0.002228 \tValidation Loss: 1.293122\n",
      "Epoch: 1660 \tTraining Loss: 0.001118 \tValidation Loss: 1.404308\n",
      "Epoch: 1661 \tTraining Loss: 0.001656 \tValidation Loss: 1.413470\n",
      "Epoch: 1662 \tTraining Loss: 0.002068 \tValidation Loss: 1.291756\n",
      "Epoch: 1663 \tTraining Loss: 0.001264 \tValidation Loss: 1.279823\n",
      "Epoch: 1664 \tTraining Loss: 0.001333 \tValidation Loss: 1.270830\n",
      "Epoch: 1665 \tTraining Loss: 0.001208 \tValidation Loss: 1.272559\n",
      "Epoch: 1666 \tTraining Loss: 0.000824 \tValidation Loss: 1.349175\n",
      "Epoch: 1667 \tTraining Loss: 0.002356 \tValidation Loss: 1.284455\n",
      "Epoch: 1668 \tTraining Loss: 0.001205 \tValidation Loss: 1.276386\n",
      "Epoch: 1669 \tTraining Loss: 0.001815 \tValidation Loss: 1.272298\n",
      "Epoch: 1670 \tTraining Loss: 0.001533 \tValidation Loss: 1.285795\n",
      "Epoch: 1671 \tTraining Loss: 0.001311 \tValidation Loss: 1.363576\n",
      "Epoch: 1672 \tTraining Loss: 0.000531 \tValidation Loss: 1.422873\n",
      "Epoch: 1673 \tTraining Loss: 0.000704 \tValidation Loss: 1.478140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1674 \tTraining Loss: 0.001075 \tValidation Loss: 1.485319\n",
      "Epoch: 1675 \tTraining Loss: 0.001624 \tValidation Loss: 1.561257\n",
      "Epoch: 1676 \tTraining Loss: 0.004495 \tValidation Loss: 1.823754\n",
      "Epoch: 1677 \tTraining Loss: 0.093515 \tValidation Loss: 1.244599\n",
      "Epoch: 1678 \tTraining Loss: 0.062977 \tValidation Loss: 1.790861\n",
      "Epoch: 1679 \tTraining Loss: 0.097852 \tValidation Loss: 1.218247\n",
      "Epoch: 1680 \tTraining Loss: 0.030056 \tValidation Loss: 0.974011\n",
      "Epoch: 1681 \tTraining Loss: 0.005678 \tValidation Loss: 0.981896\n",
      "Epoch: 1682 \tTraining Loss: 0.006512 \tValidation Loss: 0.940525\n",
      "Epoch: 1683 \tTraining Loss: 0.007148 \tValidation Loss: 0.956343\n",
      "Epoch: 1684 \tTraining Loss: 0.005341 \tValidation Loss: 0.966827\n",
      "Epoch: 1685 \tTraining Loss: 0.004092 \tValidation Loss: 1.033384\n",
      "Epoch: 1686 \tTraining Loss: 0.003526 \tValidation Loss: 1.029178\n",
      "Epoch: 1687 \tTraining Loss: 0.004582 \tValidation Loss: 1.061536\n",
      "Epoch: 1688 \tTraining Loss: 0.003786 \tValidation Loss: 1.015753\n",
      "Epoch: 1689 \tTraining Loss: 0.001686 \tValidation Loss: 1.017065\n",
      "Epoch: 1690 \tTraining Loss: 0.001756 \tValidation Loss: 1.017757\n",
      "Epoch: 1691 \tTraining Loss: 0.001893 \tValidation Loss: 1.089656\n",
      "Epoch: 1692 \tTraining Loss: 0.002874 \tValidation Loss: 1.035304\n",
      "Epoch: 1693 \tTraining Loss: 0.002514 \tValidation Loss: 1.042526\n",
      "Epoch: 1694 \tTraining Loss: 0.009053 \tValidation Loss: 0.992626\n",
      "Epoch: 1695 \tTraining Loss: 0.004147 \tValidation Loss: 1.138134\n",
      "Epoch: 1696 \tTraining Loss: 0.002521 \tValidation Loss: 1.126919\n",
      "Epoch: 1697 \tTraining Loss: 0.001584 \tValidation Loss: 1.129145\n",
      "Epoch: 1698 \tTraining Loss: 0.002015 \tValidation Loss: 1.166460\n",
      "Epoch: 1699 \tTraining Loss: 0.002642 \tValidation Loss: 1.201453\n",
      "Epoch: 1700 \tTraining Loss: 0.001729 \tValidation Loss: 1.147022\n",
      "Epoch: 1701 \tTraining Loss: 0.001104 \tValidation Loss: 1.187257\n",
      "Epoch: 1702 \tTraining Loss: 0.002333 \tValidation Loss: 1.145384\n",
      "Epoch: 1703 \tTraining Loss: 0.004564 \tValidation Loss: 1.162723\n",
      "Epoch: 1704 \tTraining Loss: 0.004409 \tValidation Loss: 1.249994\n",
      "Epoch: 1705 \tTraining Loss: 0.003150 \tValidation Loss: 1.019980\n",
      "Epoch: 1706 \tTraining Loss: 0.002177 \tValidation Loss: 1.069787\n",
      "Epoch: 1707 \tTraining Loss: 0.002079 \tValidation Loss: 1.167065\n",
      "Epoch: 1708 \tTraining Loss: 0.002237 \tValidation Loss: 1.094517\n",
      "Epoch: 1709 \tTraining Loss: 0.001703 \tValidation Loss: 1.150701\n",
      "Epoch: 1710 \tTraining Loss: 0.001637 \tValidation Loss: 1.155902\n",
      "Epoch: 1711 \tTraining Loss: 0.001564 \tValidation Loss: 1.136313\n",
      "Epoch: 1712 \tTraining Loss: 0.001873 \tValidation Loss: 1.140084\n",
      "Epoch: 1713 \tTraining Loss: 0.001914 \tValidation Loss: 1.111412\n",
      "Epoch: 1714 \tTraining Loss: 0.001902 \tValidation Loss: 1.143862\n",
      "Epoch: 1715 \tTraining Loss: 0.000539 \tValidation Loss: 1.122025\n",
      "Epoch: 1716 \tTraining Loss: 0.004776 \tValidation Loss: 1.015784\n",
      "Epoch: 1717 \tTraining Loss: 0.001529 \tValidation Loss: 1.047462\n",
      "Epoch: 1718 \tTraining Loss: 0.001069 \tValidation Loss: 1.101088\n",
      "Epoch: 1719 \tTraining Loss: 0.002007 \tValidation Loss: 1.133957\n",
      "Epoch: 1720 \tTraining Loss: 0.007112 \tValidation Loss: 1.648070\n",
      "Epoch: 1721 \tTraining Loss: 0.072264 \tValidation Loss: 1.220943\n",
      "Epoch: 1722 \tTraining Loss: 0.208951 \tValidation Loss: 1.383481\n",
      "Epoch: 1723 \tTraining Loss: 0.139898 \tValidation Loss: 1.323715\n",
      "Epoch: 1724 \tTraining Loss: 0.095373 \tValidation Loss: 1.285764\n",
      "Epoch: 1725 \tTraining Loss: 0.267405 \tValidation Loss: 0.850105\n",
      "Epoch: 1726 \tTraining Loss: 0.145458 \tValidation Loss: 0.855305\n",
      "Epoch: 1727 \tTraining Loss: 0.078944 \tValidation Loss: 1.065444\n",
      "Epoch: 1728 \tTraining Loss: 0.044411 \tValidation Loss: 1.382333\n",
      "Epoch: 1729 \tTraining Loss: 0.022284 \tValidation Loss: 1.181069\n",
      "Epoch: 1730 \tTraining Loss: 0.018710 \tValidation Loss: 1.088511\n",
      "Epoch: 1731 \tTraining Loss: 0.005562 \tValidation Loss: 0.964479\n",
      "Epoch: 1732 \tTraining Loss: 0.010118 \tValidation Loss: 1.045170\n",
      "Epoch: 1733 \tTraining Loss: 0.007351 \tValidation Loss: 1.005794\n",
      "Epoch: 1734 \tTraining Loss: 0.004617 \tValidation Loss: 1.010869\n",
      "Epoch: 1735 \tTraining Loss: 0.004459 \tValidation Loss: 1.107020\n",
      "Epoch: 1736 \tTraining Loss: 0.002631 \tValidation Loss: 1.035151\n",
      "Epoch: 1737 \tTraining Loss: 0.004069 \tValidation Loss: 1.110242\n",
      "Epoch: 1738 \tTraining Loss: 0.003298 \tValidation Loss: 0.944626\n",
      "Epoch: 1739 \tTraining Loss: 0.004182 \tValidation Loss: 0.986277\n",
      "Epoch: 1740 \tTraining Loss: 0.005017 \tValidation Loss: 1.032351\n",
      "Epoch: 1741 \tTraining Loss: 0.001986 \tValidation Loss: 0.954316\n",
      "Epoch: 1742 \tTraining Loss: 0.003025 \tValidation Loss: 1.143438\n",
      "Epoch: 1743 \tTraining Loss: 0.001460 \tValidation Loss: 1.156816\n",
      "Epoch: 1744 \tTraining Loss: 0.002173 \tValidation Loss: 1.274271\n",
      "Epoch: 1745 \tTraining Loss: 0.001634 \tValidation Loss: 1.072036\n",
      "Epoch: 1746 \tTraining Loss: 0.002186 \tValidation Loss: 1.096400\n",
      "Epoch: 1747 \tTraining Loss: 0.002932 \tValidation Loss: 1.148538\n",
      "Epoch: 1748 \tTraining Loss: 0.001587 \tValidation Loss: 1.221550\n",
      "Epoch: 1749 \tTraining Loss: 0.004892 \tValidation Loss: 1.426869\n",
      "Epoch: 1750 \tTraining Loss: 0.026856 \tValidation Loss: 0.822432\n",
      "Epoch: 1751 \tTraining Loss: 0.018662 \tValidation Loss: 0.845516\n",
      "Epoch: 1752 \tTraining Loss: 0.174746 \tValidation Loss: 1.715819\n",
      "Epoch: 1753 \tTraining Loss: 0.129444 \tValidation Loss: 1.084837\n",
      "Epoch: 1754 \tTraining Loss: 0.280617 \tValidation Loss: 1.672554\n",
      "Epoch: 1755 \tTraining Loss: 0.396286 \tValidation Loss: 0.712984\n",
      "Epoch: 1756 \tTraining Loss: 0.110650 \tValidation Loss: 1.035995\n",
      "Epoch: 1757 \tTraining Loss: 0.081420 \tValidation Loss: 1.200513\n",
      "Epoch: 1758 \tTraining Loss: 0.088853 \tValidation Loss: 0.996737\n",
      "Epoch: 1759 \tTraining Loss: 0.099853 \tValidation Loss: 1.109911\n",
      "Epoch: 1760 \tTraining Loss: 0.149097 \tValidation Loss: 1.095530\n",
      "Epoch: 1761 \tTraining Loss: 0.078992 \tValidation Loss: 0.820391\n",
      "Epoch: 1762 \tTraining Loss: 0.066561 \tValidation Loss: 0.784201\n",
      "Epoch: 1763 \tTraining Loss: 0.022943 \tValidation Loss: 0.856523\n",
      "Epoch: 1764 \tTraining Loss: 0.023996 \tValidation Loss: 0.889613\n",
      "Epoch: 1765 \tTraining Loss: 0.012326 \tValidation Loss: 0.932573\n",
      "Epoch: 1766 \tTraining Loss: 0.008320 \tValidation Loss: 0.953384\n",
      "Epoch: 1767 \tTraining Loss: 0.006872 \tValidation Loss: 0.928185\n",
      "Epoch: 1768 \tTraining Loss: 0.009466 \tValidation Loss: 0.843765\n",
      "Epoch: 1769 \tTraining Loss: 0.008046 \tValidation Loss: 0.836333\n",
      "Epoch: 1770 \tTraining Loss: 0.003428 \tValidation Loss: 0.869309\n",
      "Epoch: 1771 \tTraining Loss: 0.008338 \tValidation Loss: 0.883869\n",
      "Epoch: 1772 \tTraining Loss: 0.004719 \tValidation Loss: 0.871926\n",
      "Epoch: 1773 \tTraining Loss: 0.002502 \tValidation Loss: 0.896738\n",
      "Epoch: 1774 \tTraining Loss: 0.006083 \tValidation Loss: 0.930133\n",
      "Epoch: 1775 \tTraining Loss: 0.005685 \tValidation Loss: 0.830089\n",
      "Epoch: 1776 \tTraining Loss: 0.002408 \tValidation Loss: 0.883004\n",
      "Epoch: 1777 \tTraining Loss: 0.008173 \tValidation Loss: 0.879718\n",
      "Epoch: 1778 \tTraining Loss: 0.003312 \tValidation Loss: 0.882835\n",
      "Epoch: 1779 \tTraining Loss: 0.006725 \tValidation Loss: 0.908436\n",
      "Epoch: 1780 \tTraining Loss: 0.003540 \tValidation Loss: 0.869962\n",
      "Epoch: 1781 \tTraining Loss: 0.017950 \tValidation Loss: 1.288180\n",
      "Epoch: 1782 \tTraining Loss: 0.048392 \tValidation Loss: 1.271041\n",
      "Epoch: 1783 \tTraining Loss: 0.062011 \tValidation Loss: 1.269285\n",
      "Epoch: 1784 \tTraining Loss: 0.134161 \tValidation Loss: 0.955006\n",
      "Epoch: 1785 \tTraining Loss: 0.124826 \tValidation Loss: 0.847519\n",
      "Epoch: 1786 \tTraining Loss: 0.043884 \tValidation Loss: 1.120413\n",
      "Epoch: 1787 \tTraining Loss: 0.024671 \tValidation Loss: 1.095078\n",
      "Epoch: 1788 \tTraining Loss: 0.016143 \tValidation Loss: 1.052869\n",
      "Epoch: 1789 \tTraining Loss: 0.010900 \tValidation Loss: 1.089674\n",
      "Epoch: 1790 \tTraining Loss: 0.049007 \tValidation Loss: 1.095943\n",
      "Epoch: 1791 \tTraining Loss: 0.033328 \tValidation Loss: 0.653110\n",
      "Epoch: 1792 \tTraining Loss: 0.130681 \tValidation Loss: 1.656398\n",
      "Epoch: 1793 \tTraining Loss: 0.081481 \tValidation Loss: 1.328862\n",
      "Epoch: 1794 \tTraining Loss: 0.087679 \tValidation Loss: 0.829689\n",
      "Epoch: 1795 \tTraining Loss: 0.024035 \tValidation Loss: 0.721254\n",
      "Epoch: 1796 \tTraining Loss: 0.008876 \tValidation Loss: 0.528926\n",
      "Epoch: 1797 \tTraining Loss: 0.018937 \tValidation Loss: 0.609400\n",
      "Epoch: 1798 \tTraining Loss: 0.008102 \tValidation Loss: 0.645021\n",
      "Epoch: 1799 \tTraining Loss: 0.012499 \tValidation Loss: 0.704549\n",
      "Epoch: 1800 \tTraining Loss: 0.005612 \tValidation Loss: 0.540164\n",
      "Epoch: 1801 \tTraining Loss: 0.004444 \tValidation Loss: 0.619136\n",
      "Epoch: 1802 \tTraining Loss: 0.006650 \tValidation Loss: 0.646430\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1803 \tTraining Loss: 0.004500 \tValidation Loss: 0.635912\n",
      "Epoch: 1804 \tTraining Loss: 0.007433 \tValidation Loss: 0.662658\n",
      "Epoch: 1805 \tTraining Loss: 0.006903 \tValidation Loss: 0.730000\n",
      "Epoch: 1806 \tTraining Loss: 0.008613 \tValidation Loss: 0.834199\n",
      "Epoch: 1807 \tTraining Loss: 0.003990 \tValidation Loss: 0.982901\n",
      "Epoch: 1808 \tTraining Loss: 0.012377 \tValidation Loss: 1.031898\n",
      "Epoch: 1809 \tTraining Loss: 0.002164 \tValidation Loss: 0.957509\n",
      "Epoch: 1810 \tTraining Loss: 0.002899 \tValidation Loss: 0.927239\n",
      "Epoch: 1811 \tTraining Loss: 0.002684 \tValidation Loss: 1.004484\n",
      "Epoch: 1812 \tTraining Loss: 0.033619 \tValidation Loss: 1.388668\n",
      "Epoch: 1813 \tTraining Loss: 0.014776 \tValidation Loss: 1.036290\n",
      "Epoch: 1814 \tTraining Loss: 0.022016 \tValidation Loss: 1.065940\n",
      "Epoch: 1815 \tTraining Loss: 0.030313 \tValidation Loss: 1.072626\n",
      "Epoch: 1816 \tTraining Loss: 0.005487 \tValidation Loss: 1.182973\n",
      "Epoch: 1817 \tTraining Loss: 0.100599 \tValidation Loss: 0.590297\n",
      "Epoch: 1818 \tTraining Loss: 0.142283 \tValidation Loss: 0.537083\n",
      "Epoch: 1819 \tTraining Loss: 0.072267 \tValidation Loss: 1.309866\n",
      "Epoch: 1820 \tTraining Loss: 0.050629 \tValidation Loss: 0.682320\n",
      "Epoch: 1821 \tTraining Loss: 0.025843 \tValidation Loss: 1.044717\n",
      "Epoch: 1822 \tTraining Loss: 0.013850 \tValidation Loss: 1.238476\n",
      "Epoch: 1823 \tTraining Loss: 0.003509 \tValidation Loss: 0.971521\n",
      "Epoch: 1824 \tTraining Loss: 0.013504 \tValidation Loss: 1.221421\n",
      "Epoch: 1825 \tTraining Loss: 0.007876 \tValidation Loss: 1.232400\n",
      "Epoch: 1826 \tTraining Loss: 0.006708 \tValidation Loss: 1.157190\n",
      "Epoch: 1827 \tTraining Loss: 0.004706 \tValidation Loss: 1.203182\n",
      "Epoch: 1828 \tTraining Loss: 0.002350 \tValidation Loss: 1.160890\n",
      "Epoch: 1829 \tTraining Loss: 0.002738 \tValidation Loss: 1.131087\n",
      "Epoch: 1830 \tTraining Loss: 0.002316 \tValidation Loss: 1.154360\n",
      "Epoch: 1831 \tTraining Loss: 0.006534 \tValidation Loss: 1.223836\n",
      "Epoch: 1832 \tTraining Loss: 0.007092 \tValidation Loss: 1.191874\n",
      "Epoch: 1833 \tTraining Loss: 0.002193 \tValidation Loss: 1.081299\n",
      "Epoch: 1834 \tTraining Loss: 0.002653 \tValidation Loss: 1.078191\n",
      "Epoch: 1835 \tTraining Loss: 0.001155 \tValidation Loss: 1.098575\n",
      "Epoch: 1836 \tTraining Loss: 0.002890 \tValidation Loss: 1.066730\n",
      "Epoch: 1837 \tTraining Loss: 0.001436 \tValidation Loss: 1.078413\n",
      "Epoch: 1838 \tTraining Loss: 0.002382 \tValidation Loss: 1.099611\n",
      "Epoch: 1839 \tTraining Loss: 0.001858 \tValidation Loss: 1.080325\n",
      "Epoch: 1840 \tTraining Loss: 0.001477 \tValidation Loss: 1.124406\n",
      "Epoch: 1841 \tTraining Loss: 0.001299 \tValidation Loss: 1.156709\n",
      "Epoch: 1842 \tTraining Loss: 0.002079 \tValidation Loss: 1.143650\n",
      "Epoch: 1843 \tTraining Loss: 0.000756 \tValidation Loss: 1.119666\n",
      "Epoch: 1844 \tTraining Loss: 0.011135 \tValidation Loss: 1.218231\n",
      "Epoch: 1845 \tTraining Loss: 0.032154 \tValidation Loss: 1.508065\n",
      "Epoch: 1846 \tTraining Loss: 0.034346 \tValidation Loss: 1.023002\n",
      "Epoch: 1847 \tTraining Loss: 0.019359 \tValidation Loss: 0.887140\n",
      "Epoch: 1848 \tTraining Loss: 0.006833 \tValidation Loss: 0.893725\n",
      "Epoch: 1849 \tTraining Loss: 0.027315 \tValidation Loss: 0.799226\n",
      "Epoch: 1850 \tTraining Loss: 0.013496 \tValidation Loss: 0.875444\n",
      "Epoch: 1851 \tTraining Loss: 0.003135 \tValidation Loss: 0.956467\n",
      "Epoch: 1852 \tTraining Loss: 0.002791 \tValidation Loss: 0.990535\n",
      "Epoch: 1853 \tTraining Loss: 0.005750 \tValidation Loss: 0.985461\n",
      "Epoch: 1854 \tTraining Loss: 0.004512 \tValidation Loss: 0.943215\n",
      "Epoch: 1855 \tTraining Loss: 0.002850 \tValidation Loss: 0.980848\n",
      "Epoch: 1856 \tTraining Loss: 0.001828 \tValidation Loss: 0.984035\n",
      "Epoch: 1857 \tTraining Loss: 0.003162 \tValidation Loss: 1.011677\n",
      "Epoch: 1858 \tTraining Loss: 0.002890 \tValidation Loss: 1.028801\n",
      "Epoch: 1859 \tTraining Loss: 0.002061 \tValidation Loss: 1.065307\n",
      "Epoch: 1860 \tTraining Loss: 0.004192 \tValidation Loss: 0.964825\n",
      "Epoch: 1861 \tTraining Loss: 0.002142 \tValidation Loss: 0.944673\n",
      "Epoch: 1862 \tTraining Loss: 0.002549 \tValidation Loss: 0.986523\n",
      "Epoch: 1863 \tTraining Loss: 0.003263 \tValidation Loss: 0.925039\n",
      "Epoch: 1864 \tTraining Loss: 0.001707 \tValidation Loss: 0.939598\n",
      "Epoch: 1865 \tTraining Loss: 0.001603 \tValidation Loss: 0.937945\n",
      "Epoch: 1866 \tTraining Loss: 0.001930 \tValidation Loss: 0.983270\n",
      "Epoch: 1867 \tTraining Loss: 0.001621 \tValidation Loss: 0.994150\n",
      "Epoch: 1868 \tTraining Loss: 0.001164 \tValidation Loss: 0.977381\n",
      "Epoch: 1869 \tTraining Loss: 0.001528 \tValidation Loss: 1.015934\n",
      "Epoch: 1870 \tTraining Loss: 0.000929 \tValidation Loss: 1.018065\n",
      "Epoch: 1871 \tTraining Loss: 0.000632 \tValidation Loss: 1.046656\n",
      "Epoch: 1872 \tTraining Loss: 0.000954 \tValidation Loss: 1.090458\n",
      "Epoch: 1873 \tTraining Loss: 0.001361 \tValidation Loss: 1.161416\n",
      "Epoch: 1874 \tTraining Loss: 0.001192 \tValidation Loss: 1.045286\n",
      "Epoch: 1875 \tTraining Loss: 0.001017 \tValidation Loss: 1.061903\n",
      "Epoch: 1876 \tTraining Loss: 0.001027 \tValidation Loss: 1.121660\n",
      "Epoch: 1877 \tTraining Loss: 0.000666 \tValidation Loss: 1.074484\n",
      "Epoch: 1878 \tTraining Loss: 0.000847 \tValidation Loss: 1.051317\n",
      "Epoch: 1879 \tTraining Loss: 0.002989 \tValidation Loss: 0.987849\n",
      "Epoch: 1880 \tTraining Loss: 0.000624 \tValidation Loss: 0.999447\n",
      "Epoch: 1881 \tTraining Loss: 0.001590 \tValidation Loss: 1.034077\n",
      "Epoch: 1882 \tTraining Loss: 0.003318 \tValidation Loss: 0.963085\n",
      "Epoch: 1883 \tTraining Loss: 0.001773 \tValidation Loss: 0.972322\n",
      "Epoch: 1884 \tTraining Loss: 0.001012 \tValidation Loss: 0.942527\n",
      "Epoch: 1885 \tTraining Loss: 0.002644 \tValidation Loss: 1.127791\n",
      "Epoch: 1886 \tTraining Loss: 0.001092 \tValidation Loss: 1.213470\n",
      "Epoch: 1887 \tTraining Loss: 0.001781 \tValidation Loss: 1.118522\n",
      "Epoch: 1888 \tTraining Loss: 0.001844 \tValidation Loss: 1.037860\n",
      "Epoch: 1889 \tTraining Loss: 0.000891 \tValidation Loss: 1.026786\n",
      "Epoch: 1890 \tTraining Loss: 0.000862 \tValidation Loss: 1.081769\n",
      "Epoch: 1891 \tTraining Loss: 0.000671 \tValidation Loss: 1.105327\n",
      "Epoch: 1892 \tTraining Loss: 0.001876 \tValidation Loss: 0.978933\n",
      "Epoch: 1893 \tTraining Loss: 0.001238 \tValidation Loss: 1.126483\n",
      "Epoch: 1894 \tTraining Loss: 0.000595 \tValidation Loss: 1.159091\n",
      "Epoch: 1895 \tTraining Loss: 0.000759 \tValidation Loss: 1.202228\n",
      "Epoch: 1896 \tTraining Loss: 0.001190 \tValidation Loss: 1.188837\n",
      "Epoch: 1897 \tTraining Loss: 0.000786 \tValidation Loss: 1.123515\n",
      "Epoch: 1898 \tTraining Loss: 0.001530 \tValidation Loss: 1.095604\n",
      "Epoch: 1899 \tTraining Loss: 0.000910 \tValidation Loss: 1.040468\n",
      "Epoch: 1900 \tTraining Loss: 0.000778 \tValidation Loss: 1.064353\n",
      "Epoch: 1901 \tTraining Loss: 0.020312 \tValidation Loss: 1.076994\n",
      "Epoch: 1902 \tTraining Loss: 0.013387 \tValidation Loss: 0.877511\n",
      "Epoch: 1903 \tTraining Loss: 0.107327 \tValidation Loss: 0.698446\n",
      "Epoch: 1904 \tTraining Loss: 0.283312 \tValidation Loss: 2.075527\n",
      "Epoch: 1905 \tTraining Loss: 0.263653 \tValidation Loss: 1.732324\n",
      "Epoch: 1906 \tTraining Loss: 0.328088 \tValidation Loss: 2.168052\n",
      "Epoch: 1907 \tTraining Loss: 0.405562 \tValidation Loss: 0.944191\n",
      "Epoch: 1908 \tTraining Loss: 0.278429 \tValidation Loss: 1.234093\n",
      "Epoch: 1909 \tTraining Loss: 0.186339 \tValidation Loss: 0.907573\n",
      "Epoch: 1910 \tTraining Loss: 0.096587 \tValidation Loss: 0.876558\n",
      "Epoch: 1911 \tTraining Loss: 0.046428 \tValidation Loss: 0.901592\n",
      "Epoch: 1912 \tTraining Loss: 0.029485 \tValidation Loss: 0.814589\n",
      "Epoch: 1913 \tTraining Loss: 0.021205 \tValidation Loss: 0.861955\n",
      "Epoch: 1914 \tTraining Loss: 0.010584 \tValidation Loss: 1.014155\n",
      "Epoch: 1915 \tTraining Loss: 0.015102 \tValidation Loss: 1.082477\n",
      "Epoch: 1916 \tTraining Loss: 0.019201 \tValidation Loss: 1.183663\n",
      "Epoch: 1917 \tTraining Loss: 0.018858 \tValidation Loss: 1.131118\n",
      "Epoch: 1918 \tTraining Loss: 0.034905 \tValidation Loss: 0.950743\n",
      "Epoch: 1919 \tTraining Loss: 0.190790 \tValidation Loss: 1.308791\n",
      "Epoch: 1920 \tTraining Loss: 0.192201 \tValidation Loss: 1.517233\n",
      "Epoch: 1921 \tTraining Loss: 0.066301 \tValidation Loss: 0.869422\n",
      "Epoch: 1922 \tTraining Loss: 0.066070 \tValidation Loss: 0.929235\n",
      "Epoch: 1923 \tTraining Loss: 0.023199 \tValidation Loss: 0.892291\n",
      "Epoch: 1924 \tTraining Loss: 0.013918 \tValidation Loss: 0.944625\n",
      "Epoch: 1925 \tTraining Loss: 0.038997 \tValidation Loss: 0.993058\n",
      "Epoch: 1926 \tTraining Loss: 0.046002 \tValidation Loss: 0.841559\n",
      "Epoch: 1927 \tTraining Loss: 0.073451 \tValidation Loss: 0.822242\n",
      "Epoch: 1928 \tTraining Loss: 0.025544 \tValidation Loss: 1.072200\n",
      "Epoch: 1929 \tTraining Loss: 0.051275 \tValidation Loss: 1.205451\n",
      "Epoch: 1930 \tTraining Loss: 0.008551 \tValidation Loss: 1.227031\n",
      "Epoch: 1931 \tTraining Loss: 0.011790 \tValidation Loss: 1.241990\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 1932 \tTraining Loss: 0.010606 \tValidation Loss: 1.099922\n",
      "Epoch: 1933 \tTraining Loss: 0.007679 \tValidation Loss: 1.070686\n",
      "Epoch: 1934 \tTraining Loss: 0.003964 \tValidation Loss: 1.042799\n",
      "Epoch: 1935 \tTraining Loss: 0.019316 \tValidation Loss: 1.103779\n",
      "Epoch: 1936 \tTraining Loss: 0.017743 \tValidation Loss: 1.285254\n",
      "Epoch: 1937 \tTraining Loss: 0.010580 \tValidation Loss: 1.226893\n",
      "Epoch: 1938 \tTraining Loss: 0.013531 \tValidation Loss: 1.140841\n",
      "Epoch: 1939 \tTraining Loss: 0.038240 \tValidation Loss: 1.165723\n",
      "Epoch: 1940 \tTraining Loss: 0.004556 \tValidation Loss: 1.081739\n",
      "Epoch: 1941 \tTraining Loss: 0.007812 \tValidation Loss: 1.266311\n",
      "Epoch: 1942 \tTraining Loss: 0.004526 \tValidation Loss: 1.347701\n",
      "Epoch: 1943 \tTraining Loss: 0.004092 \tValidation Loss: 1.315771\n",
      "Epoch: 1944 \tTraining Loss: 0.002919 \tValidation Loss: 1.326231\n",
      "Epoch: 1945 \tTraining Loss: 0.005306 \tValidation Loss: 1.448005\n",
      "Epoch: 1946 \tTraining Loss: 0.001309 \tValidation Loss: 1.489182\n",
      "Epoch: 1947 \tTraining Loss: 0.003094 \tValidation Loss: 1.560530\n",
      "Epoch: 1948 \tTraining Loss: 0.006763 \tValidation Loss: 1.521303\n",
      "Epoch: 1949 \tTraining Loss: 0.001381 \tValidation Loss: 1.321091\n",
      "Epoch: 1950 \tTraining Loss: 0.002961 \tValidation Loss: 1.353884\n",
      "Epoch: 1951 \tTraining Loss: 0.005158 \tValidation Loss: 1.371933\n",
      "Epoch: 1952 \tTraining Loss: 0.006452 \tValidation Loss: 1.233928\n",
      "Epoch: 1953 \tTraining Loss: 0.004244 \tValidation Loss: 1.238477\n",
      "Epoch: 1954 \tTraining Loss: 0.002835 \tValidation Loss: 1.217873\n",
      "Epoch: 1955 \tTraining Loss: 0.002315 \tValidation Loss: 1.210805\n",
      "Epoch: 1956 \tTraining Loss: 0.004472 \tValidation Loss: 1.075049\n",
      "Epoch: 1957 \tTraining Loss: 0.001777 \tValidation Loss: 1.201234\n",
      "Epoch: 1958 \tTraining Loss: 0.001294 \tValidation Loss: 1.213776\n",
      "Epoch: 1959 \tTraining Loss: 0.001151 \tValidation Loss: 1.258230\n",
      "Epoch: 1960 \tTraining Loss: 0.001548 \tValidation Loss: 1.374793\n",
      "Epoch: 1961 \tTraining Loss: 0.001351 \tValidation Loss: 1.453072\n",
      "Epoch: 1962 \tTraining Loss: 0.002654 \tValidation Loss: 1.289564\n",
      "Epoch: 1963 \tTraining Loss: 0.004820 \tValidation Loss: 1.355278\n",
      "Epoch: 1964 \tTraining Loss: 0.001774 \tValidation Loss: 1.234249\n",
      "Epoch: 1965 \tTraining Loss: 0.001928 \tValidation Loss: 1.268771\n",
      "Epoch: 1966 \tTraining Loss: 0.002645 \tValidation Loss: 1.316550\n",
      "Epoch: 1967 \tTraining Loss: 0.001939 \tValidation Loss: 1.278842\n",
      "Epoch: 1968 \tTraining Loss: 0.002321 \tValidation Loss: 1.245668\n",
      "Epoch: 1969 \tTraining Loss: 0.001071 \tValidation Loss: 1.334783\n",
      "Epoch: 1970 \tTraining Loss: 0.001551 \tValidation Loss: 1.316286\n",
      "Epoch: 1971 \tTraining Loss: 0.000739 \tValidation Loss: 1.237815\n",
      "Epoch: 1972 \tTraining Loss: 0.001254 \tValidation Loss: 1.252075\n",
      "Epoch: 1973 \tTraining Loss: 0.005275 \tValidation Loss: 1.145842\n",
      "Epoch: 1974 \tTraining Loss: 0.001404 \tValidation Loss: 1.186162\n",
      "Epoch: 1975 \tTraining Loss: 0.002572 \tValidation Loss: 1.279760\n",
      "Epoch: 1976 \tTraining Loss: 0.002139 \tValidation Loss: 1.181506\n",
      "Epoch: 1977 \tTraining Loss: 0.001413 \tValidation Loss: 1.184034\n",
      "Epoch: 1978 \tTraining Loss: 0.001657 \tValidation Loss: 1.202641\n",
      "Epoch: 1979 \tTraining Loss: 0.001886 \tValidation Loss: 1.242518\n",
      "Epoch: 1980 \tTraining Loss: 0.001337 \tValidation Loss: 1.269544\n",
      "Epoch: 1981 \tTraining Loss: 0.001847 \tValidation Loss: 1.218051\n",
      "Epoch: 1982 \tTraining Loss: 0.001212 \tValidation Loss: 1.136054\n",
      "Epoch: 1983 \tTraining Loss: 0.001663 \tValidation Loss: 1.167990\n",
      "Epoch: 1984 \tTraining Loss: 0.001049 \tValidation Loss: 1.215389\n",
      "Epoch: 1985 \tTraining Loss: 0.001071 \tValidation Loss: 1.191705\n",
      "Epoch: 1986 \tTraining Loss: 0.000945 \tValidation Loss: 1.224022\n",
      "Epoch: 1987 \tTraining Loss: 0.001107 \tValidation Loss: 1.227313\n",
      "Epoch: 1988 \tTraining Loss: 0.000817 \tValidation Loss: 1.200007\n",
      "Epoch: 1989 \tTraining Loss: 0.000877 \tValidation Loss: 1.178707\n",
      "Epoch: 1990 \tTraining Loss: 0.001751 \tValidation Loss: 1.222299\n",
      "Epoch: 1991 \tTraining Loss: 0.000627 \tValidation Loss: 1.239746\n",
      "Epoch: 1992 \tTraining Loss: 0.000693 \tValidation Loss: 1.309866\n",
      "Epoch: 1993 \tTraining Loss: 0.000722 \tValidation Loss: 1.271658\n",
      "Epoch: 1994 \tTraining Loss: 0.001408 \tValidation Loss: 1.276796\n",
      "Epoch: 1995 \tTraining Loss: 0.005620 \tValidation Loss: 1.465315\n",
      "Epoch: 1996 \tTraining Loss: 0.045137 \tValidation Loss: 2.929503\n",
      "Epoch: 1997 \tTraining Loss: 0.181345 \tValidation Loss: 1.304766\n",
      "Epoch: 1998 \tTraining Loss: 0.469300 \tValidation Loss: 2.343756\n",
      "Epoch: 1999 \tTraining Loss: 0.137065 \tValidation Loss: 1.491586\n",
      "Epoch: 2000 \tTraining Loss: 0.233164 \tValidation Loss: 1.777261\n",
      "Epoch: 2001 \tTraining Loss: 0.056455 \tValidation Loss: 1.283775\n",
      "Epoch: 2002 \tTraining Loss: 0.055451 \tValidation Loss: 1.227496\n",
      "Epoch: 2003 \tTraining Loss: 0.225235 \tValidation Loss: 1.099382\n",
      "Epoch: 2004 \tTraining Loss: 0.099152 \tValidation Loss: 1.044714\n",
      "Epoch: 2005 \tTraining Loss: 0.048992 \tValidation Loss: 1.074176\n",
      "Epoch: 2006 \tTraining Loss: 0.060738 \tValidation Loss: 1.304761\n",
      "Epoch: 2007 \tTraining Loss: 0.066930 \tValidation Loss: 1.371674\n",
      "Epoch: 2008 \tTraining Loss: 0.352800 \tValidation Loss: 1.204773\n",
      "Epoch: 2009 \tTraining Loss: 0.427125 \tValidation Loss: 1.008955\n",
      "Epoch: 2010 \tTraining Loss: 0.314960 \tValidation Loss: 0.897844\n",
      "Epoch: 2011 \tTraining Loss: 0.147934 \tValidation Loss: 0.833274\n",
      "Epoch: 2012 \tTraining Loss: 0.099732 \tValidation Loss: 0.824481\n",
      "Epoch: 2013 \tTraining Loss: 0.152460 \tValidation Loss: 0.939993\n",
      "Epoch: 2014 \tTraining Loss: 0.044859 \tValidation Loss: 0.915649\n",
      "Epoch: 2015 \tTraining Loss: 0.041227 \tValidation Loss: 0.982360\n",
      "Epoch: 2016 \tTraining Loss: 0.084045 \tValidation Loss: 0.930854\n",
      "Epoch: 2017 \tTraining Loss: 0.135074 \tValidation Loss: 1.516292\n",
      "Epoch: 2018 \tTraining Loss: 0.202776 \tValidation Loss: 1.242974\n",
      "Epoch: 2019 \tTraining Loss: 0.066217 \tValidation Loss: 1.218891\n",
      "Epoch: 2020 \tTraining Loss: 0.114784 \tValidation Loss: 0.906320\n",
      "Epoch: 2021 \tTraining Loss: 0.035807 \tValidation Loss: 0.852458\n",
      "Epoch: 2022 \tTraining Loss: 0.038529 \tValidation Loss: 0.872780\n",
      "Epoch: 2023 \tTraining Loss: 0.012618 \tValidation Loss: 0.753376\n",
      "Epoch: 2024 \tTraining Loss: 0.007479 \tValidation Loss: 0.726755\n",
      "Epoch: 2025 \tTraining Loss: 0.009683 \tValidation Loss: 0.806188\n",
      "Epoch: 2026 \tTraining Loss: 0.006676 \tValidation Loss: 0.791653\n",
      "Epoch: 2027 \tTraining Loss: 0.005367 \tValidation Loss: 0.837860\n",
      "Epoch: 2028 \tTraining Loss: 0.006540 \tValidation Loss: 0.807969\n",
      "Epoch: 2029 \tTraining Loss: 0.007298 \tValidation Loss: 0.819070\n",
      "Epoch: 2030 \tTraining Loss: 0.007844 \tValidation Loss: 0.780470\n",
      "Epoch: 2031 \tTraining Loss: 0.004867 \tValidation Loss: 0.856032\n",
      "Epoch: 2032 \tTraining Loss: 0.006956 \tValidation Loss: 0.845098\n",
      "Epoch: 2033 \tTraining Loss: 0.005182 \tValidation Loss: 0.875655\n",
      "Epoch: 2034 \tTraining Loss: 0.005329 \tValidation Loss: 0.917614\n",
      "Epoch: 2035 \tTraining Loss: 0.003979 \tValidation Loss: 0.999654\n",
      "Epoch: 2036 \tTraining Loss: 0.003019 \tValidation Loss: 0.953108\n",
      "Epoch: 2037 \tTraining Loss: 0.004476 \tValidation Loss: 1.022797\n",
      "Epoch: 2038 \tTraining Loss: 0.004199 \tValidation Loss: 0.960022\n",
      "Epoch: 2039 \tTraining Loss: 0.005245 \tValidation Loss: 1.040103\n",
      "Epoch: 2040 \tTraining Loss: 0.003265 \tValidation Loss: 1.042078\n",
      "Epoch: 2041 \tTraining Loss: 0.002668 \tValidation Loss: 0.933255\n",
      "Epoch: 2042 \tTraining Loss: 0.005254 \tValidation Loss: 0.933716\n",
      "Epoch: 2043 \tTraining Loss: 0.004646 \tValidation Loss: 0.896003\n",
      "Epoch: 2044 \tTraining Loss: 0.003959 \tValidation Loss: 0.839186\n",
      "Epoch: 2045 \tTraining Loss: 0.009068 \tValidation Loss: 0.828497\n",
      "Epoch: 2046 \tTraining Loss: 0.001519 \tValidation Loss: 0.837602\n",
      "Epoch: 2047 \tTraining Loss: 0.002515 \tValidation Loss: 0.837421\n",
      "Epoch: 2048 \tTraining Loss: 0.002498 \tValidation Loss: 0.866400\n",
      "Epoch: 2049 \tTraining Loss: 0.001959 \tValidation Loss: 0.867423\n",
      "Epoch: 2050 \tTraining Loss: 0.003071 \tValidation Loss: 0.874276\n",
      "Epoch: 2051 \tTraining Loss: 0.004948 \tValidation Loss: 0.883490\n",
      "Epoch: 2052 \tTraining Loss: 0.003947 \tValidation Loss: 0.842142\n",
      "Epoch: 2053 \tTraining Loss: 0.002192 \tValidation Loss: 0.940257\n",
      "Epoch: 2054 \tTraining Loss: 0.001789 \tValidation Loss: 0.909692\n",
      "Epoch: 2055 \tTraining Loss: 0.003216 \tValidation Loss: 0.890636\n",
      "Epoch: 2056 \tTraining Loss: 0.001003 \tValidation Loss: 0.895891\n",
      "Epoch: 2057 \tTraining Loss: 0.002746 \tValidation Loss: 0.921148\n",
      "Epoch: 2058 \tTraining Loss: 0.001307 \tValidation Loss: 0.895770\n",
      "Epoch: 2059 \tTraining Loss: 0.003818 \tValidation Loss: 0.902103\n",
      "Epoch: 2060 \tTraining Loss: 0.004838 \tValidation Loss: 0.875971\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2061 \tTraining Loss: 0.003148 \tValidation Loss: 0.812247\n",
      "Epoch: 2062 \tTraining Loss: 0.002673 \tValidation Loss: 0.841735\n",
      "Epoch: 2063 \tTraining Loss: 0.001799 \tValidation Loss: 0.850944\n",
      "Epoch: 2064 \tTraining Loss: 0.003281 \tValidation Loss: 0.868368\n",
      "Epoch: 2065 \tTraining Loss: 0.001715 \tValidation Loss: 0.895762\n",
      "Epoch: 2066 \tTraining Loss: 0.002591 \tValidation Loss: 0.912739\n",
      "Epoch: 2067 \tTraining Loss: 0.000989 \tValidation Loss: 1.024249\n",
      "Epoch: 2068 \tTraining Loss: 0.001131 \tValidation Loss: 0.949327\n",
      "Epoch: 2069 \tTraining Loss: 0.001434 \tValidation Loss: 0.954591\n",
      "Epoch: 2070 \tTraining Loss: 0.002074 \tValidation Loss: 0.983223\n",
      "Epoch: 2071 \tTraining Loss: 0.001137 \tValidation Loss: 1.013233\n",
      "Epoch: 2072 \tTraining Loss: 0.065395 \tValidation Loss: 1.378250\n",
      "Epoch: 2073 \tTraining Loss: 0.266585 \tValidation Loss: 1.503425\n",
      "Epoch: 2074 \tTraining Loss: 0.383030 \tValidation Loss: 1.173728\n",
      "Epoch: 2075 \tTraining Loss: 0.188012 \tValidation Loss: 0.821132\n",
      "Epoch: 2076 \tTraining Loss: 0.058694 \tValidation Loss: 0.692979\n",
      "Epoch: 2077 \tTraining Loss: 0.038656 \tValidation Loss: 0.980427\n",
      "Epoch: 2078 \tTraining Loss: 0.141967 \tValidation Loss: 0.761028\n",
      "Epoch: 2079 \tTraining Loss: 0.105572 \tValidation Loss: 1.105630\n",
      "Epoch: 2080 \tTraining Loss: 0.042411 \tValidation Loss: 0.705464\n",
      "Epoch: 2081 \tTraining Loss: 0.031438 \tValidation Loss: 0.707790\n",
      "Epoch: 2082 \tTraining Loss: 0.028388 \tValidation Loss: 0.720503\n",
      "Epoch: 2083 \tTraining Loss: 0.011709 \tValidation Loss: 0.745739\n",
      "Epoch: 2084 \tTraining Loss: 0.027717 \tValidation Loss: 0.806801\n",
      "Epoch: 2085 \tTraining Loss: 0.006900 \tValidation Loss: 0.789895\n",
      "Epoch: 2086 \tTraining Loss: 0.006361 \tValidation Loss: 0.780657\n",
      "Epoch: 2087 \tTraining Loss: 0.005028 \tValidation Loss: 0.784761\n",
      "Epoch: 2088 \tTraining Loss: 0.008150 \tValidation Loss: 0.780681\n",
      "Epoch: 2089 \tTraining Loss: 0.006868 \tValidation Loss: 0.758324\n",
      "Epoch: 2090 \tTraining Loss: 0.002678 \tValidation Loss: 0.757892\n",
      "Epoch: 2091 \tTraining Loss: 0.003343 \tValidation Loss: 0.845213\n",
      "Epoch: 2092 \tTraining Loss: 0.009776 \tValidation Loss: 0.861435\n",
      "Epoch: 2093 \tTraining Loss: 0.009727 \tValidation Loss: 0.893528\n",
      "Epoch: 2094 \tTraining Loss: 0.006768 \tValidation Loss: 0.936630\n",
      "Epoch: 2095 \tTraining Loss: 0.004253 \tValidation Loss: 0.879240\n",
      "Epoch: 2096 \tTraining Loss: 0.002258 \tValidation Loss: 0.910651\n",
      "Epoch: 2097 \tTraining Loss: 0.002750 \tValidation Loss: 0.837222\n",
      "Epoch: 2098 \tTraining Loss: 0.002401 \tValidation Loss: 0.835473\n",
      "Epoch: 2099 \tTraining Loss: 0.009086 \tValidation Loss: 0.803297\n",
      "Epoch: 2100 \tTraining Loss: 0.002032 \tValidation Loss: 0.789927\n",
      "Epoch: 2101 \tTraining Loss: 0.008905 \tValidation Loss: 0.829071\n",
      "Epoch: 2102 \tTraining Loss: 0.003233 \tValidation Loss: 0.769826\n",
      "Epoch: 2103 \tTraining Loss: 0.002246 \tValidation Loss: 0.730168\n",
      "Epoch: 2104 \tTraining Loss: 0.004821 \tValidation Loss: 0.797337\n",
      "Epoch: 2105 \tTraining Loss: 0.002570 \tValidation Loss: 0.747752\n",
      "Epoch: 2106 \tTraining Loss: 0.001163 \tValidation Loss: 0.789945\n",
      "Epoch: 2107 \tTraining Loss: 0.006136 \tValidation Loss: 0.771791\n",
      "Epoch: 2108 \tTraining Loss: 0.002026 \tValidation Loss: 0.750107\n",
      "Epoch: 2109 \tTraining Loss: 0.001636 \tValidation Loss: 0.748997\n",
      "Epoch: 2110 \tTraining Loss: 0.002592 \tValidation Loss: 0.756962\n",
      "Epoch: 2111 \tTraining Loss: 0.006503 \tValidation Loss: 0.831675\n",
      "Epoch: 2112 \tTraining Loss: 0.005155 \tValidation Loss: 0.878211\n",
      "Epoch: 2113 \tTraining Loss: 0.002601 \tValidation Loss: 0.778240\n",
      "Epoch: 2114 \tTraining Loss: 0.001936 \tValidation Loss: 0.786322\n",
      "Epoch: 2115 \tTraining Loss: 0.002050 \tValidation Loss: 0.789222\n",
      "Epoch: 2116 \tTraining Loss: 0.001300 \tValidation Loss: 0.789783\n",
      "Epoch: 2117 \tTraining Loss: 0.003014 \tValidation Loss: 0.802435\n",
      "Epoch: 2118 \tTraining Loss: 0.002798 \tValidation Loss: 0.818841\n",
      "Epoch: 2119 \tTraining Loss: 0.001760 \tValidation Loss: 0.831237\n",
      "Epoch: 2120 \tTraining Loss: 0.001407 \tValidation Loss: 0.819153\n",
      "Epoch: 2121 \tTraining Loss: 0.001986 \tValidation Loss: 0.822593\n",
      "Epoch: 2122 \tTraining Loss: 0.003073 \tValidation Loss: 0.818420\n",
      "Epoch: 2123 \tTraining Loss: 0.000996 \tValidation Loss: 0.795987\n",
      "Epoch: 2124 \tTraining Loss: 0.001108 \tValidation Loss: 0.805412\n",
      "Epoch: 2125 \tTraining Loss: 0.001700 \tValidation Loss: 0.779400\n",
      "Epoch: 2126 \tTraining Loss: 0.001684 \tValidation Loss: 0.776679\n",
      "Epoch: 2127 \tTraining Loss: 0.001808 \tValidation Loss: 0.777006\n",
      "Epoch: 2128 \tTraining Loss: 0.001220 \tValidation Loss: 0.789576\n",
      "Epoch: 2129 \tTraining Loss: 0.001465 \tValidation Loss: 0.789690\n",
      "Epoch: 2130 \tTraining Loss: 0.000984 \tValidation Loss: 0.796377\n",
      "Epoch: 2131 \tTraining Loss: 0.002071 \tValidation Loss: 0.792074\n",
      "Epoch: 2132 \tTraining Loss: 0.000787 \tValidation Loss: 0.757759\n",
      "Epoch: 2133 \tTraining Loss: 0.000825 \tValidation Loss: 0.764880\n",
      "Epoch: 2134 \tTraining Loss: 0.001671 \tValidation Loss: 0.760904\n",
      "Epoch: 2135 \tTraining Loss: 0.001367 \tValidation Loss: 0.777550\n",
      "Epoch: 2136 \tTraining Loss: 0.002245 \tValidation Loss: 0.801910\n",
      "Epoch: 2137 \tTraining Loss: 0.001496 \tValidation Loss: 0.806725\n",
      "Epoch: 2138 \tTraining Loss: 0.000661 \tValidation Loss: 0.806709\n",
      "Epoch: 2139 \tTraining Loss: 0.001908 \tValidation Loss: 0.837168\n",
      "Epoch: 2140 \tTraining Loss: 0.001032 \tValidation Loss: 0.823420\n",
      "Epoch: 2141 \tTraining Loss: 0.002144 \tValidation Loss: 0.784958\n",
      "Epoch: 2142 \tTraining Loss: 0.001804 \tValidation Loss: 0.782181\n",
      "Epoch: 2143 \tTraining Loss: 0.001393 \tValidation Loss: 0.799240\n",
      "Epoch: 2144 \tTraining Loss: 0.000787 \tValidation Loss: 0.835098\n",
      "Epoch: 2145 \tTraining Loss: 0.042923 \tValidation Loss: 1.305092\n",
      "Epoch: 2146 \tTraining Loss: 0.534043 \tValidation Loss: 1.042999\n",
      "Epoch: 2147 \tTraining Loss: 0.423666 \tValidation Loss: 0.565012\n",
      "Epoch: 2148 \tTraining Loss: 0.276613 \tValidation Loss: 0.836940\n",
      "Epoch: 2149 \tTraining Loss: 0.201528 \tValidation Loss: 0.712246\n",
      "Epoch: 2150 \tTraining Loss: 0.583618 \tValidation Loss: 0.799158\n",
      "Epoch: 2151 \tTraining Loss: 0.301223 \tValidation Loss: 0.687650\n",
      "Epoch: 2152 \tTraining Loss: 0.077703 \tValidation Loss: 0.750302\n",
      "Epoch: 2153 \tTraining Loss: 0.279811 \tValidation Loss: 0.657663\n",
      "Epoch: 2154 \tTraining Loss: 0.237234 \tValidation Loss: 0.645971\n",
      "Epoch: 2155 \tTraining Loss: 0.171478 \tValidation Loss: 0.723994\n",
      "Epoch: 2156 \tTraining Loss: 0.051075 \tValidation Loss: 0.810627\n",
      "Epoch: 2157 \tTraining Loss: 0.044623 \tValidation Loss: 0.799418\n",
      "Epoch: 2158 \tTraining Loss: 0.016396 \tValidation Loss: 0.643700\n",
      "Epoch: 2159 \tTraining Loss: 0.023387 \tValidation Loss: 0.635647\n",
      "Epoch: 2160 \tTraining Loss: 0.010388 \tValidation Loss: 0.702192\n",
      "Epoch: 2161 \tTraining Loss: 0.094415 \tValidation Loss: 0.623821\n",
      "Epoch: 2162 \tTraining Loss: 0.026235 \tValidation Loss: 0.618561\n",
      "Epoch: 2163 \tTraining Loss: 0.321155 \tValidation Loss: 1.451110\n",
      "Epoch: 2164 \tTraining Loss: 0.076545 \tValidation Loss: 1.200732\n",
      "Epoch: 2165 \tTraining Loss: 0.092477 \tValidation Loss: 1.106860\n",
      "Epoch: 2166 \tTraining Loss: 0.071369 \tValidation Loss: 0.946833\n",
      "Epoch: 2167 \tTraining Loss: 0.105769 \tValidation Loss: 1.128682\n",
      "Epoch: 2168 \tTraining Loss: 0.036812 \tValidation Loss: 0.973615\n",
      "Epoch: 2169 \tTraining Loss: 0.096430 \tValidation Loss: 0.993259\n",
      "Epoch: 2170 \tTraining Loss: 0.053753 \tValidation Loss: 1.082022\n",
      "Epoch: 2171 \tTraining Loss: 0.074019 \tValidation Loss: 0.902596\n",
      "Epoch: 2172 \tTraining Loss: 0.022742 \tValidation Loss: 0.943599\n",
      "Epoch: 2173 \tTraining Loss: 0.012703 \tValidation Loss: 0.975526\n",
      "Epoch: 2174 \tTraining Loss: 0.014229 \tValidation Loss: 0.946547\n",
      "Epoch: 2175 \tTraining Loss: 0.014248 \tValidation Loss: 0.973409\n",
      "Epoch: 2176 \tTraining Loss: 0.007556 \tValidation Loss: 1.026056\n",
      "Epoch: 2177 \tTraining Loss: 0.008394 \tValidation Loss: 0.985629\n",
      "Epoch: 2178 \tTraining Loss: 0.035368 \tValidation Loss: 1.117219\n",
      "Epoch: 2179 \tTraining Loss: 0.023882 \tValidation Loss: 0.900040\n",
      "Epoch: 2180 \tTraining Loss: 0.009719 \tValidation Loss: 0.917146\n",
      "Epoch: 2181 \tTraining Loss: 0.012984 \tValidation Loss: 1.000342\n",
      "Epoch: 2182 \tTraining Loss: 0.012853 \tValidation Loss: 0.944431\n",
      "Epoch: 2183 \tTraining Loss: 0.035244 \tValidation Loss: 0.903790\n",
      "Epoch: 2184 \tTraining Loss: 0.019199 \tValidation Loss: 0.903054\n",
      "Epoch: 2185 \tTraining Loss: 0.004786 \tValidation Loss: 0.946031\n",
      "Epoch: 2186 \tTraining Loss: 0.021034 \tValidation Loss: 1.034061\n",
      "Epoch: 2187 \tTraining Loss: 0.015895 \tValidation Loss: 0.877523\n",
      "Epoch: 2188 \tTraining Loss: 0.014717 \tValidation Loss: 0.787399\n",
      "Epoch: 2189 \tTraining Loss: 0.007425 \tValidation Loss: 0.854457\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2190 \tTraining Loss: 0.006275 \tValidation Loss: 0.855635\n",
      "Epoch: 2191 \tTraining Loss: 0.004284 \tValidation Loss: 0.873131\n",
      "Epoch: 2192 \tTraining Loss: 0.002663 \tValidation Loss: 0.908452\n",
      "Epoch: 2193 \tTraining Loss: 0.004790 \tValidation Loss: 0.962809\n",
      "Epoch: 2194 \tTraining Loss: 0.006224 \tValidation Loss: 0.885033\n",
      "Epoch: 2195 \tTraining Loss: 0.007551 \tValidation Loss: 0.963418\n",
      "Epoch: 2196 \tTraining Loss: 0.004244 \tValidation Loss: 0.996435\n",
      "Epoch: 2197 \tTraining Loss: 0.004096 \tValidation Loss: 1.030002\n",
      "Epoch: 2198 \tTraining Loss: 0.004863 \tValidation Loss: 1.105597\n",
      "Epoch: 2199 \tTraining Loss: 0.003011 \tValidation Loss: 1.063128\n",
      "Epoch: 2200 \tTraining Loss: 0.002577 \tValidation Loss: 1.126617\n",
      "Epoch: 2201 \tTraining Loss: 0.002984 \tValidation Loss: 1.045251\n",
      "Epoch: 2202 \tTraining Loss: 0.003772 \tValidation Loss: 1.023451\n",
      "Epoch: 2203 \tTraining Loss: 0.002076 \tValidation Loss: 1.112112\n",
      "Epoch: 2204 \tTraining Loss: 0.003426 \tValidation Loss: 1.120730\n",
      "Epoch: 2205 \tTraining Loss: 0.003217 \tValidation Loss: 1.046085\n",
      "Epoch: 2206 \tTraining Loss: 0.003559 \tValidation Loss: 1.120771\n",
      "Epoch: 2207 \tTraining Loss: 0.005970 \tValidation Loss: 1.037119\n",
      "Epoch: 2208 \tTraining Loss: 0.003433 \tValidation Loss: 0.960204\n",
      "Epoch: 2209 \tTraining Loss: 0.002136 \tValidation Loss: 1.025990\n",
      "Epoch: 2210 \tTraining Loss: 0.001614 \tValidation Loss: 1.053207\n",
      "Epoch: 2211 \tTraining Loss: 0.001084 \tValidation Loss: 1.079037\n",
      "Epoch: 2212 \tTraining Loss: 0.001366 \tValidation Loss: 1.021043\n",
      "Epoch: 2213 \tTraining Loss: 0.001022 \tValidation Loss: 1.016290\n",
      "Epoch: 2214 \tTraining Loss: 0.004178 \tValidation Loss: 1.042808\n",
      "Epoch: 2215 \tTraining Loss: 0.003457 \tValidation Loss: 1.065025\n",
      "Epoch: 2216 \tTraining Loss: 0.009540 \tValidation Loss: 1.157279\n",
      "Epoch: 2217 \tTraining Loss: 0.010343 \tValidation Loss: 1.130704\n",
      "Epoch: 2218 \tTraining Loss: 0.003802 \tValidation Loss: 1.070220\n",
      "Epoch: 2219 \tTraining Loss: 0.003174 \tValidation Loss: 1.019581\n",
      "Epoch: 2220 \tTraining Loss: 0.001804 \tValidation Loss: 1.038672\n",
      "Epoch: 2221 \tTraining Loss: 0.002253 \tValidation Loss: 1.053271\n",
      "Epoch: 2222 \tTraining Loss: 0.001821 \tValidation Loss: 1.084537\n",
      "Epoch: 2223 \tTraining Loss: 0.002814 \tValidation Loss: 1.061769\n",
      "Epoch: 2224 \tTraining Loss: 0.001220 \tValidation Loss: 1.082091\n",
      "Epoch: 2225 \tTraining Loss: 0.008662 \tValidation Loss: 1.389652\n",
      "Epoch: 2226 \tTraining Loss: 0.006682 \tValidation Loss: 1.008062\n",
      "Epoch: 2227 \tTraining Loss: 0.002688 \tValidation Loss: 1.058889\n",
      "Epoch: 2228 \tTraining Loss: 0.005678 \tValidation Loss: 1.359691\n",
      "Epoch: 2229 \tTraining Loss: 0.004182 \tValidation Loss: 1.347801\n",
      "Epoch: 2230 \tTraining Loss: 0.005469 \tValidation Loss: 1.197134\n",
      "Epoch: 2231 \tTraining Loss: 0.004566 \tValidation Loss: 1.245785\n",
      "Epoch: 2232 \tTraining Loss: 0.004559 \tValidation Loss: 1.192560\n",
      "Epoch: 2233 \tTraining Loss: 0.003007 \tValidation Loss: 1.153987\n",
      "Epoch: 2234 \tTraining Loss: 0.002668 \tValidation Loss: 1.130725\n",
      "Epoch: 2235 \tTraining Loss: 0.001756 \tValidation Loss: 1.134274\n",
      "Epoch: 2236 \tTraining Loss: 0.001006 \tValidation Loss: 1.190533\n",
      "Epoch: 2237 \tTraining Loss: 0.004434 \tValidation Loss: 1.115731\n",
      "Epoch: 2238 \tTraining Loss: 0.001990 \tValidation Loss: 1.195209\n",
      "Epoch: 2239 \tTraining Loss: 0.001901 \tValidation Loss: 1.009714\n",
      "Epoch: 2240 \tTraining Loss: 0.001861 \tValidation Loss: 1.063670\n",
      "Epoch: 2241 \tTraining Loss: 0.000945 \tValidation Loss: 1.115407\n",
      "Epoch: 2242 \tTraining Loss: 0.002415 \tValidation Loss: 1.164384\n",
      "Epoch: 2243 \tTraining Loss: 0.002392 \tValidation Loss: 1.178908\n",
      "Epoch: 2244 \tTraining Loss: 0.002484 \tValidation Loss: 1.101163\n",
      "Epoch: 2245 \tTraining Loss: 0.002546 \tValidation Loss: 1.150093\n",
      "Epoch: 2246 \tTraining Loss: 0.001217 \tValidation Loss: 1.211095\n",
      "Epoch: 2247 \tTraining Loss: 0.001530 \tValidation Loss: 1.182205\n",
      "Epoch: 2248 \tTraining Loss: 0.001371 \tValidation Loss: 1.236703\n",
      "Epoch: 2249 \tTraining Loss: 0.001908 \tValidation Loss: 1.188851\n",
      "Epoch: 2250 \tTraining Loss: 0.001740 \tValidation Loss: 1.215920\n",
      "Epoch: 2251 \tTraining Loss: 0.001137 \tValidation Loss: 1.244839\n",
      "Epoch: 2252 \tTraining Loss: 0.003416 \tValidation Loss: 1.140573\n",
      "Epoch: 2253 \tTraining Loss: 0.001752 \tValidation Loss: 1.234425\n",
      "Epoch: 2254 \tTraining Loss: 0.001348 \tValidation Loss: 1.156908\n",
      "Epoch: 2255 \tTraining Loss: 0.000904 \tValidation Loss: 1.234657\n",
      "Epoch: 2256 \tTraining Loss: 0.001479 \tValidation Loss: 1.231834\n",
      "Epoch: 2257 \tTraining Loss: 0.001530 \tValidation Loss: 1.213761\n",
      "Epoch: 2258 \tTraining Loss: 0.001721 \tValidation Loss: 1.275362\n",
      "Epoch: 2259 \tTraining Loss: 0.001476 \tValidation Loss: 1.296845\n",
      "Epoch: 2260 \tTraining Loss: 0.001226 \tValidation Loss: 1.225263\n",
      "Epoch: 2261 \tTraining Loss: 0.001957 \tValidation Loss: 1.090713\n",
      "Epoch: 2262 \tTraining Loss: 0.001299 \tValidation Loss: 1.130345\n",
      "Epoch: 2263 \tTraining Loss: 0.001074 \tValidation Loss: 1.158436\n",
      "Epoch: 2264 \tTraining Loss: 0.001926 \tValidation Loss: 1.138537\n",
      "Epoch: 2265 \tTraining Loss: 0.000512 \tValidation Loss: 1.201631\n",
      "Epoch: 2266 \tTraining Loss: 0.001492 \tValidation Loss: 1.130098\n",
      "Epoch: 2267 \tTraining Loss: 0.000649 \tValidation Loss: 1.120162\n",
      "Epoch: 2268 \tTraining Loss: 0.000729 \tValidation Loss: 1.118059\n",
      "Epoch: 2269 \tTraining Loss: 0.001469 \tValidation Loss: 1.208262\n",
      "Epoch: 2270 \tTraining Loss: 0.000650 \tValidation Loss: 1.257782\n",
      "Epoch: 2271 \tTraining Loss: 0.001893 \tValidation Loss: 1.196496\n",
      "Epoch: 2272 \tTraining Loss: 0.001030 \tValidation Loss: 1.180681\n",
      "Epoch: 2273 \tTraining Loss: 0.003765 \tValidation Loss: 1.056486\n",
      "Epoch: 2274 \tTraining Loss: 0.274918 \tValidation Loss: 1.166136\n",
      "Epoch: 2275 \tTraining Loss: 0.399252 \tValidation Loss: 2.217092\n",
      "Epoch: 2276 \tTraining Loss: 0.239958 \tValidation Loss: 0.949280\n",
      "Epoch: 2277 \tTraining Loss: 0.189520 \tValidation Loss: 0.754245\n",
      "Epoch: 2278 \tTraining Loss: 0.408712 \tValidation Loss: 0.698681\n",
      "Epoch: 2279 \tTraining Loss: 0.440400 \tValidation Loss: 0.708436\n",
      "Epoch: 2280 \tTraining Loss: 0.288176 \tValidation Loss: 0.543792\n",
      "Epoch: 2281 \tTraining Loss: 0.288295 \tValidation Loss: 0.696731\n",
      "Epoch: 2282 \tTraining Loss: 0.143757 \tValidation Loss: 1.372997\n",
      "Epoch: 2283 \tTraining Loss: 0.058279 \tValidation Loss: 1.171865\n",
      "Epoch: 2284 \tTraining Loss: 0.149420 \tValidation Loss: 0.981528\n",
      "Epoch: 2285 \tTraining Loss: 0.068904 \tValidation Loss: 0.589163\n",
      "Epoch: 2286 \tTraining Loss: 0.022463 \tValidation Loss: 0.689971\n",
      "Epoch: 2287 \tTraining Loss: 0.031163 \tValidation Loss: 0.747870\n",
      "Epoch: 2288 \tTraining Loss: 0.128401 \tValidation Loss: 0.682072\n",
      "Epoch: 2289 \tTraining Loss: 0.045133 \tValidation Loss: 0.741422\n",
      "Epoch: 2290 \tTraining Loss: 0.063331 \tValidation Loss: 1.323355\n",
      "Epoch: 2291 \tTraining Loss: 0.055425 \tValidation Loss: 0.918803\n",
      "Epoch: 2292 \tTraining Loss: 0.018946 \tValidation Loss: 0.936612\n",
      "Epoch: 2293 \tTraining Loss: 0.012145 \tValidation Loss: 0.778896\n",
      "Epoch: 2294 \tTraining Loss: 0.005558 \tValidation Loss: 0.738520\n",
      "Epoch: 2295 \tTraining Loss: 0.013932 \tValidation Loss: 0.906780\n",
      "Epoch: 2296 \tTraining Loss: 0.005095 \tValidation Loss: 0.855933\n",
      "Epoch: 2297 \tTraining Loss: 0.005067 \tValidation Loss: 0.743623\n",
      "Epoch: 2298 \tTraining Loss: 0.007296 \tValidation Loss: 0.744795\n",
      "Epoch: 2299 \tTraining Loss: 0.008138 \tValidation Loss: 0.798947\n",
      "Epoch: 2300 \tTraining Loss: 0.006732 \tValidation Loss: 0.847876\n",
      "Epoch: 2301 \tTraining Loss: 0.004343 \tValidation Loss: 0.891934\n",
      "Epoch: 2302 \tTraining Loss: 0.002824 \tValidation Loss: 0.882137\n",
      "Epoch: 2303 \tTraining Loss: 0.001904 \tValidation Loss: 0.973817\n",
      "Epoch: 2304 \tTraining Loss: 0.003932 \tValidation Loss: 0.973132\n",
      "Epoch: 2305 \tTraining Loss: 0.005009 \tValidation Loss: 1.013752\n",
      "Epoch: 2306 \tTraining Loss: 0.010520 \tValidation Loss: 0.708293\n",
      "Epoch: 2307 \tTraining Loss: 0.004439 \tValidation Loss: 0.640061\n",
      "Epoch: 2308 \tTraining Loss: 0.004788 \tValidation Loss: 0.708180\n",
      "Epoch: 2309 \tTraining Loss: 0.003733 \tValidation Loss: 0.703966\n",
      "Epoch: 2310 \tTraining Loss: 0.002930 \tValidation Loss: 0.761290\n",
      "Epoch: 2311 \tTraining Loss: 0.052526 \tValidation Loss: 0.801015\n",
      "Epoch: 2312 \tTraining Loss: 0.062787 \tValidation Loss: 1.034177\n",
      "Epoch: 2313 \tTraining Loss: 0.019166 \tValidation Loss: 0.623968\n",
      "Epoch: 2314 \tTraining Loss: 0.020757 \tValidation Loss: 0.771470\n",
      "Epoch: 2315 \tTraining Loss: 0.018973 \tValidation Loss: 0.798427\n",
      "Epoch: 2316 \tTraining Loss: 0.002981 \tValidation Loss: 0.706785\n",
      "Epoch: 2317 \tTraining Loss: 0.005003 \tValidation Loss: 0.761528\n",
      "Epoch: 2318 \tTraining Loss: 0.006163 \tValidation Loss: 0.743705\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2319 \tTraining Loss: 0.002862 \tValidation Loss: 0.734835\n",
      "Epoch: 2320 \tTraining Loss: 0.002394 \tValidation Loss: 0.680322\n",
      "Epoch: 2321 \tTraining Loss: 0.002815 \tValidation Loss: 0.675013\n",
      "Epoch: 2322 \tTraining Loss: 0.003935 \tValidation Loss: 0.684468\n",
      "Epoch: 2323 \tTraining Loss: 0.001425 \tValidation Loss: 0.700937\n",
      "Epoch: 2324 \tTraining Loss: 0.004999 \tValidation Loss: 0.725908\n",
      "Epoch: 2325 \tTraining Loss: 0.005281 \tValidation Loss: 0.699522\n",
      "Epoch: 2326 \tTraining Loss: 0.015937 \tValidation Loss: 1.052761\n",
      "Epoch: 2327 \tTraining Loss: 0.002897 \tValidation Loss: 0.918093\n",
      "Epoch: 2328 \tTraining Loss: 0.014388 \tValidation Loss: 0.848106\n",
      "Epoch: 2329 \tTraining Loss: 0.007953 \tValidation Loss: 0.918812\n",
      "Epoch: 2330 \tTraining Loss: 0.028701 \tValidation Loss: 1.071814\n",
      "Epoch: 2331 \tTraining Loss: 0.004536 \tValidation Loss: 0.845806\n",
      "Epoch: 2332 \tTraining Loss: 0.051196 \tValidation Loss: 1.131007\n",
      "Epoch: 2333 \tTraining Loss: 0.230617 \tValidation Loss: 0.870837\n",
      "Epoch: 2334 \tTraining Loss: 0.278505 \tValidation Loss: 1.474465\n",
      "Epoch: 2335 \tTraining Loss: 0.046708 \tValidation Loss: 0.877851\n",
      "Epoch: 2336 \tTraining Loss: 0.020006 \tValidation Loss: 0.794892\n",
      "Epoch: 2337 \tTraining Loss: 0.021624 \tValidation Loss: 0.947904\n",
      "Epoch: 2338 \tTraining Loss: 0.017646 \tValidation Loss: 0.892723\n",
      "Epoch: 2339 \tTraining Loss: 0.009600 \tValidation Loss: 0.909846\n",
      "Epoch: 2340 \tTraining Loss: 0.023778 \tValidation Loss: 0.897680\n",
      "Epoch: 2341 \tTraining Loss: 0.021642 \tValidation Loss: 0.722963\n",
      "Epoch: 2342 \tTraining Loss: 0.011502 \tValidation Loss: 0.873715\n",
      "Epoch: 2343 \tTraining Loss: 0.005048 \tValidation Loss: 0.930673\n",
      "Epoch: 2344 \tTraining Loss: 0.009952 \tValidation Loss: 0.830148\n",
      "Epoch: 2345 \tTraining Loss: 0.004413 \tValidation Loss: 0.844712\n",
      "Epoch: 2346 \tTraining Loss: 0.003763 \tValidation Loss: 0.879362\n",
      "Epoch: 2347 \tTraining Loss: 0.006299 \tValidation Loss: 0.905476\n",
      "Epoch: 2348 \tTraining Loss: 0.004150 \tValidation Loss: 0.885554\n",
      "Epoch: 2349 \tTraining Loss: 0.004089 \tValidation Loss: 0.930314\n",
      "Epoch: 2350 \tTraining Loss: 0.014894 \tValidation Loss: 1.036646\n",
      "Epoch: 2351 \tTraining Loss: 0.005447 \tValidation Loss: 1.032251\n",
      "Epoch: 2352 \tTraining Loss: 0.021546 \tValidation Loss: 1.003524\n",
      "Epoch: 2353 \tTraining Loss: 0.007665 \tValidation Loss: 1.094654\n",
      "Epoch: 2354 \tTraining Loss: 0.004414 \tValidation Loss: 0.951310\n",
      "Epoch: 2355 \tTraining Loss: 0.030660 \tValidation Loss: 1.002554\n",
      "Epoch: 2356 \tTraining Loss: 0.003757 \tValidation Loss: 0.938081\n",
      "Epoch: 2357 \tTraining Loss: 0.002261 \tValidation Loss: 0.960648\n",
      "Epoch: 2358 \tTraining Loss: 0.003063 \tValidation Loss: 0.991349\n",
      "Epoch: 2359 \tTraining Loss: 0.004495 \tValidation Loss: 0.964953\n",
      "Epoch: 2360 \tTraining Loss: 0.005177 \tValidation Loss: 0.829304\n",
      "Epoch: 2361 \tTraining Loss: 0.061280 \tValidation Loss: 0.776137\n",
      "Epoch: 2362 \tTraining Loss: 0.012208 \tValidation Loss: 1.146778\n",
      "Epoch: 2363 \tTraining Loss: 0.008008 \tValidation Loss: 0.964399\n",
      "Epoch: 2364 \tTraining Loss: 0.009438 \tValidation Loss: 0.924409\n",
      "Epoch: 2365 \tTraining Loss: 0.006925 \tValidation Loss: 0.870077\n",
      "Epoch: 2366 \tTraining Loss: 0.001858 \tValidation Loss: 0.858264\n",
      "Epoch: 2367 \tTraining Loss: 0.004138 \tValidation Loss: 0.810920\n",
      "Epoch: 2368 \tTraining Loss: 0.003727 \tValidation Loss: 0.814626\n",
      "Epoch: 2369 \tTraining Loss: 0.001226 \tValidation Loss: 0.846933\n",
      "Epoch: 2370 \tTraining Loss: 0.007911 \tValidation Loss: 0.857519\n",
      "Epoch: 2371 \tTraining Loss: 0.005076 \tValidation Loss: 0.772808\n",
      "Epoch: 2372 \tTraining Loss: 0.080970 \tValidation Loss: 0.909027\n",
      "Epoch: 2373 \tTraining Loss: 0.144914 \tValidation Loss: 1.145414\n",
      "Epoch: 2374 \tTraining Loss: 0.075574 \tValidation Loss: 0.911943\n",
      "Epoch: 2375 \tTraining Loss: 0.300470 \tValidation Loss: 0.782902\n",
      "Epoch: 2376 \tTraining Loss: 0.176942 \tValidation Loss: 0.998245\n",
      "Epoch: 2377 \tTraining Loss: 0.256375 \tValidation Loss: 0.651512\n",
      "Epoch: 2378 \tTraining Loss: 0.051104 \tValidation Loss: 0.643667\n",
      "Epoch: 2379 \tTraining Loss: 0.018289 \tValidation Loss: 0.661448\n",
      "Epoch: 2380 \tTraining Loss: 0.030195 \tValidation Loss: 0.768004\n",
      "Epoch: 2381 \tTraining Loss: 0.014264 \tValidation Loss: 0.836289\n",
      "Epoch: 2382 \tTraining Loss: 0.026111 \tValidation Loss: 0.895789\n",
      "Epoch: 2383 \tTraining Loss: 0.015576 \tValidation Loss: 1.008620\n",
      "Epoch: 2384 \tTraining Loss: 0.012225 \tValidation Loss: 0.914813\n",
      "Epoch: 2385 \tTraining Loss: 0.007459 \tValidation Loss: 0.929526\n",
      "Epoch: 2386 \tTraining Loss: 0.010209 \tValidation Loss: 1.013585\n",
      "Epoch: 2387 \tTraining Loss: 0.008193 \tValidation Loss: 0.964891\n",
      "Epoch: 2388 \tTraining Loss: 0.005839 \tValidation Loss: 0.955979\n",
      "Epoch: 2389 \tTraining Loss: 0.004987 \tValidation Loss: 0.978307\n",
      "Epoch: 2390 \tTraining Loss: 0.010616 \tValidation Loss: 1.075415\n",
      "Epoch: 2391 \tTraining Loss: 0.002891 \tValidation Loss: 1.099963\n",
      "Epoch: 2392 \tTraining Loss: 0.004446 \tValidation Loss: 1.024108\n",
      "Epoch: 2393 \tTraining Loss: 0.004313 \tValidation Loss: 0.953799\n",
      "Epoch: 2394 \tTraining Loss: 0.005585 \tValidation Loss: 1.011619\n",
      "Epoch: 2395 \tTraining Loss: 0.006372 \tValidation Loss: 1.017372\n",
      "Epoch: 2396 \tTraining Loss: 0.002473 \tValidation Loss: 1.099599\n",
      "Epoch: 2397 \tTraining Loss: 0.005016 \tValidation Loss: 1.078907\n",
      "Epoch: 2398 \tTraining Loss: 0.003435 \tValidation Loss: 1.031829\n",
      "Epoch: 2399 \tTraining Loss: 0.002994 \tValidation Loss: 1.034975\n",
      "Epoch: 2400 \tTraining Loss: 0.003263 \tValidation Loss: 0.978649\n",
      "Epoch: 2401 \tTraining Loss: 0.002835 \tValidation Loss: 1.047197\n",
      "Epoch: 2402 \tTraining Loss: 0.010734 \tValidation Loss: 0.943164\n",
      "Epoch: 2403 \tTraining Loss: 0.002150 \tValidation Loss: 0.941405\n",
      "Epoch: 2404 \tTraining Loss: 0.002720 \tValidation Loss: 0.934173\n",
      "Epoch: 2405 \tTraining Loss: 0.004021 \tValidation Loss: 0.969144\n",
      "Epoch: 2406 \tTraining Loss: 0.002267 \tValidation Loss: 1.064756\n",
      "Epoch: 2407 \tTraining Loss: 0.002602 \tValidation Loss: 1.050576\n",
      "Epoch: 2408 \tTraining Loss: 0.001158 \tValidation Loss: 1.058504\n",
      "Epoch: 2409 \tTraining Loss: 0.003594 \tValidation Loss: 1.078041\n",
      "Epoch: 2410 \tTraining Loss: 0.009252 \tValidation Loss: 0.872480\n",
      "Epoch: 2411 \tTraining Loss: 0.245926 \tValidation Loss: 2.169913\n",
      "Epoch: 2412 \tTraining Loss: 0.189976 \tValidation Loss: 0.611639\n",
      "Epoch: 2413 \tTraining Loss: 0.106196 \tValidation Loss: 0.812527\n",
      "Epoch: 2414 \tTraining Loss: 0.046930 \tValidation Loss: 1.048216\n",
      "Epoch: 2415 \tTraining Loss: 0.055926 \tValidation Loss: 0.954539\n",
      "Epoch: 2416 \tTraining Loss: 0.038201 \tValidation Loss: 0.985004\n",
      "Epoch: 2417 \tTraining Loss: 0.015005 \tValidation Loss: 1.032342\n",
      "Epoch: 2418 \tTraining Loss: 0.006096 \tValidation Loss: 1.280610\n",
      "Epoch: 2419 \tTraining Loss: 0.006858 \tValidation Loss: 1.265039\n",
      "Epoch: 2420 \tTraining Loss: 0.015641 \tValidation Loss: 1.060871\n",
      "Epoch: 2421 \tTraining Loss: 0.004718 \tValidation Loss: 1.084582\n",
      "Epoch: 2422 \tTraining Loss: 0.007023 \tValidation Loss: 1.071872\n",
      "Epoch: 2423 \tTraining Loss: 0.005499 \tValidation Loss: 1.138794\n",
      "Epoch: 2424 \tTraining Loss: 0.005197 \tValidation Loss: 1.125724\n",
      "Epoch: 2425 \tTraining Loss: 0.005279 \tValidation Loss: 1.209642\n",
      "Epoch: 2426 \tTraining Loss: 0.028429 \tValidation Loss: 0.973207\n",
      "Epoch: 2427 \tTraining Loss: 0.009190 \tValidation Loss: 1.114293\n",
      "Epoch: 2428 \tTraining Loss: 0.001786 \tValidation Loss: 1.118506\n",
      "Epoch: 2429 \tTraining Loss: 0.002966 \tValidation Loss: 1.184334\n",
      "Epoch: 2430 \tTraining Loss: 0.099618 \tValidation Loss: 1.758706\n",
      "Epoch: 2431 \tTraining Loss: 0.273715 \tValidation Loss: 0.992155\n",
      "Epoch: 2432 \tTraining Loss: 0.147749 \tValidation Loss: 0.880829\n",
      "Epoch: 2433 \tTraining Loss: 0.068126 \tValidation Loss: 0.982023\n",
      "Epoch: 2434 \tTraining Loss: 0.051393 \tValidation Loss: 1.139133\n",
      "Epoch: 2435 \tTraining Loss: 0.035912 \tValidation Loss: 0.809311\n",
      "Epoch: 2436 \tTraining Loss: 0.009036 \tValidation Loss: 0.851918\n",
      "Epoch: 2437 \tTraining Loss: 0.023775 \tValidation Loss: 0.927197\n",
      "Epoch: 2438 \tTraining Loss: 0.014433 \tValidation Loss: 0.845638\n",
      "Epoch: 2439 \tTraining Loss: 0.005348 \tValidation Loss: 0.846660\n",
      "Epoch: 2440 \tTraining Loss: 0.003236 \tValidation Loss: 0.863379\n",
      "Epoch: 2441 \tTraining Loss: 0.006074 \tValidation Loss: 0.894520\n",
      "Epoch: 2442 \tTraining Loss: 0.039196 \tValidation Loss: 0.837930\n",
      "Epoch: 2443 \tTraining Loss: 0.298814 \tValidation Loss: 1.141530\n",
      "Epoch: 2444 \tTraining Loss: 0.269968 \tValidation Loss: 0.818284\n",
      "Epoch: 2445 \tTraining Loss: 0.057942 \tValidation Loss: 1.200903\n",
      "Epoch: 2446 \tTraining Loss: 0.030394 \tValidation Loss: 0.851394\n",
      "Epoch: 2447 \tTraining Loss: 0.017554 \tValidation Loss: 0.788614\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2448 \tTraining Loss: 0.011583 \tValidation Loss: 0.867067\n",
      "Epoch: 2449 \tTraining Loss: 0.009875 \tValidation Loss: 0.797653\n",
      "Epoch: 2450 \tTraining Loss: 0.009074 \tValidation Loss: 0.866000\n",
      "Epoch: 2451 \tTraining Loss: 0.005902 \tValidation Loss: 0.875521\n",
      "Epoch: 2452 \tTraining Loss: 0.003024 \tValidation Loss: 0.870453\n",
      "Epoch: 2453 \tTraining Loss: 0.023466 \tValidation Loss: 0.960964\n",
      "Epoch: 2454 \tTraining Loss: 0.021341 \tValidation Loss: 1.196899\n",
      "Epoch: 2455 \tTraining Loss: 0.003839 \tValidation Loss: 1.013630\n",
      "Epoch: 2456 \tTraining Loss: 0.007654 \tValidation Loss: 0.902541\n",
      "Epoch: 2457 \tTraining Loss: 0.004651 \tValidation Loss: 0.927361\n",
      "Epoch: 2458 \tTraining Loss: 0.005452 \tValidation Loss: 1.002146\n",
      "Epoch: 2459 \tTraining Loss: 0.002990 \tValidation Loss: 1.067491\n",
      "Epoch: 2460 \tTraining Loss: 0.003410 \tValidation Loss: 1.046578\n",
      "Epoch: 2461 \tTraining Loss: 0.008946 \tValidation Loss: 1.138423\n",
      "Epoch: 2462 \tTraining Loss: 0.006749 \tValidation Loss: 1.091531\n",
      "Epoch: 2463 \tTraining Loss: 0.004579 \tValidation Loss: 0.918921\n",
      "Epoch: 2464 \tTraining Loss: 0.002574 \tValidation Loss: 0.906839\n",
      "Epoch: 2465 \tTraining Loss: 0.002305 \tValidation Loss: 0.943721\n",
      "Epoch: 2466 \tTraining Loss: 0.004305 \tValidation Loss: 0.877983\n",
      "Epoch: 2467 \tTraining Loss: 0.001417 \tValidation Loss: 0.839361\n",
      "Epoch: 2468 \tTraining Loss: 0.033995 \tValidation Loss: 0.768884\n",
      "Epoch: 2469 \tTraining Loss: 0.004860 \tValidation Loss: 0.751715\n",
      "Epoch: 2470 \tTraining Loss: 0.010838 \tValidation Loss: 0.646542\n",
      "Epoch: 2471 \tTraining Loss: 0.005512 \tValidation Loss: 0.688901\n",
      "Epoch: 2472 \tTraining Loss: 0.006655 \tValidation Loss: 0.771325\n",
      "Epoch: 2473 \tTraining Loss: 0.004836 \tValidation Loss: 0.784079\n",
      "Epoch: 2474 \tTraining Loss: 0.002033 \tValidation Loss: 0.811676\n",
      "Epoch: 2475 \tTraining Loss: 0.002994 \tValidation Loss: 0.801736\n",
      "Epoch: 2476 \tTraining Loss: 0.001561 \tValidation Loss: 0.829272\n",
      "Epoch: 2477 \tTraining Loss: 0.002058 \tValidation Loss: 0.857398\n",
      "Epoch: 2478 \tTraining Loss: 0.001551 \tValidation Loss: 0.932821\n",
      "Epoch: 2479 \tTraining Loss: 0.002484 \tValidation Loss: 0.920533\n",
      "Epoch: 2480 \tTraining Loss: 0.003548 \tValidation Loss: 0.862006\n",
      "Epoch: 2481 \tTraining Loss: 0.001957 \tValidation Loss: 0.818125\n",
      "Epoch: 2482 \tTraining Loss: 0.001332 \tValidation Loss: 0.784701\n",
      "Epoch: 2483 \tTraining Loss: 0.001551 \tValidation Loss: 0.835881\n",
      "Epoch: 2484 \tTraining Loss: 0.001715 \tValidation Loss: 0.835496\n",
      "Epoch: 2485 \tTraining Loss: 0.001335 \tValidation Loss: 0.865774\n",
      "Epoch: 2486 \tTraining Loss: 0.002346 \tValidation Loss: 0.783616\n",
      "Epoch: 2487 \tTraining Loss: 0.001293 \tValidation Loss: 0.808317\n",
      "Epoch: 2488 \tTraining Loss: 0.001298 \tValidation Loss: 0.803219\n",
      "Epoch: 2489 \tTraining Loss: 0.001843 \tValidation Loss: 0.762747\n",
      "Epoch: 2490 \tTraining Loss: 0.002173 \tValidation Loss: 0.785548\n",
      "Epoch: 2491 \tTraining Loss: 0.001471 \tValidation Loss: 0.816023\n",
      "Epoch: 2492 \tTraining Loss: 0.001087 \tValidation Loss: 0.816454\n",
      "Epoch: 2493 \tTraining Loss: 0.001509 \tValidation Loss: 0.802285\n",
      "Epoch: 2494 \tTraining Loss: 0.001050 \tValidation Loss: 0.770841\n",
      "Epoch: 2495 \tTraining Loss: 0.000878 \tValidation Loss: 0.773947\n",
      "Epoch: 2496 \tTraining Loss: 0.001156 \tValidation Loss: 0.764313\n",
      "Epoch: 2497 \tTraining Loss: 0.001132 \tValidation Loss: 0.800017\n",
      "Epoch: 2498 \tTraining Loss: 0.001152 \tValidation Loss: 0.781451\n",
      "Epoch: 2499 \tTraining Loss: 0.000833 \tValidation Loss: 0.800725\n",
      "Epoch: 2500 \tTraining Loss: 0.001029 \tValidation Loss: 0.779258\n",
      "Epoch: 2501 \tTraining Loss: 0.000656 \tValidation Loss: 0.785303\n",
      "Epoch: 2502 \tTraining Loss: 0.002501 \tValidation Loss: 0.746085\n",
      "Epoch: 2503 \tTraining Loss: 0.070777 \tValidation Loss: 0.757705\n",
      "Epoch: 2504 \tTraining Loss: 0.010439 \tValidation Loss: 0.803392\n",
      "Epoch: 2505 \tTraining Loss: 0.004304 \tValidation Loss: 0.849139\n",
      "Epoch: 2506 \tTraining Loss: 0.007039 \tValidation Loss: 0.877065\n",
      "Epoch: 2507 \tTraining Loss: 0.025192 \tValidation Loss: 1.483392\n",
      "Epoch: 2508 \tTraining Loss: 0.045769 \tValidation Loss: 1.649265\n",
      "Epoch: 2509 \tTraining Loss: 0.137151 \tValidation Loss: 1.202940\n",
      "Epoch: 2510 \tTraining Loss: 0.520787 \tValidation Loss: 0.529575\n",
      "Epoch: 2511 \tTraining Loss: 0.392984 \tValidation Loss: 1.191562\n",
      "Epoch: 2512 \tTraining Loss: 0.452929 \tValidation Loss: 0.633186\n",
      "Epoch: 2513 \tTraining Loss: 0.181883 \tValidation Loss: 0.697582\n",
      "Epoch: 2514 \tTraining Loss: 0.083636 \tValidation Loss: 0.745702\n",
      "Epoch: 2515 \tTraining Loss: 0.079437 \tValidation Loss: 1.042691\n",
      "Epoch: 2516 \tTraining Loss: 0.039781 \tValidation Loss: 0.893028\n",
      "Epoch: 2517 \tTraining Loss: 0.022480 \tValidation Loss: 0.952066\n",
      "Epoch: 2518 \tTraining Loss: 0.047429 \tValidation Loss: 0.864274\n",
      "Epoch: 2519 \tTraining Loss: 0.054031 \tValidation Loss: 0.854315\n",
      "Epoch: 2520 \tTraining Loss: 0.110019 \tValidation Loss: 1.285381\n",
      "Epoch: 2521 \tTraining Loss: 0.052792 \tValidation Loss: 1.367090\n",
      "Epoch: 2522 \tTraining Loss: 0.040215 \tValidation Loss: 1.526783\n",
      "Epoch: 2523 \tTraining Loss: 0.021312 \tValidation Loss: 1.169594\n",
      "Epoch: 2524 \tTraining Loss: 0.009378 \tValidation Loss: 1.081671\n",
      "Epoch: 2525 \tTraining Loss: 0.012348 \tValidation Loss: 1.055958\n",
      "Epoch: 2526 \tTraining Loss: 0.010484 \tValidation Loss: 1.099959\n",
      "Epoch: 2527 \tTraining Loss: 0.007911 \tValidation Loss: 1.118761\n",
      "Epoch: 2528 \tTraining Loss: 0.006972 \tValidation Loss: 1.104687\n",
      "Epoch: 2529 \tTraining Loss: 0.022491 \tValidation Loss: 1.072583\n",
      "Epoch: 2530 \tTraining Loss: 0.018045 \tValidation Loss: 1.107916\n",
      "Epoch: 2531 \tTraining Loss: 0.012675 \tValidation Loss: 1.083079\n",
      "Epoch: 2532 \tTraining Loss: 0.012519 \tValidation Loss: 0.896676\n",
      "Epoch: 2533 \tTraining Loss: 0.002902 \tValidation Loss: 1.056087\n",
      "Epoch: 2534 \tTraining Loss: 0.003042 \tValidation Loss: 1.118305\n",
      "Epoch: 2535 \tTraining Loss: 0.006495 \tValidation Loss: 0.981371\n",
      "Epoch: 2536 \tTraining Loss: 0.002727 \tValidation Loss: 1.018688\n",
      "Epoch: 2537 \tTraining Loss: 0.004320 \tValidation Loss: 1.076231\n",
      "Epoch: 2538 \tTraining Loss: 0.004872 \tValidation Loss: 1.162627\n",
      "Epoch: 2539 \tTraining Loss: 0.003911 \tValidation Loss: 1.286732\n",
      "Epoch: 2540 \tTraining Loss: 0.003560 \tValidation Loss: 1.214708\n",
      "Epoch: 2541 \tTraining Loss: 0.001741 \tValidation Loss: 1.292148\n",
      "Epoch: 2542 \tTraining Loss: 0.002509 \tValidation Loss: 1.219940\n",
      "Epoch: 2543 \tTraining Loss: 0.001910 \tValidation Loss: 1.219380\n",
      "Epoch: 2544 \tTraining Loss: 0.002049 \tValidation Loss: 1.105990\n",
      "Epoch: 2545 \tTraining Loss: 0.002025 \tValidation Loss: 1.234389\n",
      "Epoch: 2546 \tTraining Loss: 0.001542 \tValidation Loss: 1.219884\n",
      "Epoch: 2547 \tTraining Loss: 0.001948 \tValidation Loss: 1.233131\n",
      "Epoch: 2548 \tTraining Loss: 0.002985 \tValidation Loss: 1.203012\n",
      "Epoch: 2549 \tTraining Loss: 0.001059 \tValidation Loss: 1.258599\n",
      "Epoch: 2550 \tTraining Loss: 0.002811 \tValidation Loss: 1.141036\n",
      "Epoch: 2551 \tTraining Loss: 0.001501 \tValidation Loss: 1.229278\n",
      "Epoch: 2552 \tTraining Loss: 0.001461 \tValidation Loss: 1.109041\n",
      "Epoch: 2553 \tTraining Loss: 0.003029 \tValidation Loss: 1.234831\n",
      "Epoch: 2554 \tTraining Loss: 0.001386 \tValidation Loss: 1.183356\n",
      "Epoch: 2555 \tTraining Loss: 0.001589 \tValidation Loss: 1.013874\n",
      "Epoch: 2556 \tTraining Loss: 0.003649 \tValidation Loss: 1.020088\n",
      "Epoch: 2557 \tTraining Loss: 0.001200 \tValidation Loss: 1.077990\n",
      "Epoch: 2558 \tTraining Loss: 0.002700 \tValidation Loss: 1.136947\n",
      "Epoch: 2559 \tTraining Loss: 0.002282 \tValidation Loss: 1.022720\n",
      "Epoch: 2560 \tTraining Loss: 0.001491 \tValidation Loss: 1.090492\n",
      "Epoch: 2561 \tTraining Loss: 0.003130 \tValidation Loss: 1.060932\n",
      "Epoch: 2562 \tTraining Loss: 0.001934 \tValidation Loss: 1.120392\n",
      "Epoch: 2563 \tTraining Loss: 0.001846 \tValidation Loss: 1.028959\n",
      "Epoch: 2564 \tTraining Loss: 0.001666 \tValidation Loss: 1.092022\n",
      "Epoch: 2565 \tTraining Loss: 0.001576 \tValidation Loss: 1.067673\n",
      "Epoch: 2566 \tTraining Loss: 0.001030 \tValidation Loss: 1.130818\n",
      "Epoch: 2567 \tTraining Loss: 0.001040 \tValidation Loss: 1.123200\n",
      "Epoch: 2568 \tTraining Loss: 0.001550 \tValidation Loss: 1.060100\n",
      "Epoch: 2569 \tTraining Loss: 0.001461 \tValidation Loss: 1.105923\n",
      "Epoch: 2570 \tTraining Loss: 0.002422 \tValidation Loss: 1.061777\n",
      "Epoch: 2571 \tTraining Loss: 0.001363 \tValidation Loss: 1.245113\n",
      "Epoch: 2572 \tTraining Loss: 0.001085 \tValidation Loss: 1.093994\n",
      "Epoch: 2573 \tTraining Loss: 0.002892 \tValidation Loss: 1.204401\n",
      "Epoch: 2574 \tTraining Loss: 0.002903 \tValidation Loss: 1.048297\n",
      "Epoch: 2575 \tTraining Loss: 0.001463 \tValidation Loss: 0.994803\n",
      "Epoch: 2576 \tTraining Loss: 0.001969 \tValidation Loss: 1.031381\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2577 \tTraining Loss: 0.001611 \tValidation Loss: 1.042952\n",
      "Epoch: 2578 \tTraining Loss: 0.001877 \tValidation Loss: 1.148545\n",
      "Epoch: 2579 \tTraining Loss: 0.000830 \tValidation Loss: 1.254552\n",
      "Epoch: 2580 \tTraining Loss: 0.000905 \tValidation Loss: 1.159709\n",
      "Epoch: 2581 \tTraining Loss: 0.001009 \tValidation Loss: 1.052427\n",
      "Epoch: 2582 \tTraining Loss: 0.000591 \tValidation Loss: 1.321298\n",
      "Epoch: 2583 \tTraining Loss: 0.001315 \tValidation Loss: 1.247214\n",
      "Epoch: 2584 \tTraining Loss: 0.001606 \tValidation Loss: 1.075740\n",
      "Epoch: 2585 \tTraining Loss: 0.001421 \tValidation Loss: 1.101417\n",
      "Epoch: 2586 \tTraining Loss: 0.002303 \tValidation Loss: 1.196392\n",
      "Epoch: 2587 \tTraining Loss: 0.000938 \tValidation Loss: 1.052445\n",
      "Epoch: 2588 \tTraining Loss: 0.001059 \tValidation Loss: 1.066909\n",
      "Epoch: 2589 \tTraining Loss: 0.001734 \tValidation Loss: 1.120096\n",
      "Epoch: 2590 \tTraining Loss: 0.001605 \tValidation Loss: 1.173122\n",
      "Epoch: 2591 \tTraining Loss: 0.000648 \tValidation Loss: 1.150023\n",
      "Epoch: 2592 \tTraining Loss: 0.002445 \tValidation Loss: 1.051790\n",
      "Epoch: 2593 \tTraining Loss: 0.000985 \tValidation Loss: 1.269755\n",
      "Epoch: 2594 \tTraining Loss: 0.001320 \tValidation Loss: 1.238277\n",
      "Epoch: 2595 \tTraining Loss: 0.000974 \tValidation Loss: 1.309279\n",
      "Epoch: 2596 \tTraining Loss: 0.000694 \tValidation Loss: 1.291116\n",
      "Epoch: 2597 \tTraining Loss: 0.001361 \tValidation Loss: 1.126737\n",
      "Epoch: 2598 \tTraining Loss: 0.000955 \tValidation Loss: 1.225656\n",
      "Epoch: 2599 \tTraining Loss: 0.000915 \tValidation Loss: 1.099711\n",
      "Epoch: 2600 \tTraining Loss: 0.000772 \tValidation Loss: 1.103443\n",
      "Epoch: 2601 \tTraining Loss: 0.000728 \tValidation Loss: 1.188866\n",
      "Epoch: 2602 \tTraining Loss: 0.000735 \tValidation Loss: 1.226723\n",
      "Epoch: 2603 \tTraining Loss: 0.001871 \tValidation Loss: 1.218396\n",
      "Epoch: 2604 \tTraining Loss: 0.001232 \tValidation Loss: 1.301717\n",
      "Epoch: 2605 \tTraining Loss: 0.001360 \tValidation Loss: 1.275302\n",
      "Epoch: 2606 \tTraining Loss: 0.000940 \tValidation Loss: 1.179900\n",
      "Epoch: 2607 \tTraining Loss: 0.000552 \tValidation Loss: 1.202937\n",
      "Epoch: 2608 \tTraining Loss: 0.001042 \tValidation Loss: 1.097628\n",
      "Epoch: 2609 \tTraining Loss: 0.000801 \tValidation Loss: 1.070713\n",
      "Epoch: 2610 \tTraining Loss: 0.000957 \tValidation Loss: 1.193398\n",
      "Epoch: 2611 \tTraining Loss: 0.000730 \tValidation Loss: 1.292631\n",
      "Epoch: 2612 \tTraining Loss: 0.001022 \tValidation Loss: 1.208939\n",
      "Epoch: 2613 \tTraining Loss: 0.001114 \tValidation Loss: 1.360153\n",
      "Epoch: 2614 \tTraining Loss: 0.000525 \tValidation Loss: 1.305468\n",
      "Epoch: 2615 \tTraining Loss: 0.000895 \tValidation Loss: 1.177895\n",
      "Epoch: 2616 \tTraining Loss: 0.000844 \tValidation Loss: 1.300389\n",
      "Epoch: 2617 \tTraining Loss: 0.000830 \tValidation Loss: 1.132819\n",
      "Epoch: 2618 \tTraining Loss: 0.000705 \tValidation Loss: 1.234278\n",
      "Epoch: 2619 \tTraining Loss: 0.001426 \tValidation Loss: 1.347863\n",
      "Epoch: 2620 \tTraining Loss: 0.001260 \tValidation Loss: 1.025258\n",
      "Epoch: 2621 \tTraining Loss: 0.005898 \tValidation Loss: 1.222394\n",
      "Epoch: 2622 \tTraining Loss: 0.001566 \tValidation Loss: 1.113193\n",
      "Epoch: 2623 \tTraining Loss: 0.001209 \tValidation Loss: 1.067968\n",
      "Epoch: 2624 \tTraining Loss: 0.002516 \tValidation Loss: 1.067421\n",
      "Epoch: 2625 \tTraining Loss: 0.001461 \tValidation Loss: 1.150461\n",
      "Epoch: 2626 \tTraining Loss: 0.001351 \tValidation Loss: 1.075544\n",
      "Epoch: 2627 \tTraining Loss: 0.000847 \tValidation Loss: 1.064249\n",
      "Epoch: 2628 \tTraining Loss: 0.001344 \tValidation Loss: 1.222228\n",
      "Epoch: 2629 \tTraining Loss: 0.001074 \tValidation Loss: 1.097344\n",
      "Epoch: 2630 \tTraining Loss: 0.001946 \tValidation Loss: 1.087983\n",
      "Epoch: 2631 \tTraining Loss: 0.000873 \tValidation Loss: 1.090592\n",
      "Epoch: 2632 \tTraining Loss: 0.003074 \tValidation Loss: 0.991104\n",
      "Epoch: 2633 \tTraining Loss: 0.003606 \tValidation Loss: 1.097171\n",
      "Epoch: 2634 \tTraining Loss: 0.100858 \tValidation Loss: 6.096508\n",
      "Epoch: 2635 \tTraining Loss: 0.128798 \tValidation Loss: 1.492537\n",
      "Epoch: 2636 \tTraining Loss: 0.476380 \tValidation Loss: 0.808295\n",
      "Epoch: 2637 \tTraining Loss: 0.225182 \tValidation Loss: 0.994286\n",
      "Epoch: 2638 \tTraining Loss: 0.196849 \tValidation Loss: 1.304941\n",
      "Epoch: 2639 \tTraining Loss: 0.235442 \tValidation Loss: 0.858134\n",
      "Epoch: 2640 \tTraining Loss: 0.128724 \tValidation Loss: 1.237784\n",
      "Epoch: 2641 \tTraining Loss: 0.045893 \tValidation Loss: 1.133679\n",
      "Epoch: 2642 \tTraining Loss: 0.153333 \tValidation Loss: 1.100109\n",
      "Epoch: 2643 \tTraining Loss: 0.381579 \tValidation Loss: 1.082290\n",
      "Epoch: 2644 \tTraining Loss: 0.164577 \tValidation Loss: 1.092271\n",
      "Epoch: 2645 \tTraining Loss: 0.123530 \tValidation Loss: 1.041671\n",
      "Epoch: 2646 \tTraining Loss: 0.069810 \tValidation Loss: 0.875556\n",
      "Epoch: 2647 \tTraining Loss: 0.125698 \tValidation Loss: 0.963426\n",
      "Epoch: 2648 \tTraining Loss: 0.117864 \tValidation Loss: 0.761947\n",
      "Epoch: 2649 \tTraining Loss: 0.067959 \tValidation Loss: 0.991963\n",
      "Epoch: 2650 \tTraining Loss: 0.038219 \tValidation Loss: 0.752220\n",
      "Epoch: 2651 \tTraining Loss: 0.022000 \tValidation Loss: 0.889728\n",
      "Epoch: 2652 \tTraining Loss: 0.030189 \tValidation Loss: 1.155150\n",
      "Epoch: 2653 \tTraining Loss: 0.008831 \tValidation Loss: 0.913540\n",
      "Epoch: 2654 \tTraining Loss: 0.007779 \tValidation Loss: 0.954635\n",
      "Epoch: 2655 \tTraining Loss: 0.029917 \tValidation Loss: 1.349092\n",
      "Epoch: 2656 \tTraining Loss: 0.067031 \tValidation Loss: 1.016497\n",
      "Epoch: 2657 \tTraining Loss: 0.169667 \tValidation Loss: 0.799944\n",
      "Epoch: 2658 \tTraining Loss: 0.078913 \tValidation Loss: 1.452234\n",
      "Epoch: 2659 \tTraining Loss: 0.037321 \tValidation Loss: 1.367944\n",
      "Epoch: 2660 \tTraining Loss: 0.043906 \tValidation Loss: 1.159106\n",
      "Epoch: 2661 \tTraining Loss: 0.021014 \tValidation Loss: 1.245516\n",
      "Epoch: 2662 \tTraining Loss: 0.016962 \tValidation Loss: 1.255114\n",
      "Epoch: 2663 \tTraining Loss: 0.005555 \tValidation Loss: 1.134656\n",
      "Epoch: 2664 \tTraining Loss: 0.042341 \tValidation Loss: 1.330360\n",
      "Epoch: 2665 \tTraining Loss: 0.100057 \tValidation Loss: 1.630328\n",
      "Epoch: 2666 \tTraining Loss: 0.106397 \tValidation Loss: 1.290858\n",
      "Epoch: 2667 \tTraining Loss: 0.088382 \tValidation Loss: 1.238598\n",
      "Epoch: 2668 \tTraining Loss: 0.034617 \tValidation Loss: 1.409221\n",
      "Epoch: 2669 \tTraining Loss: 0.048629 \tValidation Loss: 1.000579\n",
      "Epoch: 2670 \tTraining Loss: 0.050011 \tValidation Loss: 1.550911\n",
      "Epoch: 2671 \tTraining Loss: 0.022292 \tValidation Loss: 1.736786\n",
      "Epoch: 2672 \tTraining Loss: 0.021856 \tValidation Loss: 1.709286\n",
      "Epoch: 2673 \tTraining Loss: 0.009368 \tValidation Loss: 1.559195\n",
      "Epoch: 2674 \tTraining Loss: 0.021654 \tValidation Loss: 1.540409\n",
      "Epoch: 2675 \tTraining Loss: 0.010824 \tValidation Loss: 1.608103\n",
      "Epoch: 2676 \tTraining Loss: 0.005322 \tValidation Loss: 1.636705\n",
      "Epoch: 2677 \tTraining Loss: 0.003481 \tValidation Loss: 1.692823\n",
      "Epoch: 2678 \tTraining Loss: 0.021126 \tValidation Loss: 1.385666\n",
      "Epoch: 2679 \tTraining Loss: 0.011938 \tValidation Loss: 1.451287\n",
      "Epoch: 2680 \tTraining Loss: 0.005688 \tValidation Loss: 1.454606\n",
      "Epoch: 2681 \tTraining Loss: 0.006275 \tValidation Loss: 1.444916\n",
      "Epoch: 2682 \tTraining Loss: 0.004480 \tValidation Loss: 1.488327\n",
      "Epoch: 2683 \tTraining Loss: 0.005324 \tValidation Loss: 1.486483\n",
      "Epoch: 2684 \tTraining Loss: 0.003317 \tValidation Loss: 1.529711\n",
      "Epoch: 2685 \tTraining Loss: 0.004006 \tValidation Loss: 1.456156\n",
      "Epoch: 2686 \tTraining Loss: 0.004283 \tValidation Loss: 1.426897\n",
      "Epoch: 2687 \tTraining Loss: 0.003752 \tValidation Loss: 1.493714\n",
      "Epoch: 2688 \tTraining Loss: 0.002421 \tValidation Loss: 1.506305\n",
      "Epoch: 2689 \tTraining Loss: 0.002757 \tValidation Loss: 1.550603\n",
      "Epoch: 2690 \tTraining Loss: 0.002651 \tValidation Loss: 1.675090\n",
      "Epoch: 2691 \tTraining Loss: 0.002231 \tValidation Loss: 1.599097\n",
      "Epoch: 2692 \tTraining Loss: 0.002825 \tValidation Loss: 1.667044\n",
      "Epoch: 2693 \tTraining Loss: 0.003045 \tValidation Loss: 1.485475\n",
      "Epoch: 2694 \tTraining Loss: 0.001703 \tValidation Loss: 1.526112\n",
      "Epoch: 2695 \tTraining Loss: 0.010650 \tValidation Loss: 1.407696\n",
      "Epoch: 2696 \tTraining Loss: 0.054373 \tValidation Loss: 1.887794\n",
      "Epoch: 2697 \tTraining Loss: 0.030287 \tValidation Loss: 2.603852\n",
      "Epoch: 2698 \tTraining Loss: 0.027704 \tValidation Loss: 1.837002\n",
      "Epoch: 2699 \tTraining Loss: 0.023719 \tValidation Loss: 1.758466\n",
      "Epoch: 2700 \tTraining Loss: 0.108634 \tValidation Loss: 1.862108\n",
      "Epoch: 2701 \tTraining Loss: 0.149751 \tValidation Loss: 1.451413\n",
      "Epoch: 2702 \tTraining Loss: 0.089838 \tValidation Loss: 1.524148\n",
      "Epoch: 2703 \tTraining Loss: 0.020212 \tValidation Loss: 1.324336\n",
      "Epoch: 2704 \tTraining Loss: 0.074905 \tValidation Loss: 1.514528\n",
      "Epoch: 2705 \tTraining Loss: 0.013156 \tValidation Loss: 1.592203\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2706 \tTraining Loss: 0.015299 \tValidation Loss: 1.623005\n",
      "Epoch: 2707 \tTraining Loss: 0.007236 \tValidation Loss: 1.652006\n",
      "Epoch: 2708 \tTraining Loss: 0.008618 \tValidation Loss: 1.654893\n",
      "Epoch: 2709 \tTraining Loss: 0.005355 \tValidation Loss: 1.441664\n",
      "Epoch: 2710 \tTraining Loss: 0.010846 \tValidation Loss: 1.294726\n",
      "Epoch: 2711 \tTraining Loss: 0.008656 \tValidation Loss: 1.361304\n",
      "Epoch: 2712 \tTraining Loss: 0.004163 \tValidation Loss: 1.442423\n",
      "Epoch: 2713 \tTraining Loss: 0.036273 \tValidation Loss: 1.475934\n",
      "Epoch: 2714 \tTraining Loss: 0.006955 \tValidation Loss: 1.345081\n",
      "Epoch: 2715 \tTraining Loss: 0.005931 \tValidation Loss: 1.295678\n",
      "Epoch: 2716 \tTraining Loss: 0.005010 \tValidation Loss: 1.416841\n",
      "Epoch: 2717 \tTraining Loss: 0.002171 \tValidation Loss: 1.393845\n",
      "Epoch: 2718 \tTraining Loss: 0.002322 \tValidation Loss: 1.491790\n",
      "Epoch: 2719 \tTraining Loss: 0.006100 \tValidation Loss: 1.415854\n",
      "Epoch: 2720 \tTraining Loss: 0.002945 \tValidation Loss: 1.244819\n",
      "Epoch: 2721 \tTraining Loss: 0.003510 \tValidation Loss: 1.277567\n",
      "Epoch: 2722 \tTraining Loss: 0.004979 \tValidation Loss: 1.226845\n",
      "Epoch: 2723 \tTraining Loss: 0.009302 \tValidation Loss: 1.243795\n",
      "Epoch: 2724 \tTraining Loss: 0.002815 \tValidation Loss: 1.245591\n",
      "Epoch: 2725 \tTraining Loss: 0.006499 \tValidation Loss: 1.219907\n",
      "Epoch: 2726 \tTraining Loss: 0.005091 \tValidation Loss: 1.224854\n",
      "Epoch: 2727 \tTraining Loss: 0.001972 \tValidation Loss: 1.295907\n",
      "Epoch: 2728 \tTraining Loss: 0.002200 \tValidation Loss: 1.255806\n",
      "Epoch: 2729 \tTraining Loss: 0.002613 \tValidation Loss: 1.183332\n",
      "Epoch: 2730 \tTraining Loss: 0.000916 \tValidation Loss: 1.276121\n",
      "Epoch: 2731 \tTraining Loss: 0.006403 \tValidation Loss: 1.151168\n",
      "Epoch: 2732 \tTraining Loss: 0.002192 \tValidation Loss: 1.201664\n",
      "Epoch: 2733 \tTraining Loss: 0.001936 \tValidation Loss: 1.101035\n",
      "Epoch: 2734 \tTraining Loss: 0.002637 \tValidation Loss: 1.216673\n",
      "Epoch: 2735 \tTraining Loss: 0.005515 \tValidation Loss: 1.211887\n",
      "Epoch: 2736 \tTraining Loss: 0.001053 \tValidation Loss: 1.092510\n",
      "Epoch: 2737 \tTraining Loss: 0.002302 \tValidation Loss: 1.150841\n",
      "Epoch: 2738 \tTraining Loss: 0.001301 \tValidation Loss: 1.106923\n",
      "Epoch: 2739 \tTraining Loss: 0.003788 \tValidation Loss: 1.099238\n",
      "Epoch: 2740 \tTraining Loss: 0.005795 \tValidation Loss: 1.289620\n",
      "Epoch: 2741 \tTraining Loss: 0.005151 \tValidation Loss: 1.151206\n",
      "Epoch: 2742 \tTraining Loss: 0.003860 \tValidation Loss: 1.215290\n",
      "Epoch: 2743 \tTraining Loss: 0.030628 \tValidation Loss: 1.431953\n",
      "Epoch: 2744 \tTraining Loss: 0.206987 \tValidation Loss: 1.161846\n",
      "Epoch: 2745 \tTraining Loss: 0.247063 \tValidation Loss: 1.072720\n",
      "Epoch: 2746 \tTraining Loss: 0.135792 \tValidation Loss: 0.810951\n",
      "Epoch: 2747 \tTraining Loss: 0.052405 \tValidation Loss: 0.924688\n",
      "Epoch: 2748 \tTraining Loss: 0.046236 \tValidation Loss: 0.894117\n",
      "Epoch: 2749 \tTraining Loss: 0.029861 \tValidation Loss: 1.087605\n",
      "Epoch: 2750 \tTraining Loss: 0.006565 \tValidation Loss: 1.115834\n",
      "Epoch: 2751 \tTraining Loss: 0.005970 \tValidation Loss: 1.091908\n",
      "Epoch: 2752 \tTraining Loss: 0.006568 \tValidation Loss: 1.105376\n",
      "Epoch: 2753 \tTraining Loss: 0.044657 \tValidation Loss: 1.206356\n",
      "Epoch: 2754 \tTraining Loss: 0.015205 \tValidation Loss: 1.257502\n",
      "Epoch: 2755 \tTraining Loss: 0.011064 \tValidation Loss: 1.279746\n",
      "Epoch: 2756 \tTraining Loss: 0.005481 \tValidation Loss: 1.275087\n",
      "Epoch: 2757 \tTraining Loss: 0.004103 \tValidation Loss: 1.213928\n",
      "Epoch: 2758 \tTraining Loss: 0.009335 \tValidation Loss: 1.147256\n",
      "Epoch: 2759 \tTraining Loss: 0.022414 \tValidation Loss: 1.060865\n",
      "Epoch: 2760 \tTraining Loss: 0.030567 \tValidation Loss: 1.157703\n",
      "Epoch: 2761 \tTraining Loss: 0.015366 \tValidation Loss: 1.077335\n",
      "Epoch: 2762 \tTraining Loss: 0.023642 \tValidation Loss: 1.392508\n",
      "Epoch: 2763 \tTraining Loss: 0.089283 \tValidation Loss: 1.079735\n",
      "Epoch: 2764 \tTraining Loss: 0.023388 \tValidation Loss: 1.323263\n",
      "Epoch: 2765 \tTraining Loss: 0.007328 \tValidation Loss: 1.067053\n",
      "Epoch: 2766 \tTraining Loss: 0.004009 \tValidation Loss: 1.099582\n",
      "Epoch: 2767 \tTraining Loss: 0.006052 \tValidation Loss: 0.996400\n",
      "Epoch: 2768 \tTraining Loss: 0.008691 \tValidation Loss: 1.121935\n",
      "Epoch: 2769 \tTraining Loss: 0.003891 \tValidation Loss: 1.063174\n",
      "Epoch: 2770 \tTraining Loss: 0.002574 \tValidation Loss: 1.118384\n",
      "Epoch: 2771 \tTraining Loss: 0.003480 \tValidation Loss: 1.025371\n",
      "Epoch: 2772 \tTraining Loss: 0.002767 \tValidation Loss: 1.057858\n",
      "Epoch: 2773 \tTraining Loss: 0.004351 \tValidation Loss: 1.061748\n",
      "Epoch: 2774 \tTraining Loss: 0.001968 \tValidation Loss: 1.174158\n",
      "Epoch: 2775 \tTraining Loss: 0.006133 \tValidation Loss: 1.109712\n",
      "Epoch: 2776 \tTraining Loss: 0.001999 \tValidation Loss: 1.068156\n",
      "Epoch: 2777 \tTraining Loss: 0.003680 \tValidation Loss: 1.031702\n",
      "Epoch: 2778 \tTraining Loss: 0.002272 \tValidation Loss: 1.096150\n",
      "Epoch: 2779 \tTraining Loss: 0.001095 \tValidation Loss: 1.112640\n",
      "Epoch: 2780 \tTraining Loss: 0.002290 \tValidation Loss: 1.087278\n",
      "Epoch: 2781 \tTraining Loss: 0.002164 \tValidation Loss: 1.080681\n",
      "Epoch: 2782 \tTraining Loss: 0.001789 \tValidation Loss: 1.119046\n",
      "Epoch: 2783 \tTraining Loss: 0.006167 \tValidation Loss: 1.167853\n",
      "Epoch: 2784 \tTraining Loss: 0.001909 \tValidation Loss: 1.082588\n",
      "Epoch: 2785 \tTraining Loss: 0.002774 \tValidation Loss: 1.086992\n",
      "Epoch: 2786 \tTraining Loss: 0.003014 \tValidation Loss: 1.104812\n",
      "Epoch: 2787 \tTraining Loss: 0.000983 \tValidation Loss: 1.122494\n",
      "Epoch: 2788 \tTraining Loss: 0.002469 \tValidation Loss: 1.114626\n",
      "Epoch: 2789 \tTraining Loss: 0.004862 \tValidation Loss: 1.103838\n",
      "Epoch: 2790 \tTraining Loss: 0.003605 \tValidation Loss: 1.118841\n",
      "Epoch: 2791 \tTraining Loss: 0.003637 \tValidation Loss: 1.042487\n",
      "Epoch: 2792 \tTraining Loss: 0.003183 \tValidation Loss: 1.053728\n",
      "Epoch: 2793 \tTraining Loss: 0.190686 \tValidation Loss: 1.010987\n",
      "Epoch: 2794 \tTraining Loss: 0.079564 \tValidation Loss: 0.870179\n",
      "Epoch: 2795 \tTraining Loss: 0.027086 \tValidation Loss: 1.387830\n",
      "Epoch: 2796 \tTraining Loss: 0.020825 \tValidation Loss: 1.180181\n",
      "Epoch: 2797 \tTraining Loss: 0.065496 \tValidation Loss: 1.329213\n",
      "Epoch: 2798 \tTraining Loss: 0.076578 \tValidation Loss: 1.324148\n",
      "Epoch: 2799 \tTraining Loss: 0.122942 \tValidation Loss: 1.660937\n",
      "Epoch: 2800 \tTraining Loss: 0.127694 \tValidation Loss: 1.071792\n",
      "Epoch: 2801 \tTraining Loss: 0.052975 \tValidation Loss: 1.295312\n",
      "Epoch: 2802 \tTraining Loss: 0.103668 \tValidation Loss: 0.956430\n",
      "Epoch: 2803 \tTraining Loss: 0.020992 \tValidation Loss: 0.695636\n",
      "Epoch: 2804 \tTraining Loss: 0.008152 \tValidation Loss: 0.735007\n",
      "Epoch: 2805 \tTraining Loss: 0.004536 \tValidation Loss: 0.784284\n",
      "Epoch: 2806 \tTraining Loss: 0.007482 \tValidation Loss: 0.842833\n",
      "Epoch: 2807 \tTraining Loss: 0.003005 \tValidation Loss: 0.847443\n",
      "Epoch: 2808 \tTraining Loss: 0.005009 \tValidation Loss: 0.908559\n",
      "Epoch: 2809 \tTraining Loss: 0.003160 \tValidation Loss: 0.890670\n",
      "Epoch: 2810 \tTraining Loss: 0.007091 \tValidation Loss: 0.924562\n",
      "Epoch: 2811 \tTraining Loss: 0.006815 \tValidation Loss: 0.778982\n",
      "Epoch: 2812 \tTraining Loss: 0.003713 \tValidation Loss: 0.748234\n",
      "Epoch: 2813 \tTraining Loss: 0.001798 \tValidation Loss: 0.725343\n",
      "Epoch: 2814 \tTraining Loss: 0.004536 \tValidation Loss: 0.726928\n",
      "Epoch: 2815 \tTraining Loss: 0.005752 \tValidation Loss: 0.873817\n",
      "Epoch: 2816 \tTraining Loss: 0.001849 \tValidation Loss: 0.808764\n",
      "Epoch: 2817 \tTraining Loss: 0.001560 \tValidation Loss: 0.834365\n",
      "Epoch: 2818 \tTraining Loss: 0.001818 \tValidation Loss: 0.778098\n",
      "Epoch: 2819 \tTraining Loss: 0.002039 \tValidation Loss: 0.826044\n",
      "Epoch: 2820 \tTraining Loss: 0.001900 \tValidation Loss: 0.807439\n",
      "Epoch: 2821 \tTraining Loss: 0.002734 \tValidation Loss: 0.905188\n",
      "Epoch: 2822 \tTraining Loss: 0.003022 \tValidation Loss: 0.834003\n",
      "Epoch: 2823 \tTraining Loss: 0.002483 \tValidation Loss: 0.906868\n",
      "Epoch: 2824 \tTraining Loss: 0.001622 \tValidation Loss: 0.917156\n",
      "Epoch: 2825 \tTraining Loss: 0.001577 \tValidation Loss: 0.889870\n",
      "Epoch: 2826 \tTraining Loss: 0.002338 \tValidation Loss: 0.911584\n",
      "Epoch: 2827 \tTraining Loss: 0.004287 \tValidation Loss: 1.002177\n",
      "Epoch: 2828 \tTraining Loss: 0.001775 \tValidation Loss: 1.038100\n",
      "Epoch: 2829 \tTraining Loss: 0.001764 \tValidation Loss: 0.935556\n",
      "Epoch: 2830 \tTraining Loss: 0.004747 \tValidation Loss: 1.113203\n",
      "Epoch: 2831 \tTraining Loss: 0.001677 \tValidation Loss: 1.055523\n",
      "Epoch: 2832 \tTraining Loss: 0.002450 \tValidation Loss: 0.998701\n",
      "Epoch: 2833 \tTraining Loss: 0.001452 \tValidation Loss: 1.124255\n",
      "Epoch: 2834 \tTraining Loss: 0.001988 \tValidation Loss: 1.110347\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2835 \tTraining Loss: 0.001502 \tValidation Loss: 1.093996\n",
      "Epoch: 2836 \tTraining Loss: 0.002344 \tValidation Loss: 1.093498\n",
      "Epoch: 2837 \tTraining Loss: 0.007357 \tValidation Loss: 0.894852\n",
      "Epoch: 2838 \tTraining Loss: 0.001705 \tValidation Loss: 0.776315\n",
      "Epoch: 2839 \tTraining Loss: 0.003894 \tValidation Loss: 0.864909\n",
      "Epoch: 2840 \tTraining Loss: 0.001403 \tValidation Loss: 0.808670\n",
      "Epoch: 2841 \tTraining Loss: 0.001583 \tValidation Loss: 0.863028\n",
      "Epoch: 2842 \tTraining Loss: 0.004752 \tValidation Loss: 0.750566\n",
      "Epoch: 2843 \tTraining Loss: 0.001821 \tValidation Loss: 0.741259\n",
      "Epoch: 2844 \tTraining Loss: 0.016913 \tValidation Loss: 0.892424\n",
      "Epoch: 2845 \tTraining Loss: 0.005352 \tValidation Loss: 0.843472\n",
      "Epoch: 2846 \tTraining Loss: 0.028767 \tValidation Loss: 0.836089\n",
      "Epoch: 2847 \tTraining Loss: 0.090046 \tValidation Loss: 2.036305\n",
      "Epoch: 2848 \tTraining Loss: 0.007610 \tValidation Loss: 1.453599\n",
      "Epoch: 2849 \tTraining Loss: 0.105157 \tValidation Loss: 0.791784\n",
      "Epoch: 2850 \tTraining Loss: 0.158929 \tValidation Loss: 1.078484\n",
      "Epoch: 2851 \tTraining Loss: 0.261289 \tValidation Loss: 1.125345\n",
      "Epoch: 2852 \tTraining Loss: 0.087171 \tValidation Loss: 1.186686\n",
      "Epoch: 2853 \tTraining Loss: 0.040635 \tValidation Loss: 0.895164\n",
      "Epoch: 2854 \tTraining Loss: 0.052247 \tValidation Loss: 0.933070\n",
      "Epoch: 2855 \tTraining Loss: 0.024669 \tValidation Loss: 0.730140\n",
      "Epoch: 2856 \tTraining Loss: 0.024475 \tValidation Loss: 0.879160\n",
      "Epoch: 2857 \tTraining Loss: 0.004479 \tValidation Loss: 0.889919\n",
      "Epoch: 2858 \tTraining Loss: 0.008160 \tValidation Loss: 0.864333\n",
      "Epoch: 2859 \tTraining Loss: 0.010396 \tValidation Loss: 0.816007\n",
      "Epoch: 2860 \tTraining Loss: 0.004962 \tValidation Loss: 0.873274\n",
      "Epoch: 2861 \tTraining Loss: 0.003174 \tValidation Loss: 0.827865\n",
      "Epoch: 2862 \tTraining Loss: 0.013347 \tValidation Loss: 0.850720\n",
      "Epoch: 2863 \tTraining Loss: 0.009144 \tValidation Loss: 1.016199\n",
      "Epoch: 2864 \tTraining Loss: 0.009944 \tValidation Loss: 0.829365\n",
      "Epoch: 2865 \tTraining Loss: 0.006424 \tValidation Loss: 0.835076\n",
      "Epoch: 2866 \tTraining Loss: 0.002161 \tValidation Loss: 0.831705\n",
      "Epoch: 2867 \tTraining Loss: 0.001695 \tValidation Loss: 0.823030\n",
      "Epoch: 2868 \tTraining Loss: 0.001956 \tValidation Loss: 0.832701\n",
      "Epoch: 2869 \tTraining Loss: 0.003766 \tValidation Loss: 0.819152\n",
      "Epoch: 2870 \tTraining Loss: 0.013265 \tValidation Loss: 0.873886\n",
      "Epoch: 2871 \tTraining Loss: 0.010380 \tValidation Loss: 0.814451\n",
      "Epoch: 2872 \tTraining Loss: 0.004140 \tValidation Loss: 0.829525\n",
      "Epoch: 2873 \tTraining Loss: 0.001848 \tValidation Loss: 0.788477\n",
      "Epoch: 2874 \tTraining Loss: 0.002340 \tValidation Loss: 0.834003\n",
      "Epoch: 2875 \tTraining Loss: 0.001996 \tValidation Loss: 0.814038\n",
      "Epoch: 2876 \tTraining Loss: 0.001718 \tValidation Loss: 0.793732\n",
      "Epoch: 2877 \tTraining Loss: 0.004972 \tValidation Loss: 0.860700\n",
      "Epoch: 2878 \tTraining Loss: 0.004212 \tValidation Loss: 0.861651\n",
      "Epoch: 2879 \tTraining Loss: 0.003403 \tValidation Loss: 0.810233\n",
      "Epoch: 2880 \tTraining Loss: 0.002002 \tValidation Loss: 0.858094\n",
      "Epoch: 2881 \tTraining Loss: 0.004113 \tValidation Loss: 0.851504\n",
      "Epoch: 2882 \tTraining Loss: 0.002412 \tValidation Loss: 0.860341\n",
      "Epoch: 2883 \tTraining Loss: 0.001462 \tValidation Loss: 0.812019\n",
      "Epoch: 2884 \tTraining Loss: 0.002361 \tValidation Loss: 0.809543\n",
      "Epoch: 2885 \tTraining Loss: 0.001772 \tValidation Loss: 0.852544\n",
      "Epoch: 2886 \tTraining Loss: 0.000925 \tValidation Loss: 0.846535\n",
      "Epoch: 2887 \tTraining Loss: 0.001513 \tValidation Loss: 0.910250\n",
      "Epoch: 2888 \tTraining Loss: 0.000880 \tValidation Loss: 0.945317\n",
      "Epoch: 2889 \tTraining Loss: 0.002705 \tValidation Loss: 0.845901\n",
      "Epoch: 2890 \tTraining Loss: 0.001509 \tValidation Loss: 0.871593\n",
      "Epoch: 2891 \tTraining Loss: 0.000748 \tValidation Loss: 0.867767\n",
      "Epoch: 2892 \tTraining Loss: 0.013369 \tValidation Loss: 0.955989\n",
      "Epoch: 2893 \tTraining Loss: 0.005226 \tValidation Loss: 1.056986\n",
      "Epoch: 2894 \tTraining Loss: 0.005188 \tValidation Loss: 1.094403\n",
      "Epoch: 2895 \tTraining Loss: 0.007573 \tValidation Loss: 1.271984\n",
      "Epoch: 2896 \tTraining Loss: 0.012739 \tValidation Loss: 0.897645\n",
      "Epoch: 2897 \tTraining Loss: 0.027757 \tValidation Loss: 1.096229\n",
      "Epoch: 2898 \tTraining Loss: 0.197209 \tValidation Loss: 1.919720\n",
      "Epoch: 2899 \tTraining Loss: 0.371833 \tValidation Loss: 1.448966\n",
      "Epoch: 2900 \tTraining Loss: 0.112387 \tValidation Loss: 0.773075\n",
      "Epoch: 2901 \tTraining Loss: 0.033469 \tValidation Loss: 0.797071\n",
      "Epoch: 2902 \tTraining Loss: 0.025677 \tValidation Loss: 1.204410\n",
      "Epoch: 2903 \tTraining Loss: 0.029912 \tValidation Loss: 0.821389\n",
      "Epoch: 2904 \tTraining Loss: 0.019398 \tValidation Loss: 0.863656\n",
      "Epoch: 2905 \tTraining Loss: 0.008851 \tValidation Loss: 0.867038\n",
      "Epoch: 2906 \tTraining Loss: 0.007550 \tValidation Loss: 1.026852\n",
      "Epoch: 2907 \tTraining Loss: 0.005926 \tValidation Loss: 0.973262\n",
      "Epoch: 2908 \tTraining Loss: 0.008091 \tValidation Loss: 0.937722\n",
      "Epoch: 2909 \tTraining Loss: 0.006668 \tValidation Loss: 0.852496\n",
      "Epoch: 2910 \tTraining Loss: 0.001676 \tValidation Loss: 0.931205\n",
      "Epoch: 2911 \tTraining Loss: 0.002477 \tValidation Loss: 0.947227\n",
      "Epoch: 2912 \tTraining Loss: 0.002666 \tValidation Loss: 0.912467\n",
      "Epoch: 2913 \tTraining Loss: 0.002748 \tValidation Loss: 0.946700\n",
      "Epoch: 2914 \tTraining Loss: 0.002005 \tValidation Loss: 1.035280\n",
      "Epoch: 2915 \tTraining Loss: 0.003400 \tValidation Loss: 0.932845\n",
      "Epoch: 2916 \tTraining Loss: 0.001985 \tValidation Loss: 1.025381\n",
      "Epoch: 2917 \tTraining Loss: 0.002570 \tValidation Loss: 0.964340\n",
      "Epoch: 2918 \tTraining Loss: 0.004892 \tValidation Loss: 0.935603\n",
      "Epoch: 2919 \tTraining Loss: 0.002647 \tValidation Loss: 0.979412\n",
      "Epoch: 2920 \tTraining Loss: 0.001138 \tValidation Loss: 0.958915\n",
      "Epoch: 2921 \tTraining Loss: 0.002710 \tValidation Loss: 0.910070\n",
      "Epoch: 2922 \tTraining Loss: 0.002637 \tValidation Loss: 0.951929\n",
      "Epoch: 2923 \tTraining Loss: 0.002141 \tValidation Loss: 0.973482\n",
      "Epoch: 2924 \tTraining Loss: 0.002871 \tValidation Loss: 0.886346\n",
      "Epoch: 2925 \tTraining Loss: 0.002124 \tValidation Loss: 0.934261\n",
      "Epoch: 2926 \tTraining Loss: 0.012085 \tValidation Loss: 0.793562\n",
      "Epoch: 2927 \tTraining Loss: 0.004459 \tValidation Loss: 0.946305\n",
      "Epoch: 2928 \tTraining Loss: 0.013295 \tValidation Loss: 0.855663\n",
      "Epoch: 2929 \tTraining Loss: 0.021894 \tValidation Loss: 0.646801\n",
      "Epoch: 2930 \tTraining Loss: 0.006615 \tValidation Loss: 0.615073\n",
      "Epoch: 2931 \tTraining Loss: 0.010195 \tValidation Loss: 0.681399\n",
      "Epoch: 2932 \tTraining Loss: 0.002761 \tValidation Loss: 0.685783\n",
      "Epoch: 2933 \tTraining Loss: 0.011852 \tValidation Loss: 0.617036\n",
      "Epoch: 2934 \tTraining Loss: 0.004834 \tValidation Loss: 0.708824\n",
      "Epoch: 2935 \tTraining Loss: 0.007337 \tValidation Loss: 0.813464\n",
      "Epoch: 2936 \tTraining Loss: 0.032765 \tValidation Loss: 0.606828\n",
      "Epoch: 2937 \tTraining Loss: 0.046255 \tValidation Loss: 0.767855\n",
      "Epoch: 2938 \tTraining Loss: 0.005061 \tValidation Loss: 0.806450\n",
      "Epoch: 2939 \tTraining Loss: 0.011258 \tValidation Loss: 0.926772\n",
      "Epoch: 2940 \tTraining Loss: 0.007638 \tValidation Loss: 1.081272\n",
      "Epoch: 2941 \tTraining Loss: 0.001533 \tValidation Loss: 1.074592\n",
      "Epoch: 2942 \tTraining Loss: 0.013753 \tValidation Loss: 1.095489\n",
      "Epoch: 2943 \tTraining Loss: 0.039136 \tValidation Loss: 0.934436\n",
      "Epoch: 2944 \tTraining Loss: 0.064723 \tValidation Loss: 0.622613\n",
      "Epoch: 2945 \tTraining Loss: 0.010164 \tValidation Loss: 0.580810\n",
      "Epoch: 2946 \tTraining Loss: 0.005900 \tValidation Loss: 0.646697\n",
      "Epoch: 2947 \tTraining Loss: 0.022243 \tValidation Loss: 0.667174\n",
      "Epoch: 2948 \tTraining Loss: 0.003974 \tValidation Loss: 0.638670\n",
      "Epoch: 2949 \tTraining Loss: 0.004814 \tValidation Loss: 0.640118\n",
      "Epoch: 2950 \tTraining Loss: 0.003441 \tValidation Loss: 0.617869\n",
      "Epoch: 2951 \tTraining Loss: 0.003144 \tValidation Loss: 0.681118\n",
      "Epoch: 2952 \tTraining Loss: 0.004223 \tValidation Loss: 0.739404\n",
      "Epoch: 2953 \tTraining Loss: 0.007923 \tValidation Loss: 0.744452\n",
      "Epoch: 2954 \tTraining Loss: 0.001539 \tValidation Loss: 0.788136\n",
      "Epoch: 2955 \tTraining Loss: 0.003600 \tValidation Loss: 0.787982\n",
      "Epoch: 2956 \tTraining Loss: 0.002701 \tValidation Loss: 0.706561\n",
      "Epoch: 2957 \tTraining Loss: 0.001463 \tValidation Loss: 0.685998\n",
      "Epoch: 2958 \tTraining Loss: 0.001509 \tValidation Loss: 0.710911\n",
      "Epoch: 2959 \tTraining Loss: 0.003353 \tValidation Loss: 0.744620\n",
      "Epoch: 2960 \tTraining Loss: 0.002801 \tValidation Loss: 0.777002\n",
      "Epoch: 2961 \tTraining Loss: 0.002720 \tValidation Loss: 0.808164\n",
      "Epoch: 2962 \tTraining Loss: 0.001328 \tValidation Loss: 0.785305\n",
      "Epoch: 2963 \tTraining Loss: 0.001423 \tValidation Loss: 0.809140\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 2964 \tTraining Loss: 0.001664 \tValidation Loss: 0.812282\n",
      "Epoch: 2965 \tTraining Loss: 0.001316 \tValidation Loss: 0.814673\n",
      "Epoch: 2966 \tTraining Loss: 0.001084 \tValidation Loss: 0.826492\n",
      "Epoch: 2967 \tTraining Loss: 0.001646 \tValidation Loss: 0.836238\n",
      "Epoch: 2968 \tTraining Loss: 0.000633 \tValidation Loss: 0.830568\n",
      "Epoch: 2969 \tTraining Loss: 0.002132 \tValidation Loss: 0.833485\n",
      "Epoch: 2970 \tTraining Loss: 0.001401 \tValidation Loss: 0.837909\n",
      "Epoch: 2971 \tTraining Loss: 0.000884 \tValidation Loss: 0.861680\n",
      "Epoch: 2972 \tTraining Loss: 0.001064 \tValidation Loss: 0.876105\n",
      "Epoch: 2973 \tTraining Loss: 0.001550 \tValidation Loss: 0.861089\n",
      "Epoch: 2974 \tTraining Loss: 0.001125 \tValidation Loss: 0.860895\n",
      "Epoch: 2975 \tTraining Loss: 0.001090 \tValidation Loss: 0.868006\n",
      "Epoch: 2976 \tTraining Loss: 0.002044 \tValidation Loss: 0.827288\n",
      "Epoch: 2977 \tTraining Loss: 0.003765 \tValidation Loss: 0.826414\n",
      "Epoch: 2978 \tTraining Loss: 0.001066 \tValidation Loss: 0.806141\n",
      "Epoch: 2979 \tTraining Loss: 0.001903 \tValidation Loss: 0.839319\n",
      "Epoch: 2980 \tTraining Loss: 0.001009 \tValidation Loss: 0.855329\n",
      "Epoch: 2981 \tTraining Loss: 0.001543 \tValidation Loss: 0.838193\n",
      "Epoch: 2982 \tTraining Loss: 0.001392 \tValidation Loss: 0.837699\n",
      "Epoch: 2983 \tTraining Loss: 0.000909 \tValidation Loss: 0.791918\n",
      "Epoch: 2984 \tTraining Loss: 0.002580 \tValidation Loss: 0.765776\n",
      "Epoch: 2985 \tTraining Loss: 0.001826 \tValidation Loss: 0.761587\n",
      "Epoch: 2986 \tTraining Loss: 0.003282 \tValidation Loss: 0.792615\n",
      "Epoch: 2987 \tTraining Loss: 0.001581 \tValidation Loss: 0.708449\n",
      "Epoch: 2988 \tTraining Loss: 0.000813 \tValidation Loss: 0.700886\n",
      "Epoch: 2989 \tTraining Loss: 0.001318 \tValidation Loss: 0.759415\n",
      "Epoch: 2990 \tTraining Loss: 0.002455 \tValidation Loss: 0.800717\n",
      "Epoch: 2991 \tTraining Loss: 0.001208 \tValidation Loss: 0.773985\n",
      "Epoch: 2992 \tTraining Loss: 0.000739 \tValidation Loss: 0.784936\n",
      "Epoch: 2993 \tTraining Loss: 0.000946 \tValidation Loss: 0.765780\n",
      "Epoch: 2994 \tTraining Loss: 0.001053 \tValidation Loss: 0.762020\n",
      "Epoch: 2995 \tTraining Loss: 0.001711 \tValidation Loss: 0.813673\n",
      "Epoch: 2996 \tTraining Loss: 0.001159 \tValidation Loss: 0.809904\n",
      "Epoch: 2997 \tTraining Loss: 0.001103 \tValidation Loss: 0.799088\n",
      "Epoch: 2998 \tTraining Loss: 0.000837 \tValidation Loss: 0.803837\n",
      "Epoch: 2999 \tTraining Loss: 0.000677 \tValidation Loss: 0.811892\n",
      "Epoch: 3000 \tTraining Loss: 0.001478 \tValidation Loss: 0.812273\n",
      "Epoch: 3001 \tTraining Loss: 0.000823 \tValidation Loss: 0.803334\n",
      "Epoch: 3002 \tTraining Loss: 0.000826 \tValidation Loss: 0.774314\n",
      "Epoch: 3003 \tTraining Loss: 0.000892 \tValidation Loss: 0.828678\n",
      "Epoch: 3004 \tTraining Loss: 0.000797 \tValidation Loss: 0.824046\n",
      "Epoch: 3005 \tTraining Loss: 0.000844 \tValidation Loss: 0.835567\n",
      "Epoch: 3006 \tTraining Loss: 0.000789 \tValidation Loss: 0.826360\n",
      "Epoch: 3007 \tTraining Loss: 0.000536 \tValidation Loss: 0.846403\n",
      "Epoch: 3008 \tTraining Loss: 0.000660 \tValidation Loss: 0.835448\n",
      "Epoch: 3009 \tTraining Loss: 0.001025 \tValidation Loss: 0.845561\n",
      "Epoch: 3010 \tTraining Loss: 0.000653 \tValidation Loss: 0.806600\n",
      "Epoch: 3011 \tTraining Loss: 0.000529 \tValidation Loss: 0.856630\n",
      "Epoch: 3012 \tTraining Loss: 0.000838 \tValidation Loss: 0.887503\n",
      "Epoch: 3013 \tTraining Loss: 0.000856 \tValidation Loss: 0.881724\n",
      "Epoch: 3014 \tTraining Loss: 0.000757 \tValidation Loss: 0.894294\n",
      "Epoch: 3015 \tTraining Loss: 0.000658 \tValidation Loss: 0.906562\n",
      "Epoch: 3016 \tTraining Loss: 0.000506 \tValidation Loss: 0.875761\n",
      "Epoch: 3017 \tTraining Loss: 0.000795 \tValidation Loss: 0.869172\n",
      "Epoch: 3018 \tTraining Loss: 0.002785 \tValidation Loss: 0.904146\n",
      "Epoch: 3019 \tTraining Loss: 0.001261 \tValidation Loss: 0.791167\n",
      "Epoch: 3020 \tTraining Loss: 0.002183 \tValidation Loss: 0.769109\n",
      "Epoch: 3021 \tTraining Loss: 0.001496 \tValidation Loss: 0.817351\n",
      "Epoch: 3022 \tTraining Loss: 0.001354 \tValidation Loss: 0.885031\n",
      "Epoch: 3023 \tTraining Loss: 0.001093 \tValidation Loss: 0.847517\n",
      "Epoch: 3024 \tTraining Loss: 0.001322 \tValidation Loss: 0.835492\n",
      "Epoch: 3025 \tTraining Loss: 0.001003 \tValidation Loss: 0.827854\n",
      "Epoch: 3026 \tTraining Loss: 0.000658 \tValidation Loss: 0.833566\n",
      "Epoch: 3027 \tTraining Loss: 0.000852 \tValidation Loss: 0.820897\n",
      "Epoch: 3028 \tTraining Loss: 0.001256 \tValidation Loss: 0.839059\n",
      "Epoch: 3029 \tTraining Loss: 0.000800 \tValidation Loss: 0.870014\n",
      "Epoch: 3030 \tTraining Loss: 0.001148 \tValidation Loss: 0.876025\n",
      "Epoch: 3031 \tTraining Loss: 0.000906 \tValidation Loss: 0.850200\n",
      "Epoch: 3032 \tTraining Loss: 0.000650 \tValidation Loss: 0.843984\n",
      "Epoch: 3033 \tTraining Loss: 0.000843 \tValidation Loss: 0.847938\n",
      "Epoch: 3034 \tTraining Loss: 0.008157 \tValidation Loss: 1.006920\n",
      "Epoch: 3035 \tTraining Loss: 0.003810 \tValidation Loss: 0.905550\n",
      "Epoch: 3036 \tTraining Loss: 0.002687 \tValidation Loss: 0.925635\n",
      "Epoch: 3037 \tTraining Loss: 0.001489 \tValidation Loss: 0.963702\n",
      "Epoch: 3038 \tTraining Loss: 0.002008 \tValidation Loss: 1.049543\n",
      "Epoch: 3039 \tTraining Loss: 0.000748 \tValidation Loss: 1.051963\n",
      "Epoch: 3040 \tTraining Loss: 0.001610 \tValidation Loss: 1.054141\n",
      "Epoch: 3041 \tTraining Loss: 0.001379 \tValidation Loss: 1.061488\n",
      "Epoch: 3042 \tTraining Loss: 0.002162 \tValidation Loss: 0.992981\n",
      "Epoch: 3043 \tTraining Loss: 0.000840 \tValidation Loss: 1.044189\n",
      "Epoch: 3044 \tTraining Loss: 0.001202 \tValidation Loss: 1.085441\n",
      "Epoch: 3045 \tTraining Loss: 0.000923 \tValidation Loss: 1.082628\n",
      "Epoch: 3046 \tTraining Loss: 0.000589 \tValidation Loss: 1.043170\n",
      "Epoch: 3047 \tTraining Loss: 0.000616 \tValidation Loss: 1.010209\n",
      "Epoch: 3048 \tTraining Loss: 0.000543 \tValidation Loss: 1.065928\n",
      "Epoch: 3049 \tTraining Loss: 0.000856 \tValidation Loss: 1.031840\n",
      "Epoch: 3050 \tTraining Loss: 0.000655 \tValidation Loss: 1.002643\n",
      "Epoch: 3051 \tTraining Loss: 0.001490 \tValidation Loss: 1.067850\n",
      "Epoch: 3052 \tTraining Loss: 0.002884 \tValidation Loss: 1.114433\n",
      "Epoch: 3053 \tTraining Loss: 0.091250 \tValidation Loss: 2.668829\n",
      "Epoch: 3054 \tTraining Loss: 0.132501 \tValidation Loss: 2.591490\n",
      "Epoch: 3055 \tTraining Loss: 0.435857 \tValidation Loss: 0.968764\n",
      "Epoch: 3056 \tTraining Loss: 0.226588 \tValidation Loss: 0.797508\n",
      "Epoch: 3057 \tTraining Loss: 0.355902 \tValidation Loss: 1.352431\n",
      "Epoch: 3058 \tTraining Loss: 0.141449 \tValidation Loss: 1.531472\n",
      "Epoch: 3059 \tTraining Loss: 0.067749 \tValidation Loss: 1.218570\n",
      "Epoch: 3060 \tTraining Loss: 0.063791 \tValidation Loss: 1.212368\n",
      "Epoch: 3061 \tTraining Loss: 0.069303 \tValidation Loss: 0.717247\n",
      "Epoch: 3062 \tTraining Loss: 0.038856 \tValidation Loss: 0.829487\n",
      "Epoch: 3063 \tTraining Loss: 0.162354 \tValidation Loss: 0.813400\n",
      "Epoch: 3064 \tTraining Loss: 0.159076 \tValidation Loss: 0.799436\n",
      "Epoch: 3065 \tTraining Loss: 0.026060 \tValidation Loss: 0.697857\n",
      "Epoch: 3066 \tTraining Loss: 0.032432 \tValidation Loss: 0.777934\n",
      "Epoch: 3067 \tTraining Loss: 0.018036 \tValidation Loss: 0.758022\n",
      "Epoch: 3068 \tTraining Loss: 0.010162 \tValidation Loss: 0.814571\n",
      "Epoch: 3069 \tTraining Loss: 0.017672 \tValidation Loss: 0.804145\n",
      "Epoch: 3070 \tTraining Loss: 0.011751 \tValidation Loss: 0.789465\n",
      "Epoch: 3071 \tTraining Loss: 0.029923 \tValidation Loss: 0.788395\n",
      "Epoch: 3072 \tTraining Loss: 0.034043 \tValidation Loss: 0.969704\n",
      "Epoch: 3073 \tTraining Loss: 0.032620 \tValidation Loss: 0.855667\n",
      "Epoch: 3074 \tTraining Loss: 0.055359 \tValidation Loss: 0.804186\n",
      "Epoch: 3075 \tTraining Loss: 0.096938 \tValidation Loss: 0.932719\n",
      "Epoch: 3076 \tTraining Loss: 0.042017 \tValidation Loss: 0.943606\n",
      "Epoch: 3077 \tTraining Loss: 0.011413 \tValidation Loss: 1.057124\n",
      "Epoch: 3078 \tTraining Loss: 0.043927 \tValidation Loss: 0.955877\n",
      "Epoch: 3079 \tTraining Loss: 0.012629 \tValidation Loss: 0.755244\n",
      "Epoch: 3080 \tTraining Loss: 0.013507 \tValidation Loss: 0.763450\n",
      "Epoch: 3081 \tTraining Loss: 0.009945 \tValidation Loss: 0.910373\n",
      "Epoch: 3082 \tTraining Loss: 0.008318 \tValidation Loss: 0.912974\n",
      "Epoch: 3083 \tTraining Loss: 0.004890 \tValidation Loss: 0.936093\n",
      "Epoch: 3084 \tTraining Loss: 0.007909 \tValidation Loss: 0.870900\n",
      "Epoch: 3085 \tTraining Loss: 0.013388 \tValidation Loss: 0.913165\n",
      "Epoch: 3086 \tTraining Loss: 0.004321 \tValidation Loss: 1.014050\n",
      "Epoch: 3087 \tTraining Loss: 0.007464 \tValidation Loss: 1.080232\n",
      "Epoch: 3088 \tTraining Loss: 0.006947 \tValidation Loss: 1.045447\n",
      "Epoch: 3089 \tTraining Loss: 0.007016 \tValidation Loss: 1.033307\n",
      "Epoch: 3090 \tTraining Loss: 0.004459 \tValidation Loss: 1.017695\n",
      "Epoch: 3091 \tTraining Loss: 0.003613 \tValidation Loss: 1.010813\n",
      "Epoch: 3092 \tTraining Loss: 0.003712 \tValidation Loss: 1.052662\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3093 \tTraining Loss: 0.002710 \tValidation Loss: 0.967315\n",
      "Epoch: 3094 \tTraining Loss: 0.004760 \tValidation Loss: 1.010321\n",
      "Epoch: 3095 \tTraining Loss: 0.002051 \tValidation Loss: 0.934806\n",
      "Epoch: 3096 \tTraining Loss: 0.002700 \tValidation Loss: 1.001422\n",
      "Epoch: 3097 \tTraining Loss: 0.002272 \tValidation Loss: 1.005777\n",
      "Epoch: 3098 \tTraining Loss: 0.002890 \tValidation Loss: 1.063371\n",
      "Epoch: 3099 \tTraining Loss: 0.003930 \tValidation Loss: 1.066051\n",
      "Epoch: 3100 \tTraining Loss: 0.004402 \tValidation Loss: 0.820786\n",
      "Epoch: 3101 \tTraining Loss: 0.001488 \tValidation Loss: 0.956252\n",
      "Epoch: 3102 \tTraining Loss: 0.002248 \tValidation Loss: 1.000409\n",
      "Epoch: 3103 \tTraining Loss: 0.001456 \tValidation Loss: 1.012484\n",
      "Epoch: 3104 \tTraining Loss: 0.003913 \tValidation Loss: 1.060241\n",
      "Epoch: 3105 \tTraining Loss: 0.003529 \tValidation Loss: 0.754814\n",
      "Epoch: 3106 \tTraining Loss: 0.004137 \tValidation Loss: 0.779545\n",
      "Epoch: 3107 \tTraining Loss: 0.001624 \tValidation Loss: 0.679111\n",
      "Epoch: 3108 \tTraining Loss: 0.002683 \tValidation Loss: 0.744429\n",
      "Epoch: 3109 \tTraining Loss: 0.001220 \tValidation Loss: 0.748003\n",
      "Epoch: 3110 \tTraining Loss: 0.005985 \tValidation Loss: 0.911311\n",
      "Epoch: 3111 \tTraining Loss: 0.000962 \tValidation Loss: 0.862893\n",
      "Epoch: 3112 \tTraining Loss: 0.001139 \tValidation Loss: 0.860982\n",
      "Epoch: 3113 \tTraining Loss: 0.002617 \tValidation Loss: 0.883266\n",
      "Epoch: 3114 \tTraining Loss: 0.001358 \tValidation Loss: 0.858015\n",
      "Epoch: 3115 \tTraining Loss: 0.002523 \tValidation Loss: 0.841444\n",
      "Epoch: 3116 \tTraining Loss: 0.001555 \tValidation Loss: 0.866062\n",
      "Epoch: 3117 \tTraining Loss: 0.001615 \tValidation Loss: 0.886603\n",
      "Epoch: 3118 \tTraining Loss: 0.001271 \tValidation Loss: 0.935586\n",
      "Epoch: 3119 \tTraining Loss: 0.001182 \tValidation Loss: 0.963730\n",
      "Epoch: 3120 \tTraining Loss: 0.009183 \tValidation Loss: 0.819371\n",
      "Epoch: 3121 \tTraining Loss: 0.004480 \tValidation Loss: 0.852518\n",
      "Epoch: 3122 \tTraining Loss: 0.002275 \tValidation Loss: 0.933722\n",
      "Epoch: 3123 \tTraining Loss: 0.001633 \tValidation Loss: 0.974385\n",
      "Epoch: 3124 \tTraining Loss: 0.003206 \tValidation Loss: 0.812662\n",
      "Epoch: 3125 \tTraining Loss: 0.001360 \tValidation Loss: 0.869667\n",
      "Epoch: 3126 \tTraining Loss: 0.002689 \tValidation Loss: 0.891442\n",
      "Epoch: 3127 \tTraining Loss: 0.001237 \tValidation Loss: 0.851516\n",
      "Epoch: 3128 \tTraining Loss: 0.010938 \tValidation Loss: 1.076844\n",
      "Epoch: 3129 \tTraining Loss: 0.005219 \tValidation Loss: 0.909489\n",
      "Epoch: 3130 \tTraining Loss: 0.002611 \tValidation Loss: 0.844219\n",
      "Epoch: 3131 \tTraining Loss: 0.001713 \tValidation Loss: 0.916230\n",
      "Epoch: 3132 \tTraining Loss: 0.002257 \tValidation Loss: 0.950519\n",
      "Epoch: 3133 \tTraining Loss: 0.002925 \tValidation Loss: 1.034061\n",
      "Epoch: 3134 \tTraining Loss: 0.002108 \tValidation Loss: 1.182530\n",
      "Epoch: 3135 \tTraining Loss: 0.001382 \tValidation Loss: 1.018590\n",
      "Epoch: 3136 \tTraining Loss: 0.001430 \tValidation Loss: 1.019446\n",
      "Epoch: 3137 \tTraining Loss: 0.003174 \tValidation Loss: 1.063218\n",
      "Epoch: 3138 \tTraining Loss: 0.000845 \tValidation Loss: 0.979400\n",
      "Epoch: 3139 \tTraining Loss: 0.002562 \tValidation Loss: 1.048683\n",
      "Epoch: 3140 \tTraining Loss: 0.003510 \tValidation Loss: 0.978046\n",
      "Epoch: 3141 \tTraining Loss: 0.001829 \tValidation Loss: 1.034476\n",
      "Epoch: 3142 \tTraining Loss: 0.001101 \tValidation Loss: 0.999025\n",
      "Epoch: 3143 \tTraining Loss: 0.001305 \tValidation Loss: 0.948245\n",
      "Epoch: 3144 \tTraining Loss: 0.000815 \tValidation Loss: 1.020485\n",
      "Epoch: 3145 \tTraining Loss: 0.001042 \tValidation Loss: 1.025546\n",
      "Epoch: 3146 \tTraining Loss: 0.000863 \tValidation Loss: 1.041119\n",
      "Epoch: 3147 \tTraining Loss: 0.002066 \tValidation Loss: 0.964671\n",
      "Epoch: 3148 \tTraining Loss: 0.000875 \tValidation Loss: 0.998262\n",
      "Epoch: 3149 \tTraining Loss: 0.001258 \tValidation Loss: 0.987582\n",
      "Epoch: 3150 \tTraining Loss: 0.001780 \tValidation Loss: 1.030971\n",
      "Epoch: 3151 \tTraining Loss: 0.005645 \tValidation Loss: 1.030860\n",
      "Epoch: 3152 \tTraining Loss: 0.007955 \tValidation Loss: 1.068445\n",
      "Epoch: 3153 \tTraining Loss: 0.002633 \tValidation Loss: 1.224775\n",
      "Epoch: 3154 \tTraining Loss: 0.001648 \tValidation Loss: 1.268412\n",
      "Epoch: 3155 \tTraining Loss: 0.001629 \tValidation Loss: 1.165005\n",
      "Epoch: 3156 \tTraining Loss: 0.000914 \tValidation Loss: 1.214979\n",
      "Epoch: 3157 \tTraining Loss: 0.002532 \tValidation Loss: 1.121766\n",
      "Epoch: 3158 \tTraining Loss: 0.001953 \tValidation Loss: 1.093496\n",
      "Epoch: 3159 \tTraining Loss: 0.003939 \tValidation Loss: 1.098460\n",
      "Epoch: 3160 \tTraining Loss: 0.007280 \tValidation Loss: 1.036345\n",
      "Epoch: 3161 \tTraining Loss: 0.004391 \tValidation Loss: 1.106725\n",
      "Epoch: 3162 \tTraining Loss: 0.004717 \tValidation Loss: 0.980208\n",
      "Epoch: 3163 \tTraining Loss: 0.001604 \tValidation Loss: 0.975106\n",
      "Epoch: 3164 \tTraining Loss: 0.002239 \tValidation Loss: 1.020879\n",
      "Epoch: 3165 \tTraining Loss: 0.002890 \tValidation Loss: 1.079597\n",
      "Epoch: 3166 \tTraining Loss: 0.001282 \tValidation Loss: 1.087066\n",
      "Epoch: 3167 \tTraining Loss: 0.001816 \tValidation Loss: 1.124701\n",
      "Epoch: 3168 \tTraining Loss: 0.001876 \tValidation Loss: 1.039980\n",
      "Epoch: 3169 \tTraining Loss: 0.005104 \tValidation Loss: 0.873218\n",
      "Epoch: 3170 \tTraining Loss: 0.001863 \tValidation Loss: 0.899661\n",
      "Epoch: 3171 \tTraining Loss: 0.002629 \tValidation Loss: 1.034088\n",
      "Epoch: 3172 \tTraining Loss: 0.001427 \tValidation Loss: 0.986060\n",
      "Epoch: 3173 \tTraining Loss: 0.002872 \tValidation Loss: 1.091962\n",
      "Epoch: 3174 \tTraining Loss: 0.001690 \tValidation Loss: 0.974972\n",
      "Epoch: 3175 \tTraining Loss: 0.000921 \tValidation Loss: 1.029740\n",
      "Epoch: 3176 \tTraining Loss: 0.001688 \tValidation Loss: 1.020199\n",
      "Epoch: 3177 \tTraining Loss: 0.001539 \tValidation Loss: 0.982654\n",
      "Epoch: 3178 \tTraining Loss: 0.000985 \tValidation Loss: 0.973494\n",
      "Epoch: 3179 \tTraining Loss: 0.001042 \tValidation Loss: 1.073201\n",
      "Epoch: 3180 \tTraining Loss: 0.000716 \tValidation Loss: 0.939479\n",
      "Epoch: 3181 \tTraining Loss: 0.001054 \tValidation Loss: 0.990008\n",
      "Epoch: 3182 \tTraining Loss: 0.001112 \tValidation Loss: 0.960566\n",
      "Epoch: 3183 \tTraining Loss: 0.000960 \tValidation Loss: 1.021857\n",
      "Epoch: 3184 \tTraining Loss: 0.001212 \tValidation Loss: 0.945380\n",
      "Epoch: 3185 \tTraining Loss: 0.000637 \tValidation Loss: 0.889498\n",
      "Epoch: 3186 \tTraining Loss: 0.001388 \tValidation Loss: 0.937490\n",
      "Epoch: 3187 \tTraining Loss: 0.000995 \tValidation Loss: 0.998946\n",
      "Epoch: 3188 \tTraining Loss: 0.001925 \tValidation Loss: 1.201167\n",
      "Epoch: 3189 \tTraining Loss: 0.001229 \tValidation Loss: 0.941291\n",
      "Epoch: 3190 \tTraining Loss: 0.000499 \tValidation Loss: 0.867310\n",
      "Epoch: 3191 \tTraining Loss: 0.000648 \tValidation Loss: 0.911811\n",
      "Epoch: 3192 \tTraining Loss: 0.000838 \tValidation Loss: 0.959731\n",
      "Epoch: 3193 \tTraining Loss: 0.000396 \tValidation Loss: 0.879798\n",
      "Epoch: 3194 \tTraining Loss: 0.000870 \tValidation Loss: 0.861858\n",
      "Epoch: 3195 \tTraining Loss: 0.001218 \tValidation Loss: 0.888806\n",
      "Epoch: 3196 \tTraining Loss: 0.002526 \tValidation Loss: 0.951554\n",
      "Epoch: 3197 \tTraining Loss: 0.001340 \tValidation Loss: 0.865679\n",
      "Epoch: 3198 \tTraining Loss: 0.001507 \tValidation Loss: 0.887493\n",
      "Epoch: 3199 \tTraining Loss: 0.001285 \tValidation Loss: 0.881980\n",
      "Epoch: 3200 \tTraining Loss: 0.001157 \tValidation Loss: 0.992553\n",
      "Epoch: 3201 \tTraining Loss: 0.001437 \tValidation Loss: 0.964689\n",
      "Epoch: 3202 \tTraining Loss: 0.003504 \tValidation Loss: 0.792327\n",
      "Epoch: 3203 \tTraining Loss: 0.005375 \tValidation Loss: 1.248490\n",
      "Epoch: 3204 \tTraining Loss: 0.004140 \tValidation Loss: 1.455045\n",
      "Epoch: 3205 \tTraining Loss: 0.080752 \tValidation Loss: 2.395724\n",
      "Epoch: 3206 \tTraining Loss: 0.238157 \tValidation Loss: 2.622169\n",
      "Epoch: 3207 \tTraining Loss: 0.424457 \tValidation Loss: 2.158896\n",
      "Epoch: 3208 \tTraining Loss: 0.213661 \tValidation Loss: 1.436708\n",
      "Epoch: 3209 \tTraining Loss: 0.162671 \tValidation Loss: 1.418053\n",
      "Epoch: 3210 \tTraining Loss: 0.086921 \tValidation Loss: 1.364031\n",
      "Epoch: 3211 \tTraining Loss: 0.162083 \tValidation Loss: 1.765823\n",
      "Epoch: 3212 \tTraining Loss: 0.103949 \tValidation Loss: 1.897441\n",
      "Epoch: 3213 \tTraining Loss: 0.075032 \tValidation Loss: 1.460408\n",
      "Epoch: 3214 \tTraining Loss: 0.190373 \tValidation Loss: 1.565475\n",
      "Epoch: 3215 \tTraining Loss: 0.138462 \tValidation Loss: 1.313226\n",
      "Epoch: 3216 \tTraining Loss: 0.046571 \tValidation Loss: 1.276211\n",
      "Epoch: 3217 \tTraining Loss: 0.058512 \tValidation Loss: 0.991186\n",
      "Epoch: 3218 \tTraining Loss: 0.033700 \tValidation Loss: 1.331446\n",
      "Epoch: 3219 \tTraining Loss: 0.008565 \tValidation Loss: 1.528722\n",
      "Epoch: 3220 \tTraining Loss: 0.031096 \tValidation Loss: 1.643550\n",
      "Epoch: 3221 \tTraining Loss: 0.246356 \tValidation Loss: 1.297325\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3222 \tTraining Loss: 0.114101 \tValidation Loss: 1.083653\n",
      "Epoch: 3223 \tTraining Loss: 0.121595 \tValidation Loss: 0.828977\n",
      "Epoch: 3224 \tTraining Loss: 0.031823 \tValidation Loss: 0.628498\n",
      "Epoch: 3225 \tTraining Loss: 0.017913 \tValidation Loss: 0.667286\n",
      "Epoch: 3226 \tTraining Loss: 0.012605 \tValidation Loss: 0.775086\n",
      "Epoch: 3227 \tTraining Loss: 0.010450 \tValidation Loss: 0.782685\n",
      "Epoch: 3228 \tTraining Loss: 0.007346 \tValidation Loss: 0.827560\n",
      "Epoch: 3229 \tTraining Loss: 0.019940 \tValidation Loss: 0.857554\n",
      "Epoch: 3230 \tTraining Loss: 0.029675 \tValidation Loss: 0.826506\n",
      "Epoch: 3231 \tTraining Loss: 0.012605 \tValidation Loss: 0.872319\n",
      "Epoch: 3232 \tTraining Loss: 0.005703 \tValidation Loss: 0.889817\n",
      "Epoch: 3233 \tTraining Loss: 0.005711 \tValidation Loss: 0.934717\n",
      "Epoch: 3234 \tTraining Loss: 0.007287 \tValidation Loss: 0.905019\n",
      "Epoch: 3235 \tTraining Loss: 0.004924 \tValidation Loss: 0.887286\n",
      "Epoch: 3236 \tTraining Loss: 0.003603 \tValidation Loss: 1.028784\n",
      "Epoch: 3237 \tTraining Loss: 0.003322 \tValidation Loss: 0.944817\n",
      "Epoch: 3238 \tTraining Loss: 0.003157 \tValidation Loss: 0.950865\n",
      "Epoch: 3239 \tTraining Loss: 0.002086 \tValidation Loss: 0.983716\n",
      "Epoch: 3240 \tTraining Loss: 0.004770 \tValidation Loss: 0.981516\n",
      "Epoch: 3241 \tTraining Loss: 0.006589 \tValidation Loss: 0.977630\n",
      "Epoch: 3242 \tTraining Loss: 0.004001 \tValidation Loss: 0.976014\n",
      "Epoch: 3243 \tTraining Loss: 0.033178 \tValidation Loss: 1.309702\n",
      "Epoch: 3244 \tTraining Loss: 0.032456 \tValidation Loss: 0.887672\n",
      "Epoch: 3245 \tTraining Loss: 0.018897 \tValidation Loss: 0.964163\n",
      "Epoch: 3246 \tTraining Loss: 0.014998 \tValidation Loss: 1.220724\n",
      "Epoch: 3247 \tTraining Loss: 0.005722 \tValidation Loss: 1.081686\n",
      "Epoch: 3248 \tTraining Loss: 0.005121 \tValidation Loss: 1.063612\n",
      "Epoch: 3249 \tTraining Loss: 0.004721 \tValidation Loss: 1.071201\n",
      "Epoch: 3250 \tTraining Loss: 0.009931 \tValidation Loss: 1.063220\n",
      "Epoch: 3251 \tTraining Loss: 0.008630 \tValidation Loss: 0.986508\n",
      "Epoch: 3252 \tTraining Loss: 0.004440 \tValidation Loss: 1.066223\n",
      "Epoch: 3253 \tTraining Loss: 0.002934 \tValidation Loss: 1.294910\n",
      "Epoch: 3254 \tTraining Loss: 0.003387 \tValidation Loss: 1.276849\n",
      "Epoch: 3255 \tTraining Loss: 0.007185 \tValidation Loss: 1.238444\n",
      "Epoch: 3256 \tTraining Loss: 0.003543 \tValidation Loss: 1.199009\n",
      "Epoch: 3257 \tTraining Loss: 0.012449 \tValidation Loss: 1.183648\n",
      "Epoch: 3258 \tTraining Loss: 0.009108 \tValidation Loss: 1.283500\n",
      "Epoch: 3259 \tTraining Loss: 0.013819 \tValidation Loss: 1.174062\n",
      "Epoch: 3260 \tTraining Loss: 0.006123 \tValidation Loss: 1.277345\n",
      "Epoch: 3261 \tTraining Loss: 0.002818 \tValidation Loss: 1.219528\n",
      "Epoch: 3262 \tTraining Loss: 0.004244 \tValidation Loss: 1.220393\n",
      "Epoch: 3263 \tTraining Loss: 0.001733 \tValidation Loss: 1.273716\n",
      "Epoch: 3264 \tTraining Loss: 0.004261 \tValidation Loss: 1.289342\n",
      "Epoch: 3265 \tTraining Loss: 0.002503 \tValidation Loss: 1.264297\n",
      "Epoch: 3266 \tTraining Loss: 0.002763 \tValidation Loss: 1.319916\n",
      "Epoch: 3267 \tTraining Loss: 0.004674 \tValidation Loss: 1.431190\n",
      "Epoch: 3268 \tTraining Loss: 0.037745 \tValidation Loss: 1.774140\n",
      "Epoch: 3269 \tTraining Loss: 0.024685 \tValidation Loss: 1.146312\n",
      "Epoch: 3270 \tTraining Loss: 0.007915 \tValidation Loss: 1.187304\n",
      "Epoch: 3271 \tTraining Loss: 0.008056 \tValidation Loss: 1.119032\n",
      "Epoch: 3272 \tTraining Loss: 0.004019 \tValidation Loss: 1.062288\n",
      "Epoch: 3273 \tTraining Loss: 0.004424 \tValidation Loss: 0.972789\n",
      "Epoch: 3274 \tTraining Loss: 0.006212 \tValidation Loss: 1.057922\n",
      "Epoch: 3275 \tTraining Loss: 0.002168 \tValidation Loss: 1.067697\n",
      "Epoch: 3276 \tTraining Loss: 0.003295 \tValidation Loss: 1.061397\n",
      "Epoch: 3277 \tTraining Loss: 0.001828 \tValidation Loss: 1.074239\n",
      "Epoch: 3278 \tTraining Loss: 0.000865 \tValidation Loss: 1.090126\n",
      "Epoch: 3279 \tTraining Loss: 0.003650 \tValidation Loss: 1.004267\n",
      "Epoch: 3280 \tTraining Loss: 0.010796 \tValidation Loss: 1.041083\n",
      "Epoch: 3281 \tTraining Loss: 0.123447 \tValidation Loss: 1.383569\n",
      "Epoch: 3282 \tTraining Loss: 0.144042 \tValidation Loss: 2.847893\n",
      "Epoch: 3283 \tTraining Loss: 0.272603 \tValidation Loss: 1.787410\n",
      "Epoch: 3284 \tTraining Loss: 0.191419 \tValidation Loss: 0.978749\n",
      "Epoch: 3285 \tTraining Loss: 0.200526 \tValidation Loss: 1.110815\n",
      "Epoch: 3286 \tTraining Loss: 0.166298 \tValidation Loss: 1.068153\n",
      "Epoch: 3287 \tTraining Loss: 0.042457 \tValidation Loss: 0.893980\n",
      "Epoch: 3288 \tTraining Loss: 0.041142 \tValidation Loss: 1.054690\n",
      "Epoch: 3289 \tTraining Loss: 0.023047 \tValidation Loss: 1.001919\n",
      "Epoch: 3290 \tTraining Loss: 0.027195 \tValidation Loss: 1.040528\n",
      "Epoch: 3291 \tTraining Loss: 0.010716 \tValidation Loss: 1.035526\n",
      "Epoch: 3292 \tTraining Loss: 0.004851 \tValidation Loss: 1.059941\n",
      "Epoch: 3293 \tTraining Loss: 0.009205 \tValidation Loss: 1.023135\n",
      "Epoch: 3294 \tTraining Loss: 0.005917 \tValidation Loss: 1.092860\n",
      "Epoch: 3295 \tTraining Loss: 0.004839 \tValidation Loss: 0.999796\n",
      "Epoch: 3296 \tTraining Loss: 0.005083 \tValidation Loss: 1.026150\n",
      "Epoch: 3297 \tTraining Loss: 0.006015 \tValidation Loss: 1.024836\n",
      "Epoch: 3298 \tTraining Loss: 0.019423 \tValidation Loss: 0.956323\n",
      "Epoch: 3299 \tTraining Loss: 0.009536 \tValidation Loss: 1.149688\n",
      "Epoch: 3300 \tTraining Loss: 0.007843 \tValidation Loss: 1.267976\n",
      "Epoch: 3301 \tTraining Loss: 0.005823 \tValidation Loss: 1.272499\n",
      "Epoch: 3302 \tTraining Loss: 0.010793 \tValidation Loss: 1.410958\n",
      "Epoch: 3303 \tTraining Loss: 0.015100 \tValidation Loss: 1.160109\n",
      "Epoch: 3304 \tTraining Loss: 0.018987 \tValidation Loss: 1.100597\n",
      "Epoch: 3305 \tTraining Loss: 0.028427 \tValidation Loss: 0.991615\n",
      "Epoch: 3306 \tTraining Loss: 0.003396 \tValidation Loss: 0.840340\n",
      "Epoch: 3307 \tTraining Loss: 0.004146 \tValidation Loss: 0.858832\n",
      "Epoch: 3308 \tTraining Loss: 0.004591 \tValidation Loss: 0.852675\n",
      "Epoch: 3309 \tTraining Loss: 0.003107 \tValidation Loss: 0.884034\n",
      "Epoch: 3310 \tTraining Loss: 0.004879 \tValidation Loss: 0.914902\n",
      "Epoch: 3311 \tTraining Loss: 0.003684 \tValidation Loss: 0.875775\n",
      "Epoch: 3312 \tTraining Loss: 0.002212 \tValidation Loss: 0.914278\n",
      "Epoch: 3313 \tTraining Loss: 0.006124 \tValidation Loss: 0.949810\n",
      "Epoch: 3314 \tTraining Loss: 0.003635 \tValidation Loss: 0.996306\n",
      "Epoch: 3315 \tTraining Loss: 0.003007 \tValidation Loss: 1.003836\n",
      "Epoch: 3316 \tTraining Loss: 0.001607 \tValidation Loss: 1.083711\n",
      "Epoch: 3317 \tTraining Loss: 0.001755 \tValidation Loss: 1.098414\n",
      "Epoch: 3318 \tTraining Loss: 0.001205 \tValidation Loss: 1.065336\n",
      "Epoch: 3319 \tTraining Loss: 0.002019 \tValidation Loss: 1.088721\n",
      "Epoch: 3320 \tTraining Loss: 0.002710 \tValidation Loss: 1.090662\n",
      "Epoch: 3321 \tTraining Loss: 0.002280 \tValidation Loss: 1.086307\n",
      "Epoch: 3322 \tTraining Loss: 0.003471 \tValidation Loss: 1.060346\n",
      "Epoch: 3323 \tTraining Loss: 0.003879 \tValidation Loss: 1.017001\n",
      "Epoch: 3324 \tTraining Loss: 0.001898 \tValidation Loss: 1.045388\n",
      "Epoch: 3325 \tTraining Loss: 0.001708 \tValidation Loss: 1.106198\n",
      "Epoch: 3326 \tTraining Loss: 0.002239 \tValidation Loss: 1.038072\n",
      "Epoch: 3327 \tTraining Loss: 0.002140 \tValidation Loss: 1.076267\n",
      "Epoch: 3328 \tTraining Loss: 0.001602 \tValidation Loss: 1.073734\n",
      "Epoch: 3329 \tTraining Loss: 0.001253 \tValidation Loss: 1.125192\n",
      "Epoch: 3330 \tTraining Loss: 0.001784 \tValidation Loss: 1.160721\n",
      "Epoch: 3331 \tTraining Loss: 0.002717 \tValidation Loss: 1.101278\n",
      "Epoch: 3332 \tTraining Loss: 0.001600 \tValidation Loss: 1.089968\n",
      "Epoch: 3333 \tTraining Loss: 0.001569 \tValidation Loss: 1.108499\n",
      "Epoch: 3334 \tTraining Loss: 0.002840 \tValidation Loss: 1.110584\n",
      "Epoch: 3335 \tTraining Loss: 0.002070 \tValidation Loss: 0.907835\n",
      "Epoch: 3336 \tTraining Loss: 0.001389 \tValidation Loss: 0.950982\n",
      "Epoch: 3337 \tTraining Loss: 0.001184 \tValidation Loss: 0.950471\n",
      "Epoch: 3338 \tTraining Loss: 0.001444 \tValidation Loss: 0.976547\n",
      "Epoch: 3339 \tTraining Loss: 0.001016 \tValidation Loss: 0.969569\n",
      "Epoch: 3340 \tTraining Loss: 0.000989 \tValidation Loss: 0.919581\n",
      "Epoch: 3341 \tTraining Loss: 0.000788 \tValidation Loss: 0.970826\n",
      "Epoch: 3342 \tTraining Loss: 0.000697 \tValidation Loss: 0.969221\n",
      "Epoch: 3343 \tTraining Loss: 0.001580 \tValidation Loss: 0.915524\n",
      "Epoch: 3344 \tTraining Loss: 0.001088 \tValidation Loss: 0.991712\n",
      "Epoch: 3345 \tTraining Loss: 0.001157 \tValidation Loss: 0.959919\n",
      "Epoch: 3346 \tTraining Loss: 0.001457 \tValidation Loss: 0.960653\n",
      "Epoch: 3347 \tTraining Loss: 0.001537 \tValidation Loss: 0.970966\n",
      "Epoch: 3348 \tTraining Loss: 0.000715 \tValidation Loss: 1.009524\n",
      "Epoch: 3349 \tTraining Loss: 0.001596 \tValidation Loss: 1.006612\n",
      "Epoch: 3350 \tTraining Loss: 0.001083 \tValidation Loss: 1.039330\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3351 \tTraining Loss: 0.000677 \tValidation Loss: 1.061003\n",
      "Epoch: 3352 \tTraining Loss: 0.002331 \tValidation Loss: 0.999527\n",
      "Epoch: 3353 \tTraining Loss: 0.000898 \tValidation Loss: 1.042352\n",
      "Epoch: 3354 \tTraining Loss: 0.000826 \tValidation Loss: 1.072161\n",
      "Epoch: 3355 \tTraining Loss: 0.000701 \tValidation Loss: 1.060325\n",
      "Epoch: 3356 \tTraining Loss: 0.000902 \tValidation Loss: 1.064666\n",
      "Epoch: 3357 \tTraining Loss: 0.000751 \tValidation Loss: 1.055292\n",
      "Epoch: 3358 \tTraining Loss: 0.000849 \tValidation Loss: 1.082859\n",
      "Epoch: 3359 \tTraining Loss: 0.002213 \tValidation Loss: 1.117772\n",
      "Epoch: 3360 \tTraining Loss: 0.001738 \tValidation Loss: 1.092683\n",
      "Epoch: 3361 \tTraining Loss: 0.002113 \tValidation Loss: 1.032319\n",
      "Epoch: 3362 \tTraining Loss: 0.002182 \tValidation Loss: 1.083486\n",
      "Epoch: 3363 \tTraining Loss: 0.003884 \tValidation Loss: 1.038315\n",
      "Epoch: 3364 \tTraining Loss: 0.016272 \tValidation Loss: 1.102310\n",
      "Epoch: 3365 \tTraining Loss: 0.043816 \tValidation Loss: 1.106685\n",
      "Epoch: 3366 \tTraining Loss: 0.042609 \tValidation Loss: 0.961787\n",
      "Epoch: 3367 \tTraining Loss: 0.067748 \tValidation Loss: 1.521981\n",
      "Epoch: 3368 \tTraining Loss: 0.175277 \tValidation Loss: 0.742952\n",
      "Epoch: 3369 \tTraining Loss: 0.071946 \tValidation Loss: 0.766855\n",
      "Epoch: 3370 \tTraining Loss: 0.020759 \tValidation Loss: 0.651936\n",
      "Epoch: 3371 \tTraining Loss: 0.013260 \tValidation Loss: 0.724004\n",
      "Epoch: 3372 \tTraining Loss: 0.008381 \tValidation Loss: 0.779242\n",
      "Epoch: 3373 \tTraining Loss: 0.011279 \tValidation Loss: 0.779896\n",
      "Epoch: 3374 \tTraining Loss: 0.008956 \tValidation Loss: 0.783525\n",
      "Epoch: 3375 \tTraining Loss: 0.002725 \tValidation Loss: 0.788793\n",
      "Epoch: 3376 \tTraining Loss: 0.002467 \tValidation Loss: 0.817230\n",
      "Epoch: 3377 \tTraining Loss: 0.002554 \tValidation Loss: 0.856032\n",
      "Epoch: 3378 \tTraining Loss: 0.003052 \tValidation Loss: 0.919532\n",
      "Epoch: 3379 \tTraining Loss: 0.002305 \tValidation Loss: 0.882326\n",
      "Epoch: 3380 \tTraining Loss: 0.001431 \tValidation Loss: 0.907919\n",
      "Epoch: 3381 \tTraining Loss: 0.006964 \tValidation Loss: 0.824705\n",
      "Epoch: 3382 \tTraining Loss: 0.002061 \tValidation Loss: 0.827868\n",
      "Epoch: 3383 \tTraining Loss: 0.002610 \tValidation Loss: 0.882719\n",
      "Epoch: 3384 \tTraining Loss: 0.001331 \tValidation Loss: 0.903773\n",
      "Epoch: 3385 \tTraining Loss: 0.001460 \tValidation Loss: 0.900772\n",
      "Epoch: 3386 \tTraining Loss: 0.001900 \tValidation Loss: 0.867676\n",
      "Epoch: 3387 \tTraining Loss: 0.003760 \tValidation Loss: 0.886381\n",
      "Epoch: 3388 \tTraining Loss: 0.001472 \tValidation Loss: 0.948758\n",
      "Epoch: 3389 \tTraining Loss: 0.001462 \tValidation Loss: 0.946850\n",
      "Epoch: 3390 \tTraining Loss: 0.001255 \tValidation Loss: 0.950634\n",
      "Epoch: 3391 \tTraining Loss: 0.002341 \tValidation Loss: 0.988301\n",
      "Epoch: 3392 \tTraining Loss: 0.001752 \tValidation Loss: 0.948098\n",
      "Epoch: 3393 \tTraining Loss: 0.001810 \tValidation Loss: 0.955491\n",
      "Epoch: 3394 \tTraining Loss: 0.001318 \tValidation Loss: 1.009721\n",
      "Epoch: 3395 \tTraining Loss: 0.001128 \tValidation Loss: 0.993258\n",
      "Epoch: 3396 \tTraining Loss: 0.001880 \tValidation Loss: 0.932364\n",
      "Epoch: 3397 \tTraining Loss: 0.004097 \tValidation Loss: 1.023938\n",
      "Epoch: 3398 \tTraining Loss: 0.033416 \tValidation Loss: 0.963403\n",
      "Epoch: 3399 \tTraining Loss: 0.014221 \tValidation Loss: 0.862118\n",
      "Epoch: 3400 \tTraining Loss: 0.020878 \tValidation Loss: 0.780731\n",
      "Epoch: 3401 \tTraining Loss: 0.006917 \tValidation Loss: 0.746780\n",
      "Epoch: 3402 \tTraining Loss: 0.002998 \tValidation Loss: 0.736502\n",
      "Epoch: 3403 \tTraining Loss: 0.002710 \tValidation Loss: 0.736068\n",
      "Epoch: 3404 \tTraining Loss: 0.002318 \tValidation Loss: 0.731953\n",
      "Epoch: 3405 \tTraining Loss: 0.001425 \tValidation Loss: 0.814865\n",
      "Epoch: 3406 \tTraining Loss: 0.001012 \tValidation Loss: 0.781683\n",
      "Epoch: 3407 \tTraining Loss: 0.001985 \tValidation Loss: 0.799822\n",
      "Epoch: 3408 \tTraining Loss: 0.001184 \tValidation Loss: 0.831003\n",
      "Epoch: 3409 \tTraining Loss: 0.001219 \tValidation Loss: 0.839467\n",
      "Epoch: 3410 \tTraining Loss: 0.040352 \tValidation Loss: 0.872322\n",
      "Epoch: 3411 \tTraining Loss: 0.029198 \tValidation Loss: 1.359632\n",
      "Epoch: 3412 \tTraining Loss: 0.008606 \tValidation Loss: 1.135468\n",
      "Epoch: 3413 \tTraining Loss: 0.006835 \tValidation Loss: 1.167325\n",
      "Epoch: 3414 \tTraining Loss: 0.002470 \tValidation Loss: 1.170217\n",
      "Epoch: 3415 \tTraining Loss: 0.004587 \tValidation Loss: 1.171022\n",
      "Epoch: 3416 \tTraining Loss: 0.002402 \tValidation Loss: 1.652712\n",
      "Epoch: 3417 \tTraining Loss: 0.005223 \tValidation Loss: 1.487020\n",
      "Epoch: 3418 \tTraining Loss: 0.003813 \tValidation Loss: 1.323235\n",
      "Epoch: 3419 \tTraining Loss: 0.004887 \tValidation Loss: 1.234972\n",
      "Epoch: 3420 \tTraining Loss: 0.003222 \tValidation Loss: 1.161012\n",
      "Epoch: 3421 \tTraining Loss: 0.001917 \tValidation Loss: 1.210513\n",
      "Epoch: 3422 \tTraining Loss: 0.004568 \tValidation Loss: 1.087616\n",
      "Epoch: 3423 \tTraining Loss: 0.001105 \tValidation Loss: 1.043208\n",
      "Epoch: 3424 \tTraining Loss: 0.001890 \tValidation Loss: 1.074119\n",
      "Epoch: 3425 \tTraining Loss: 0.001687 \tValidation Loss: 1.109218\n",
      "Epoch: 3426 \tTraining Loss: 0.001197 \tValidation Loss: 1.039889\n",
      "Epoch: 3427 \tTraining Loss: 0.005856 \tValidation Loss: 0.996935\n",
      "Epoch: 3428 \tTraining Loss: 0.001352 \tValidation Loss: 1.007260\n",
      "Epoch: 3429 \tTraining Loss: 0.002838 \tValidation Loss: 1.029350\n",
      "Epoch: 3430 \tTraining Loss: 0.002851 \tValidation Loss: 0.985709\n",
      "Epoch: 3431 \tTraining Loss: 0.001258 \tValidation Loss: 1.006602\n",
      "Epoch: 3432 \tTraining Loss: 0.001560 \tValidation Loss: 0.942090\n",
      "Epoch: 3433 \tTraining Loss: 0.001107 \tValidation Loss: 0.954081\n",
      "Epoch: 3434 \tTraining Loss: 0.001575 \tValidation Loss: 0.906810\n",
      "Epoch: 3435 \tTraining Loss: 0.000621 \tValidation Loss: 0.998347\n",
      "Epoch: 3436 \tTraining Loss: 0.001182 \tValidation Loss: 0.950938\n",
      "Epoch: 3437 \tTraining Loss: 0.001259 \tValidation Loss: 0.956081\n",
      "Epoch: 3438 \tTraining Loss: 0.001208 \tValidation Loss: 0.962630\n",
      "Epoch: 3439 \tTraining Loss: 0.000627 \tValidation Loss: 0.954250\n",
      "Epoch: 3440 \tTraining Loss: 0.000724 \tValidation Loss: 1.078830\n",
      "Epoch: 3441 \tTraining Loss: 0.000824 \tValidation Loss: 0.989090\n",
      "Epoch: 3442 \tTraining Loss: 0.000746 \tValidation Loss: 0.970468\n",
      "Epoch: 3443 \tTraining Loss: 0.000800 \tValidation Loss: 0.982074\n",
      "Epoch: 3444 \tTraining Loss: 0.000521 \tValidation Loss: 0.964562\n",
      "Epoch: 3445 \tTraining Loss: 0.000464 \tValidation Loss: 0.965372\n",
      "Epoch: 3446 \tTraining Loss: 0.000573 \tValidation Loss: 1.002087\n",
      "Epoch: 3447 \tTraining Loss: 0.001050 \tValidation Loss: 0.973568\n",
      "Epoch: 3448 \tTraining Loss: 0.001525 \tValidation Loss: 0.982572\n",
      "Epoch: 3449 \tTraining Loss: 0.002316 \tValidation Loss: 0.925997\n",
      "Epoch: 3450 \tTraining Loss: 0.000840 \tValidation Loss: 0.912423\n",
      "Epoch: 3451 \tTraining Loss: 0.000615 \tValidation Loss: 0.921936\n",
      "Epoch: 3452 \tTraining Loss: 0.000921 \tValidation Loss: 0.963163\n",
      "Epoch: 3453 \tTraining Loss: 0.001074 \tValidation Loss: 0.954648\n",
      "Epoch: 3454 \tTraining Loss: 0.000865 \tValidation Loss: 0.951511\n",
      "Epoch: 3455 \tTraining Loss: 0.000659 \tValidation Loss: 0.939441\n",
      "Epoch: 3456 \tTraining Loss: 0.001256 \tValidation Loss: 0.930884\n",
      "Epoch: 3457 \tTraining Loss: 0.000728 \tValidation Loss: 0.928448\n",
      "Epoch: 3458 \tTraining Loss: 0.001041 \tValidation Loss: 0.952193\n",
      "Epoch: 3459 \tTraining Loss: 0.000938 \tValidation Loss: 0.976384\n",
      "Epoch: 3460 \tTraining Loss: 0.001326 \tValidation Loss: 0.978001\n",
      "Epoch: 3461 \tTraining Loss: 0.001416 \tValidation Loss: 0.979387\n",
      "Epoch: 3462 \tTraining Loss: 0.001238 \tValidation Loss: 0.948191\n",
      "Epoch: 3463 \tTraining Loss: 0.000939 \tValidation Loss: 1.131045\n",
      "Epoch: 3464 \tTraining Loss: 0.001557 \tValidation Loss: 0.938931\n",
      "Epoch: 3465 \tTraining Loss: 0.000682 \tValidation Loss: 1.006379\n",
      "Epoch: 3466 \tTraining Loss: 0.001034 \tValidation Loss: 0.999765\n",
      "Epoch: 3467 \tTraining Loss: 0.000424 \tValidation Loss: 1.042001\n",
      "Epoch: 3468 \tTraining Loss: 0.000799 \tValidation Loss: 1.057643\n",
      "Epoch: 3469 \tTraining Loss: 0.000757 \tValidation Loss: 1.079817\n",
      "Epoch: 3470 \tTraining Loss: 0.000808 \tValidation Loss: 1.076017\n",
      "Epoch: 3471 \tTraining Loss: 0.001232 \tValidation Loss: 1.056194\n",
      "Epoch: 3472 \tTraining Loss: 0.000770 \tValidation Loss: 1.017531\n",
      "Epoch: 3473 \tTraining Loss: 0.000508 \tValidation Loss: 0.984169\n",
      "Epoch: 3474 \tTraining Loss: 0.000971 \tValidation Loss: 1.024746\n",
      "Epoch: 3475 \tTraining Loss: 0.001092 \tValidation Loss: 1.004272\n",
      "Epoch: 3476 \tTraining Loss: 0.001353 \tValidation Loss: 0.971201\n",
      "Epoch: 3477 \tTraining Loss: 0.000899 \tValidation Loss: 0.938966\n",
      "Epoch: 3478 \tTraining Loss: 0.000643 \tValidation Loss: 0.901908\n",
      "Epoch: 3479 \tTraining Loss: 0.000629 \tValidation Loss: 0.940469\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3480 \tTraining Loss: 0.000544 \tValidation Loss: 0.941769\n",
      "Epoch: 3481 \tTraining Loss: 0.000613 \tValidation Loss: 0.916068\n",
      "Epoch: 3482 \tTraining Loss: 0.000784 \tValidation Loss: 0.902094\n",
      "Epoch: 3483 \tTraining Loss: 0.001354 \tValidation Loss: 0.907528\n",
      "Epoch: 3484 \tTraining Loss: 0.001071 \tValidation Loss: 0.925823\n",
      "Epoch: 3485 \tTraining Loss: 0.004214 \tValidation Loss: 1.146645\n",
      "Epoch: 3486 \tTraining Loss: 0.001417 \tValidation Loss: 0.902833\n",
      "Epoch: 3487 \tTraining Loss: 0.000851 \tValidation Loss: 0.921114\n",
      "Epoch: 3488 \tTraining Loss: 0.001155 \tValidation Loss: 0.886157\n",
      "Epoch: 3489 \tTraining Loss: 0.000886 \tValidation Loss: 0.861657\n",
      "Epoch: 3490 \tTraining Loss: 0.001019 \tValidation Loss: 0.866582\n",
      "Epoch: 3491 \tTraining Loss: 0.000512 \tValidation Loss: 0.851570\n",
      "Epoch: 3492 \tTraining Loss: 0.000566 \tValidation Loss: 0.843086\n",
      "Epoch: 3493 \tTraining Loss: 0.000896 \tValidation Loss: 0.812935\n",
      "Epoch: 3494 \tTraining Loss: 0.001685 \tValidation Loss: 0.886754\n",
      "Epoch: 3495 \tTraining Loss: 0.000442 \tValidation Loss: 0.849774\n",
      "Epoch: 3496 \tTraining Loss: 0.001249 \tValidation Loss: 0.811695\n",
      "Epoch: 3497 \tTraining Loss: 0.001375 \tValidation Loss: 0.751844\n",
      "Epoch: 3498 \tTraining Loss: 0.059288 \tValidation Loss: 0.717247\n",
      "Epoch: 3499 \tTraining Loss: 0.424505 \tValidation Loss: 2.894831\n",
      "Epoch: 3500 \tTraining Loss: 0.259013 \tValidation Loss: 1.953499\n",
      "Epoch: 3501 \tTraining Loss: 0.294815 \tValidation Loss: 0.886435\n",
      "Epoch: 3502 \tTraining Loss: 0.281680 \tValidation Loss: 1.043705\n",
      "Epoch: 3503 \tTraining Loss: 0.149440 \tValidation Loss: 1.252465\n",
      "Epoch: 3504 \tTraining Loss: 0.283216 \tValidation Loss: 1.039664\n",
      "Epoch: 3505 \tTraining Loss: 0.072892 \tValidation Loss: 1.049288\n",
      "Epoch: 3506 \tTraining Loss: 0.070492 \tValidation Loss: 1.055424\n",
      "Epoch: 3507 \tTraining Loss: 0.094528 \tValidation Loss: 1.438060\n",
      "Epoch: 3508 \tTraining Loss: 0.140135 \tValidation Loss: 1.178330\n",
      "Epoch: 3509 \tTraining Loss: 0.126591 \tValidation Loss: 1.380410\n",
      "Epoch: 3510 \tTraining Loss: 0.183483 \tValidation Loss: 1.601534\n",
      "Epoch: 3511 \tTraining Loss: 0.125605 \tValidation Loss: 1.634671\n",
      "Epoch: 3512 \tTraining Loss: 0.040377 \tValidation Loss: 1.445933\n",
      "Epoch: 3513 \tTraining Loss: 0.017840 \tValidation Loss: 1.511089\n",
      "Epoch: 3514 \tTraining Loss: 0.011435 \tValidation Loss: 1.679605\n",
      "Epoch: 3515 \tTraining Loss: 0.019094 \tValidation Loss: 1.452013\n",
      "Epoch: 3516 \tTraining Loss: 0.030026 \tValidation Loss: 1.331674\n",
      "Epoch: 3517 \tTraining Loss: 0.052655 \tValidation Loss: 1.507493\n",
      "Epoch: 3518 \tTraining Loss: 0.174744 \tValidation Loss: 1.538677\n",
      "Epoch: 3519 \tTraining Loss: 0.113922 \tValidation Loss: 1.240863\n",
      "Epoch: 3520 \tTraining Loss: 0.060310 \tValidation Loss: 1.367249\n",
      "Epoch: 3521 \tTraining Loss: 0.037566 \tValidation Loss: 1.392776\n",
      "Epoch: 3522 \tTraining Loss: 0.027484 \tValidation Loss: 1.460579\n",
      "Epoch: 3523 \tTraining Loss: 0.021022 \tValidation Loss: 1.470012\n",
      "Epoch: 3524 \tTraining Loss: 0.013130 \tValidation Loss: 1.439338\n",
      "Epoch: 3525 \tTraining Loss: 0.013704 \tValidation Loss: 1.452813\n",
      "Epoch: 3526 \tTraining Loss: 0.008150 \tValidation Loss: 1.370105\n",
      "Epoch: 3527 \tTraining Loss: 0.011799 \tValidation Loss: 1.307554\n",
      "Epoch: 3528 \tTraining Loss: 0.008841 \tValidation Loss: 1.448139\n",
      "Epoch: 3529 \tTraining Loss: 0.005816 \tValidation Loss: 1.431403\n",
      "Epoch: 3530 \tTraining Loss: 0.008676 \tValidation Loss: 1.453103\n",
      "Epoch: 3531 \tTraining Loss: 0.004253 \tValidation Loss: 1.472993\n",
      "Epoch: 3532 \tTraining Loss: 0.020345 \tValidation Loss: 1.446551\n",
      "Epoch: 3533 \tTraining Loss: 0.006729 \tValidation Loss: 1.266742\n",
      "Epoch: 3534 \tTraining Loss: 0.004546 \tValidation Loss: 1.374450\n",
      "Epoch: 3535 \tTraining Loss: 0.008339 \tValidation Loss: 1.428495\n",
      "Epoch: 3536 \tTraining Loss: 0.012162 \tValidation Loss: 1.344805\n",
      "Epoch: 3537 \tTraining Loss: 0.022154 \tValidation Loss: 1.817473\n",
      "Epoch: 3538 \tTraining Loss: 0.012027 \tValidation Loss: 1.424453\n",
      "Epoch: 3539 \tTraining Loss: 0.075343 \tValidation Loss: 1.175994\n",
      "Epoch: 3540 \tTraining Loss: 0.188178 \tValidation Loss: 0.654054\n",
      "Epoch: 3541 \tTraining Loss: 0.056918 \tValidation Loss: 0.692018\n",
      "Epoch: 3542 \tTraining Loss: 0.080375 \tValidation Loss: 0.697745\n",
      "Epoch: 3543 \tTraining Loss: 0.058313 \tValidation Loss: 0.884320\n",
      "Epoch: 3544 \tTraining Loss: 0.028372 \tValidation Loss: 0.741918\n",
      "Epoch: 3545 \tTraining Loss: 0.011917 \tValidation Loss: 0.744294\n",
      "Epoch: 3546 \tTraining Loss: 0.009745 \tValidation Loss: 0.662678\n",
      "Epoch: 3547 \tTraining Loss: 0.007371 \tValidation Loss: 0.680227\n",
      "Epoch: 3548 \tTraining Loss: 0.004893 \tValidation Loss: 0.697278\n",
      "Epoch: 3549 \tTraining Loss: 0.035161 \tValidation Loss: 1.209799\n",
      "Epoch: 3550 \tTraining Loss: 0.041564 \tValidation Loss: 0.929052\n",
      "Epoch: 3551 \tTraining Loss: 0.011688 \tValidation Loss: 1.196559\n",
      "Epoch: 3552 \tTraining Loss: 0.022332 \tValidation Loss: 1.248755\n",
      "Epoch: 3553 \tTraining Loss: 0.037746 \tValidation Loss: 1.404003\n",
      "Epoch: 3554 \tTraining Loss: 0.061238 \tValidation Loss: 1.026628\n",
      "Epoch: 3555 \tTraining Loss: 0.117734 \tValidation Loss: 0.915336\n",
      "Epoch: 3556 \tTraining Loss: 0.047481 \tValidation Loss: 0.856378\n",
      "Epoch: 3557 \tTraining Loss: 0.018888 \tValidation Loss: 1.102396\n",
      "Epoch: 3558 \tTraining Loss: 0.060140 \tValidation Loss: 1.086720\n",
      "Epoch: 3559 \tTraining Loss: 0.009088 \tValidation Loss: 1.178750\n",
      "Epoch: 3560 \tTraining Loss: 0.009392 \tValidation Loss: 1.095604\n",
      "Epoch: 3561 \tTraining Loss: 0.013364 \tValidation Loss: 1.235187\n",
      "Epoch: 3562 \tTraining Loss: 0.008459 \tValidation Loss: 1.265663\n",
      "Epoch: 3563 \tTraining Loss: 0.010522 \tValidation Loss: 1.236860\n",
      "Epoch: 3564 \tTraining Loss: 0.006456 \tValidation Loss: 1.279935\n",
      "Epoch: 3565 \tTraining Loss: 0.005838 \tValidation Loss: 1.300887\n",
      "Epoch: 3566 \tTraining Loss: 0.004591 \tValidation Loss: 1.371219\n",
      "Epoch: 3567 \tTraining Loss: 0.003966 \tValidation Loss: 1.317695\n",
      "Epoch: 3568 \tTraining Loss: 0.005923 \tValidation Loss: 1.441643\n",
      "Epoch: 3569 \tTraining Loss: 0.004358 \tValidation Loss: 1.242938\n",
      "Epoch: 3570 \tTraining Loss: 0.004800 \tValidation Loss: 1.210943\n",
      "Epoch: 3571 \tTraining Loss: 0.001780 \tValidation Loss: 1.262213\n",
      "Epoch: 3572 \tTraining Loss: 0.002236 \tValidation Loss: 1.277722\n",
      "Epoch: 3573 \tTraining Loss: 0.003607 \tValidation Loss: 1.175982\n",
      "Epoch: 3574 \tTraining Loss: 0.002902 \tValidation Loss: 1.225689\n",
      "Epoch: 3575 \tTraining Loss: 0.027298 \tValidation Loss: 1.526469\n",
      "Epoch: 3576 \tTraining Loss: 0.008984 \tValidation Loss: 1.457000\n",
      "Epoch: 3577 \tTraining Loss: 0.010945 \tValidation Loss: 1.211303\n",
      "Epoch: 3578 \tTraining Loss: 0.004626 \tValidation Loss: 1.210476\n",
      "Epoch: 3579 \tTraining Loss: 0.005977 \tValidation Loss: 1.175812\n",
      "Epoch: 3580 \tTraining Loss: 0.002443 \tValidation Loss: 1.237554\n",
      "Epoch: 3581 \tTraining Loss: 0.003644 \tValidation Loss: 1.274512\n",
      "Epoch: 3582 \tTraining Loss: 0.002995 \tValidation Loss: 1.301830\n",
      "Epoch: 3583 \tTraining Loss: 0.003365 \tValidation Loss: 1.320751\n",
      "Epoch: 3584 \tTraining Loss: 0.001506 \tValidation Loss: 1.230397\n",
      "Epoch: 3585 \tTraining Loss: 0.001971 \tValidation Loss: 1.328888\n",
      "Epoch: 3586 \tTraining Loss: 0.002454 \tValidation Loss: 1.323910\n",
      "Epoch: 3587 \tTraining Loss: 0.001040 \tValidation Loss: 1.371119\n",
      "Epoch: 3588 \tTraining Loss: 0.002617 \tValidation Loss: 1.370304\n",
      "Epoch: 3589 \tTraining Loss: 0.001983 \tValidation Loss: 1.422406\n",
      "Epoch: 3590 \tTraining Loss: 0.004650 \tValidation Loss: 1.421653\n",
      "Epoch: 3591 \tTraining Loss: 0.013525 \tValidation Loss: 1.331212\n",
      "Epoch: 3592 \tTraining Loss: 0.003814 \tValidation Loss: 1.483152\n",
      "Epoch: 3593 \tTraining Loss: 0.008462 \tValidation Loss: 1.405570\n",
      "Epoch: 3594 \tTraining Loss: 0.003558 \tValidation Loss: 1.312370\n",
      "Epoch: 3595 \tTraining Loss: 0.001652 \tValidation Loss: 1.435104\n",
      "Epoch: 3596 \tTraining Loss: 0.006373 \tValidation Loss: 1.433523\n",
      "Epoch: 3597 \tTraining Loss: 0.002927 \tValidation Loss: 1.359392\n",
      "Epoch: 3598 \tTraining Loss: 0.002463 \tValidation Loss: 1.364758\n",
      "Epoch: 3599 \tTraining Loss: 0.001509 \tValidation Loss: 1.414660\n",
      "Epoch: 3600 \tTraining Loss: 0.008871 \tValidation Loss: 1.299595\n",
      "Epoch: 3601 \tTraining Loss: 0.003775 \tValidation Loss: 1.388528\n",
      "Epoch: 3602 \tTraining Loss: 0.002153 \tValidation Loss: 1.279967\n",
      "Epoch: 3603 \tTraining Loss: 0.001305 \tValidation Loss: 1.274269\n",
      "Epoch: 3604 \tTraining Loss: 0.002832 \tValidation Loss: 1.362077\n",
      "Epoch: 3605 \tTraining Loss: 0.003173 \tValidation Loss: 1.353951\n",
      "Epoch: 3606 \tTraining Loss: 0.004055 \tValidation Loss: 1.370122\n",
      "Epoch: 3607 \tTraining Loss: 0.001181 \tValidation Loss: 1.277148\n",
      "Epoch: 3608 \tTraining Loss: 0.001589 \tValidation Loss: 1.334492\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3609 \tTraining Loss: 0.002176 \tValidation Loss: 1.219471\n",
      "Epoch: 3610 \tTraining Loss: 0.001768 \tValidation Loss: 1.267856\n",
      "Epoch: 3611 \tTraining Loss: 0.001268 \tValidation Loss: 1.292267\n",
      "Epoch: 3612 \tTraining Loss: 0.001744 \tValidation Loss: 1.420132\n",
      "Epoch: 3613 \tTraining Loss: 0.001563 \tValidation Loss: 1.330866\n",
      "Epoch: 3614 \tTraining Loss: 0.001855 \tValidation Loss: 1.328461\n",
      "Epoch: 3615 \tTraining Loss: 0.001182 \tValidation Loss: 1.389536\n",
      "Epoch: 3616 \tTraining Loss: 0.002795 \tValidation Loss: 1.380247\n",
      "Epoch: 3617 \tTraining Loss: 0.000676 \tValidation Loss: 1.402626\n",
      "Epoch: 3618 \tTraining Loss: 0.001935 \tValidation Loss: 1.365578\n",
      "Epoch: 3619 \tTraining Loss: 0.001177 \tValidation Loss: 1.299250\n",
      "Epoch: 3620 \tTraining Loss: 0.002848 \tValidation Loss: 1.273395\n",
      "Epoch: 3621 \tTraining Loss: 0.000800 \tValidation Loss: 1.367372\n",
      "Epoch: 3622 \tTraining Loss: 0.001669 \tValidation Loss: 1.299631\n",
      "Epoch: 3623 \tTraining Loss: 0.001407 \tValidation Loss: 1.213717\n",
      "Epoch: 3624 \tTraining Loss: 0.001790 \tValidation Loss: 1.274831\n",
      "Epoch: 3625 \tTraining Loss: 0.001260 \tValidation Loss: 1.421109\n",
      "Epoch: 3626 \tTraining Loss: 0.001137 \tValidation Loss: 1.351349\n",
      "Epoch: 3627 \tTraining Loss: 0.000707 \tValidation Loss: 1.322335\n",
      "Epoch: 3628 \tTraining Loss: 0.001848 \tValidation Loss: 1.369756\n",
      "Epoch: 3629 \tTraining Loss: 0.003314 \tValidation Loss: 1.227702\n",
      "Epoch: 3630 \tTraining Loss: 0.001329 \tValidation Loss: 1.226757\n",
      "Epoch: 3631 \tTraining Loss: 0.000725 \tValidation Loss: 1.181724\n",
      "Epoch: 3632 \tTraining Loss: 0.001209 \tValidation Loss: 1.234795\n",
      "Epoch: 3633 \tTraining Loss: 0.001179 \tValidation Loss: 1.155732\n",
      "Epoch: 3634 \tTraining Loss: 0.000928 \tValidation Loss: 1.343199\n",
      "Epoch: 3635 \tTraining Loss: 0.003120 \tValidation Loss: 1.288053\n",
      "Epoch: 3636 \tTraining Loss: 0.001071 \tValidation Loss: 1.291543\n",
      "Epoch: 3637 \tTraining Loss: 0.000752 \tValidation Loss: 1.325053\n",
      "Epoch: 3638 \tTraining Loss: 0.000808 \tValidation Loss: 1.326317\n",
      "Epoch: 3639 \tTraining Loss: 0.002332 \tValidation Loss: 1.343736\n",
      "Epoch: 3640 \tTraining Loss: 0.001546 \tValidation Loss: 1.227637\n",
      "Epoch: 3641 \tTraining Loss: 0.000988 \tValidation Loss: 1.350960\n",
      "Epoch: 3642 \tTraining Loss: 0.000993 \tValidation Loss: 1.324568\n",
      "Epoch: 3643 \tTraining Loss: 0.001168 \tValidation Loss: 1.283778\n",
      "Epoch: 3644 \tTraining Loss: 0.000656 \tValidation Loss: 1.376926\n",
      "Epoch: 3645 \tTraining Loss: 0.000886 \tValidation Loss: 1.299269\n",
      "Epoch: 3646 \tTraining Loss: 0.003284 \tValidation Loss: 1.286620\n",
      "Epoch: 3647 \tTraining Loss: 0.001497 \tValidation Loss: 1.227306\n",
      "Epoch: 3648 \tTraining Loss: 0.001247 \tValidation Loss: 1.127866\n",
      "Epoch: 3649 \tTraining Loss: 0.000894 \tValidation Loss: 1.264406\n",
      "Epoch: 3650 \tTraining Loss: 0.000732 \tValidation Loss: 1.276151\n",
      "Epoch: 3651 \tTraining Loss: 0.001113 \tValidation Loss: 1.263598\n",
      "Epoch: 3652 \tTraining Loss: 0.000558 \tValidation Loss: 1.291783\n",
      "Epoch: 3653 \tTraining Loss: 0.000597 \tValidation Loss: 1.348490\n",
      "Epoch: 3654 \tTraining Loss: 0.002276 \tValidation Loss: 1.156694\n",
      "Epoch: 3655 \tTraining Loss: 0.001107 \tValidation Loss: 1.080323\n",
      "Epoch: 3656 \tTraining Loss: 0.004107 \tValidation Loss: 1.196395\n",
      "Epoch: 3657 \tTraining Loss: 0.009183 \tValidation Loss: 1.420947\n",
      "Epoch: 3658 \tTraining Loss: 0.001825 \tValidation Loss: 1.473033\n",
      "Epoch: 3659 \tTraining Loss: 0.001482 \tValidation Loss: 1.375719\n",
      "Epoch: 3660 \tTraining Loss: 0.002404 \tValidation Loss: 1.422584\n",
      "Epoch: 3661 \tTraining Loss: 0.003533 \tValidation Loss: 1.424856\n",
      "Epoch: 3662 \tTraining Loss: 0.001781 \tValidation Loss: 1.268400\n",
      "Epoch: 3663 \tTraining Loss: 0.002213 \tValidation Loss: 1.342402\n",
      "Epoch: 3664 \tTraining Loss: 0.001233 \tValidation Loss: 1.388768\n",
      "Epoch: 3665 \tTraining Loss: 0.003705 \tValidation Loss: 1.291824\n",
      "Epoch: 3666 \tTraining Loss: 0.001268 \tValidation Loss: 1.290749\n",
      "Epoch: 3667 \tTraining Loss: 0.001437 \tValidation Loss: 1.280002\n",
      "Epoch: 3668 \tTraining Loss: 0.001292 \tValidation Loss: 1.328869\n",
      "Epoch: 3669 \tTraining Loss: 0.000807 \tValidation Loss: 1.334548\n",
      "Epoch: 3670 \tTraining Loss: 0.001440 \tValidation Loss: 1.353835\n",
      "Epoch: 3671 \tTraining Loss: 0.002469 \tValidation Loss: 1.347394\n",
      "Epoch: 3672 \tTraining Loss: 0.001882 \tValidation Loss: 1.401059\n",
      "Epoch: 3673 \tTraining Loss: 0.012521 \tValidation Loss: 1.655303\n",
      "Epoch: 3674 \tTraining Loss: 0.029923 \tValidation Loss: 3.214820\n",
      "Epoch: 3675 \tTraining Loss: 0.446694 \tValidation Loss: 0.884262\n",
      "Epoch: 3676 \tTraining Loss: 0.100463 \tValidation Loss: 0.728703\n",
      "Epoch: 3677 \tTraining Loss: 0.106625 \tValidation Loss: 0.785654\n",
      "Epoch: 3678 \tTraining Loss: 0.171820 \tValidation Loss: 0.690668\n",
      "Epoch: 3679 \tTraining Loss: 0.046325 \tValidation Loss: 0.753052\n",
      "Epoch: 3680 \tTraining Loss: 0.024630 \tValidation Loss: 0.609015\n",
      "Epoch: 3681 \tTraining Loss: 0.053187 \tValidation Loss: 0.905683\n",
      "Epoch: 3682 \tTraining Loss: 0.093570 \tValidation Loss: 0.719351\n",
      "Epoch: 3683 \tTraining Loss: 0.017347 \tValidation Loss: 0.750563\n",
      "Epoch: 3684 \tTraining Loss: 0.045739 \tValidation Loss: 0.886173\n",
      "Epoch: 3685 \tTraining Loss: 0.191854 \tValidation Loss: 0.655026\n",
      "Epoch: 3686 \tTraining Loss: 0.050371 \tValidation Loss: 0.923702\n",
      "Epoch: 3687 \tTraining Loss: 0.053191 \tValidation Loss: 0.719403\n",
      "Epoch: 3688 \tTraining Loss: 0.014027 \tValidation Loss: 0.861804\n",
      "Epoch: 3689 \tTraining Loss: 0.010644 \tValidation Loss: 0.784444\n",
      "Epoch: 3690 \tTraining Loss: 0.006725 \tValidation Loss: 0.744349\n",
      "Epoch: 3691 \tTraining Loss: 0.006158 \tValidation Loss: 0.744127\n",
      "Epoch: 3692 \tTraining Loss: 0.004040 \tValidation Loss: 0.764715\n",
      "Epoch: 3693 \tTraining Loss: 0.006098 \tValidation Loss: 0.768899\n",
      "Epoch: 3694 \tTraining Loss: 0.004420 \tValidation Loss: 0.797535\n",
      "Epoch: 3695 \tTraining Loss: 0.006019 \tValidation Loss: 0.769173\n",
      "Epoch: 3696 \tTraining Loss: 0.003697 \tValidation Loss: 0.805216\n",
      "Epoch: 3697 \tTraining Loss: 0.003765 \tValidation Loss: 0.796560\n",
      "Epoch: 3698 \tTraining Loss: 0.004175 \tValidation Loss: 0.778601\n",
      "Epoch: 3699 \tTraining Loss: 0.008414 \tValidation Loss: 0.827094\n",
      "Epoch: 3700 \tTraining Loss: 0.003903 \tValidation Loss: 0.790299\n",
      "Epoch: 3701 \tTraining Loss: 0.003784 \tValidation Loss: 0.789653\n",
      "Epoch: 3702 \tTraining Loss: 0.004271 \tValidation Loss: 0.804219\n",
      "Epoch: 3703 \tTraining Loss: 0.002764 \tValidation Loss: 0.825048\n",
      "Epoch: 3704 \tTraining Loss: 0.002187 \tValidation Loss: 0.841940\n",
      "Epoch: 3705 \tTraining Loss: 0.001822 \tValidation Loss: 0.861174\n",
      "Epoch: 3706 \tTraining Loss: 0.001526 \tValidation Loss: 0.887463\n",
      "Epoch: 3707 \tTraining Loss: 0.001933 \tValidation Loss: 0.938563\n",
      "Epoch: 3708 \tTraining Loss: 0.004171 \tValidation Loss: 0.889068\n",
      "Epoch: 3709 \tTraining Loss: 0.002520 \tValidation Loss: 0.919896\n",
      "Epoch: 3710 \tTraining Loss: 0.001867 \tValidation Loss: 0.901765\n",
      "Epoch: 3711 \tTraining Loss: 0.001317 \tValidation Loss: 0.919856\n",
      "Epoch: 3712 \tTraining Loss: 0.004051 \tValidation Loss: 1.037480\n",
      "Epoch: 3713 \tTraining Loss: 0.004454 \tValidation Loss: 0.902034\n",
      "Epoch: 3714 \tTraining Loss: 0.003894 \tValidation Loss: 0.932254\n",
      "Epoch: 3715 \tTraining Loss: 0.003582 \tValidation Loss: 0.977610\n",
      "Epoch: 3716 \tTraining Loss: 0.001294 \tValidation Loss: 0.979739\n",
      "Epoch: 3717 \tTraining Loss: 0.002356 \tValidation Loss: 0.947073\n",
      "Epoch: 3718 \tTraining Loss: 0.003012 \tValidation Loss: 0.951443\n",
      "Epoch: 3719 \tTraining Loss: 0.001939 \tValidation Loss: 0.927243\n",
      "Epoch: 3720 \tTraining Loss: 0.001972 \tValidation Loss: 0.933307\n",
      "Epoch: 3721 \tTraining Loss: 0.001832 \tValidation Loss: 0.948617\n",
      "Epoch: 3722 \tTraining Loss: 0.001039 \tValidation Loss: 1.013099\n",
      "Epoch: 3723 \tTraining Loss: 0.001134 \tValidation Loss: 0.964103\n",
      "Epoch: 3724 \tTraining Loss: 0.003539 \tValidation Loss: 1.023194\n",
      "Epoch: 3725 \tTraining Loss: 0.003303 \tValidation Loss: 1.016012\n",
      "Epoch: 3726 \tTraining Loss: 0.000881 \tValidation Loss: 0.973146\n",
      "Epoch: 3727 \tTraining Loss: 0.001102 \tValidation Loss: 0.969543\n",
      "Epoch: 3728 \tTraining Loss: 0.002682 \tValidation Loss: 0.909406\n",
      "Epoch: 3729 \tTraining Loss: 0.001822 \tValidation Loss: 1.126761\n",
      "Epoch: 3730 \tTraining Loss: 0.001442 \tValidation Loss: 0.981462\n",
      "Epoch: 3731 \tTraining Loss: 0.003768 \tValidation Loss: 0.981215\n",
      "Epoch: 3732 \tTraining Loss: 0.007258 \tValidation Loss: 0.997186\n",
      "Epoch: 3733 \tTraining Loss: 0.004320 \tValidation Loss: 0.948773\n",
      "Epoch: 3734 \tTraining Loss: 0.001472 \tValidation Loss: 1.068118\n",
      "Epoch: 3735 \tTraining Loss: 0.001626 \tValidation Loss: 1.078310\n",
      "Epoch: 3736 \tTraining Loss: 0.001981 \tValidation Loss: 1.026896\n",
      "Epoch: 3737 \tTraining Loss: 0.001703 \tValidation Loss: 1.042454\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3738 \tTraining Loss: 0.001250 \tValidation Loss: 0.991388\n",
      "Epoch: 3739 \tTraining Loss: 0.001002 \tValidation Loss: 1.022030\n",
      "Epoch: 3740 \tTraining Loss: 0.001758 \tValidation Loss: 0.996159\n",
      "Epoch: 3741 \tTraining Loss: 0.000739 \tValidation Loss: 1.071436\n",
      "Epoch: 3742 \tTraining Loss: 0.001376 \tValidation Loss: 0.970723\n",
      "Epoch: 3743 \tTraining Loss: 0.000784 \tValidation Loss: 1.010406\n",
      "Epoch: 3744 \tTraining Loss: 0.001332 \tValidation Loss: 0.961379\n",
      "Epoch: 3745 \tTraining Loss: 0.001570 \tValidation Loss: 1.033506\n",
      "Epoch: 3746 \tTraining Loss: 0.001619 \tValidation Loss: 1.034624\n",
      "Epoch: 3747 \tTraining Loss: 0.000894 \tValidation Loss: 1.017851\n",
      "Epoch: 3748 \tTraining Loss: 0.000630 \tValidation Loss: 1.068319\n",
      "Epoch: 3749 \tTraining Loss: 0.001035 \tValidation Loss: 1.136502\n",
      "Epoch: 3750 \tTraining Loss: 0.000509 \tValidation Loss: 1.146402\n",
      "Epoch: 3751 \tTraining Loss: 0.000832 \tValidation Loss: 1.023612\n",
      "Epoch: 3752 \tTraining Loss: 0.002072 \tValidation Loss: 1.006816\n",
      "Epoch: 3753 \tTraining Loss: 0.000982 \tValidation Loss: 1.009389\n",
      "Epoch: 3754 \tTraining Loss: 0.000537 \tValidation Loss: 1.008516\n",
      "Epoch: 3755 \tTraining Loss: 0.000981 \tValidation Loss: 0.999954\n",
      "Epoch: 3756 \tTraining Loss: 0.000821 \tValidation Loss: 0.991629\n",
      "Epoch: 3757 \tTraining Loss: 0.001861 \tValidation Loss: 1.104230\n",
      "Epoch: 3758 \tTraining Loss: 0.001230 \tValidation Loss: 1.041393\n",
      "Epoch: 3759 \tTraining Loss: 0.000740 \tValidation Loss: 1.134211\n",
      "Epoch: 3760 \tTraining Loss: 0.001648 \tValidation Loss: 0.999084\n",
      "Epoch: 3761 \tTraining Loss: 0.000792 \tValidation Loss: 0.987211\n",
      "Epoch: 3762 \tTraining Loss: 0.001292 \tValidation Loss: 1.058270\n",
      "Epoch: 3763 \tTraining Loss: 0.000693 \tValidation Loss: 1.151033\n",
      "Epoch: 3764 \tTraining Loss: 0.013581 \tValidation Loss: 1.193144\n",
      "Epoch: 3765 \tTraining Loss: 0.008664 \tValidation Loss: 1.005778\n",
      "Epoch: 3766 \tTraining Loss: 0.003993 \tValidation Loss: 0.979779\n",
      "Epoch: 3767 \tTraining Loss: 0.002651 \tValidation Loss: 0.964643\n",
      "Epoch: 3768 \tTraining Loss: 0.002899 \tValidation Loss: 0.979014\n",
      "Epoch: 3769 \tTraining Loss: 0.003363 \tValidation Loss: 0.973631\n",
      "Epoch: 3770 \tTraining Loss: 0.002343 \tValidation Loss: 0.976393\n",
      "Epoch: 3771 \tTraining Loss: 0.001933 \tValidation Loss: 1.034144\n",
      "Epoch: 3772 \tTraining Loss: 0.001709 \tValidation Loss: 1.032493\n",
      "Epoch: 3773 \tTraining Loss: 0.001951 \tValidation Loss: 1.010783\n",
      "Epoch: 3774 \tTraining Loss: 0.001528 \tValidation Loss: 1.019157\n",
      "Epoch: 3775 \tTraining Loss: 0.001282 \tValidation Loss: 1.035391\n",
      "Epoch: 3776 \tTraining Loss: 0.001297 \tValidation Loss: 1.044222\n",
      "Epoch: 3777 \tTraining Loss: 0.002214 \tValidation Loss: 0.983511\n",
      "Epoch: 3778 \tTraining Loss: 0.026753 \tValidation Loss: 1.105559\n",
      "Epoch: 3779 \tTraining Loss: 0.098072 \tValidation Loss: 1.780678\n",
      "Epoch: 3780 \tTraining Loss: 0.139978 \tValidation Loss: 1.300001\n",
      "Epoch: 3781 \tTraining Loss: 0.522616 \tValidation Loss: 1.609525\n",
      "Epoch: 3782 \tTraining Loss: 0.512464 \tValidation Loss: 0.786625\n",
      "Epoch: 3783 \tTraining Loss: 0.221214 \tValidation Loss: 0.855288\n",
      "Epoch: 3784 \tTraining Loss: 0.142181 \tValidation Loss: 0.941101\n",
      "Epoch: 3785 \tTraining Loss: 0.049578 \tValidation Loss: 0.805242\n",
      "Epoch: 3786 \tTraining Loss: 0.200521 \tValidation Loss: 0.873764\n",
      "Epoch: 3787 \tTraining Loss: 0.212736 \tValidation Loss: 1.306879\n",
      "Epoch: 3788 \tTraining Loss: 0.245611 \tValidation Loss: 0.819776\n",
      "Epoch: 3789 \tTraining Loss: 0.110427 \tValidation Loss: 1.116582\n",
      "Epoch: 3790 \tTraining Loss: 0.035770 \tValidation Loss: 1.012071\n",
      "Epoch: 3791 \tTraining Loss: 0.059760 \tValidation Loss: 1.080932\n",
      "Epoch: 3792 \tTraining Loss: 0.064815 \tValidation Loss: 1.020907\n",
      "Epoch: 3793 \tTraining Loss: 0.045084 \tValidation Loss: 1.004832\n",
      "Epoch: 3794 \tTraining Loss: 0.028727 \tValidation Loss: 0.922970\n",
      "Epoch: 3795 \tTraining Loss: 0.017042 \tValidation Loss: 0.908032\n",
      "Epoch: 3796 \tTraining Loss: 0.013268 \tValidation Loss: 0.876683\n",
      "Epoch: 3797 \tTraining Loss: 0.009834 \tValidation Loss: 0.911495\n",
      "Epoch: 3798 \tTraining Loss: 0.007657 \tValidation Loss: 0.973211\n",
      "Epoch: 3799 \tTraining Loss: 0.009777 \tValidation Loss: 0.968786\n",
      "Epoch: 3800 \tTraining Loss: 0.005994 \tValidation Loss: 0.979046\n",
      "Epoch: 3801 \tTraining Loss: 0.004596 \tValidation Loss: 0.963044\n",
      "Epoch: 3802 \tTraining Loss: 0.012041 \tValidation Loss: 0.917404\n",
      "Epoch: 3803 \tTraining Loss: 0.005360 \tValidation Loss: 0.913190\n",
      "Epoch: 3804 \tTraining Loss: 0.019905 \tValidation Loss: 0.962598\n",
      "Epoch: 3805 \tTraining Loss: 0.060690 \tValidation Loss: 1.466270\n",
      "Epoch: 3806 \tTraining Loss: 0.043550 \tValidation Loss: 1.321115\n",
      "Epoch: 3807 \tTraining Loss: 0.075682 \tValidation Loss: 1.182794\n",
      "Epoch: 3808 \tTraining Loss: 0.013723 \tValidation Loss: 1.089016\n",
      "Epoch: 3809 \tTraining Loss: 0.067919 \tValidation Loss: 1.179625\n",
      "Epoch: 3810 \tTraining Loss: 0.133355 \tValidation Loss: 1.211241\n",
      "Epoch: 3811 \tTraining Loss: 0.095085 \tValidation Loss: 0.945508\n",
      "Epoch: 3812 \tTraining Loss: 0.048784 \tValidation Loss: 0.943270\n",
      "Epoch: 3813 \tTraining Loss: 0.040828 \tValidation Loss: 0.986189\n",
      "Epoch: 3814 \tTraining Loss: 0.035913 \tValidation Loss: 0.981749\n",
      "Epoch: 3815 \tTraining Loss: 0.024998 \tValidation Loss: 0.989686\n",
      "Epoch: 3816 \tTraining Loss: 0.040366 \tValidation Loss: 1.009214\n",
      "Epoch: 3817 \tTraining Loss: 0.026786 \tValidation Loss: 1.251524\n",
      "Epoch: 3818 \tTraining Loss: 0.042815 \tValidation Loss: 1.153230\n",
      "Epoch: 3819 \tTraining Loss: 0.011471 \tValidation Loss: 1.193805\n",
      "Epoch: 3820 \tTraining Loss: 0.009059 \tValidation Loss: 1.155600\n",
      "Epoch: 3821 \tTraining Loss: 0.010577 \tValidation Loss: 1.109964\n",
      "Epoch: 3822 \tTraining Loss: 0.013823 \tValidation Loss: 1.116116\n",
      "Epoch: 3823 \tTraining Loss: 0.009521 \tValidation Loss: 1.062384\n",
      "Epoch: 3824 \tTraining Loss: 0.004139 \tValidation Loss: 1.089062\n",
      "Epoch: 3825 \tTraining Loss: 0.004320 \tValidation Loss: 1.098831\n",
      "Epoch: 3826 \tTraining Loss: 0.009781 \tValidation Loss: 1.226707\n",
      "Epoch: 3827 \tTraining Loss: 0.006561 \tValidation Loss: 1.129932\n",
      "Epoch: 3828 \tTraining Loss: 0.005426 \tValidation Loss: 1.143306\n",
      "Epoch: 3829 \tTraining Loss: 0.008513 \tValidation Loss: 1.123946\n",
      "Epoch: 3830 \tTraining Loss: 0.010394 \tValidation Loss: 1.237718\n",
      "Epoch: 3831 \tTraining Loss: 0.003264 \tValidation Loss: 1.164120\n",
      "Epoch: 3832 \tTraining Loss: 0.002622 \tValidation Loss: 1.184363\n",
      "Epoch: 3833 \tTraining Loss: 0.004464 \tValidation Loss: 1.178614\n",
      "Epoch: 3834 \tTraining Loss: 0.002727 \tValidation Loss: 1.211161\n",
      "Epoch: 3835 \tTraining Loss: 0.004424 \tValidation Loss: 1.151626\n",
      "Epoch: 3836 \tTraining Loss: 0.004113 \tValidation Loss: 1.207702\n",
      "Epoch: 3837 \tTraining Loss: 0.002757 \tValidation Loss: 1.115772\n",
      "Epoch: 3838 \tTraining Loss: 0.003342 \tValidation Loss: 1.121114\n",
      "Epoch: 3839 \tTraining Loss: 0.003335 \tValidation Loss: 1.161969\n",
      "Epoch: 3840 \tTraining Loss: 0.003445 \tValidation Loss: 1.177000\n",
      "Epoch: 3841 \tTraining Loss: 0.002501 \tValidation Loss: 1.131631\n",
      "Epoch: 3842 \tTraining Loss: 0.006282 \tValidation Loss: 1.333107\n",
      "Epoch: 3843 \tTraining Loss: 0.002050 \tValidation Loss: 1.381731\n",
      "Epoch: 3844 \tTraining Loss: 0.003272 \tValidation Loss: 1.207609\n",
      "Epoch: 3845 \tTraining Loss: 0.001754 \tValidation Loss: 1.285735\n",
      "Epoch: 3846 \tTraining Loss: 0.002465 \tValidation Loss: 1.315444\n",
      "Epoch: 3847 \tTraining Loss: 0.001483 \tValidation Loss: 1.252767\n",
      "Epoch: 3848 \tTraining Loss: 0.002187 \tValidation Loss: 1.335060\n",
      "Epoch: 3849 \tTraining Loss: 0.002182 \tValidation Loss: 1.269796\n",
      "Epoch: 3850 \tTraining Loss: 0.001731 \tValidation Loss: 1.321943\n",
      "Epoch: 3851 \tTraining Loss: 0.001245 \tValidation Loss: 1.276680\n",
      "Epoch: 3852 \tTraining Loss: 0.002257 \tValidation Loss: 1.247482\n",
      "Epoch: 3853 \tTraining Loss: 0.002469 \tValidation Loss: 1.279336\n",
      "Epoch: 3854 \tTraining Loss: 0.012585 \tValidation Loss: 1.219953\n",
      "Epoch: 3855 \tTraining Loss: 0.007346 \tValidation Loss: 1.151187\n",
      "Epoch: 3856 \tTraining Loss: 0.003224 \tValidation Loss: 1.075947\n",
      "Epoch: 3857 \tTraining Loss: 0.003566 \tValidation Loss: 1.088662\n",
      "Epoch: 3858 \tTraining Loss: 0.006138 \tValidation Loss: 1.085071\n",
      "Epoch: 3859 \tTraining Loss: 0.002396 \tValidation Loss: 1.026411\n",
      "Epoch: 3860 \tTraining Loss: 0.003772 \tValidation Loss: 1.041823\n",
      "Epoch: 3861 \tTraining Loss: 0.002754 \tValidation Loss: 1.080424\n",
      "Epoch: 3862 \tTraining Loss: 0.001884 \tValidation Loss: 1.067556\n",
      "Epoch: 3863 \tTraining Loss: 0.003772 \tValidation Loss: 1.160360\n",
      "Epoch: 3864 \tTraining Loss: 0.006864 \tValidation Loss: 1.210804\n",
      "Epoch: 3865 \tTraining Loss: 0.010593 \tValidation Loss: 1.550804\n",
      "Epoch: 3866 \tTraining Loss: 0.002557 \tValidation Loss: 1.645937\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3867 \tTraining Loss: 0.003083 \tValidation Loss: 1.589752\n",
      "Epoch: 3868 \tTraining Loss: 0.061725 \tValidation Loss: 1.290021\n",
      "Epoch: 3869 \tTraining Loss: 0.115239 \tValidation Loss: 1.007061\n",
      "Epoch: 3870 \tTraining Loss: 0.012507 \tValidation Loss: 1.130196\n",
      "Epoch: 3871 \tTraining Loss: 0.026275 \tValidation Loss: 0.990686\n",
      "Epoch: 3872 \tTraining Loss: 0.005868 \tValidation Loss: 0.799201\n",
      "Epoch: 3873 \tTraining Loss: 0.026938 \tValidation Loss: 0.933913\n",
      "Epoch: 3874 \tTraining Loss: 0.007299 \tValidation Loss: 0.945080\n",
      "Epoch: 3875 \tTraining Loss: 0.014948 \tValidation Loss: 1.077619\n",
      "Epoch: 3876 \tTraining Loss: 0.004367 \tValidation Loss: 1.103844\n",
      "Epoch: 3877 \tTraining Loss: 0.003410 \tValidation Loss: 1.198694\n",
      "Epoch: 3878 \tTraining Loss: 0.002690 \tValidation Loss: 1.181405\n",
      "Epoch: 3879 \tTraining Loss: 0.006009 \tValidation Loss: 1.203658\n",
      "Epoch: 3880 \tTraining Loss: 0.002338 \tValidation Loss: 1.214888\n",
      "Epoch: 3881 \tTraining Loss: 0.003297 \tValidation Loss: 1.146440\n",
      "Epoch: 3882 \tTraining Loss: 0.002021 \tValidation Loss: 1.113923\n",
      "Epoch: 3883 \tTraining Loss: 0.002295 \tValidation Loss: 1.267080\n",
      "Epoch: 3884 \tTraining Loss: 0.003132 \tValidation Loss: 1.090888\n",
      "Epoch: 3885 \tTraining Loss: 0.001285 \tValidation Loss: 1.115902\n",
      "Epoch: 3886 \tTraining Loss: 0.001641 \tValidation Loss: 1.254944\n",
      "Epoch: 3887 \tTraining Loss: 0.001130 \tValidation Loss: 1.180907\n",
      "Epoch: 3888 \tTraining Loss: 0.000995 \tValidation Loss: 1.175023\n",
      "Epoch: 3889 \tTraining Loss: 0.002177 \tValidation Loss: 1.088354\n",
      "Epoch: 3890 \tTraining Loss: 0.004428 \tValidation Loss: 1.131678\n",
      "Epoch: 3891 \tTraining Loss: 0.001053 \tValidation Loss: 1.133085\n",
      "Epoch: 3892 \tTraining Loss: 0.001858 \tValidation Loss: 1.194257\n",
      "Epoch: 3893 \tTraining Loss: 0.002544 \tValidation Loss: 1.200307\n",
      "Epoch: 3894 \tTraining Loss: 0.002559 \tValidation Loss: 1.171529\n",
      "Epoch: 3895 \tTraining Loss: 0.001576 \tValidation Loss: 1.187594\n",
      "Epoch: 3896 \tTraining Loss: 0.010782 \tValidation Loss: 1.276489\n",
      "Epoch: 3897 \tTraining Loss: 0.044042 \tValidation Loss: 1.456786\n",
      "Epoch: 3898 \tTraining Loss: 0.018174 \tValidation Loss: 1.080604\n",
      "Epoch: 3899 \tTraining Loss: 0.067061 \tValidation Loss: 1.055717\n",
      "Epoch: 3900 \tTraining Loss: 0.013271 \tValidation Loss: 0.985509\n",
      "Epoch: 3901 \tTraining Loss: 0.008274 \tValidation Loss: 0.940605\n",
      "Epoch: 3902 \tTraining Loss: 0.003963 \tValidation Loss: 0.920379\n",
      "Epoch: 3903 \tTraining Loss: 0.004647 \tValidation Loss: 0.909803\n",
      "Epoch: 3904 \tTraining Loss: 0.002121 \tValidation Loss: 0.914916\n",
      "Epoch: 3905 \tTraining Loss: 0.002326 \tValidation Loss: 0.935210\n",
      "Epoch: 3906 \tTraining Loss: 0.007849 \tValidation Loss: 0.932455\n",
      "Epoch: 3907 \tTraining Loss: 0.002636 \tValidation Loss: 0.972328\n",
      "Epoch: 3908 \tTraining Loss: 0.005608 \tValidation Loss: 0.996471\n",
      "Epoch: 3909 \tTraining Loss: 0.002966 \tValidation Loss: 1.002587\n",
      "Epoch: 3910 \tTraining Loss: 0.002957 \tValidation Loss: 1.057945\n",
      "Epoch: 3911 \tTraining Loss: 0.001046 \tValidation Loss: 0.964747\n",
      "Epoch: 3912 \tTraining Loss: 0.001470 \tValidation Loss: 0.988840\n",
      "Epoch: 3913 \tTraining Loss: 0.002816 \tValidation Loss: 0.991840\n",
      "Epoch: 3914 \tTraining Loss: 0.001241 \tValidation Loss: 1.004265\n",
      "Epoch: 3915 \tTraining Loss: 0.001309 \tValidation Loss: 0.991381\n",
      "Epoch: 3916 \tTraining Loss: 0.008808 \tValidation Loss: 1.233330\n",
      "Epoch: 3917 \tTraining Loss: 0.007866 \tValidation Loss: 1.172772\n",
      "Epoch: 3918 \tTraining Loss: 0.005374 \tValidation Loss: 1.068938\n",
      "Epoch: 3919 \tTraining Loss: 0.002510 \tValidation Loss: 1.249151\n",
      "Epoch: 3920 \tTraining Loss: 0.013634 \tValidation Loss: 0.770834\n",
      "Epoch: 3921 \tTraining Loss: 0.023877 \tValidation Loss: 0.906177\n",
      "Epoch: 3922 \tTraining Loss: 0.007547 \tValidation Loss: 1.000993\n",
      "Epoch: 3923 \tTraining Loss: 0.003581 \tValidation Loss: 0.904957\n",
      "Epoch: 3924 \tTraining Loss: 0.006309 \tValidation Loss: 1.007804\n",
      "Epoch: 3925 \tTraining Loss: 0.002371 \tValidation Loss: 0.861874\n",
      "Epoch: 3926 \tTraining Loss: 0.010459 \tValidation Loss: 1.092662\n",
      "Epoch: 3927 \tTraining Loss: 0.004668 \tValidation Loss: 1.158772\n",
      "Epoch: 3928 \tTraining Loss: 0.007674 \tValidation Loss: 1.025557\n",
      "Epoch: 3929 \tTraining Loss: 0.002263 \tValidation Loss: 0.976350\n",
      "Epoch: 3930 \tTraining Loss: 0.003705 \tValidation Loss: 1.277436\n",
      "Epoch: 3931 \tTraining Loss: 0.005034 \tValidation Loss: 0.826564\n",
      "Epoch: 3932 \tTraining Loss: 0.010205 \tValidation Loss: 1.244163\n",
      "Epoch: 3933 \tTraining Loss: 0.096602 \tValidation Loss: 2.481931\n",
      "Epoch: 3934 \tTraining Loss: 0.174753 \tValidation Loss: 2.116561\n",
      "Epoch: 3935 \tTraining Loss: 0.053588 \tValidation Loss: 0.788971\n",
      "Epoch: 3936 \tTraining Loss: 0.156392 \tValidation Loss: 0.786127\n",
      "Epoch: 3937 \tTraining Loss: 0.147422 \tValidation Loss: 1.001872\n",
      "Epoch: 3938 \tTraining Loss: 0.161721 \tValidation Loss: 1.390981\n",
      "Epoch: 3939 \tTraining Loss: 0.056689 \tValidation Loss: 0.950285\n",
      "Epoch: 3940 \tTraining Loss: 0.018417 \tValidation Loss: 0.883976\n",
      "Epoch: 3941 \tTraining Loss: 0.013241 \tValidation Loss: 0.831223\n",
      "Epoch: 3942 \tTraining Loss: 0.024866 \tValidation Loss: 0.869897\n",
      "Epoch: 3943 \tTraining Loss: 0.008393 \tValidation Loss: 0.860454\n",
      "Epoch: 3944 \tTraining Loss: 0.030419 \tValidation Loss: 0.893692\n",
      "Epoch: 3945 \tTraining Loss: 0.030805 \tValidation Loss: 0.870771\n",
      "Epoch: 3946 \tTraining Loss: 0.040125 \tValidation Loss: 1.040927\n",
      "Epoch: 3947 \tTraining Loss: 0.085351 \tValidation Loss: 0.901374\n",
      "Epoch: 3948 \tTraining Loss: 0.038696 \tValidation Loss: 0.872630\n",
      "Epoch: 3949 \tTraining Loss: 0.010558 \tValidation Loss: 0.788333\n",
      "Epoch: 3950 \tTraining Loss: 0.008045 \tValidation Loss: 0.798221\n",
      "Epoch: 3951 \tTraining Loss: 0.006173 \tValidation Loss: 0.785162\n",
      "Epoch: 3952 \tTraining Loss: 0.007493 \tValidation Loss: 0.836922\n",
      "Epoch: 3953 \tTraining Loss: 0.002790 \tValidation Loss: 0.827085\n",
      "Epoch: 3954 \tTraining Loss: 0.002887 \tValidation Loss: 0.887022\n",
      "Epoch: 3955 \tTraining Loss: 0.003236 \tValidation Loss: 0.932696\n",
      "Epoch: 3956 \tTraining Loss: 0.002540 \tValidation Loss: 0.879952\n",
      "Epoch: 3957 \tTraining Loss: 0.006575 \tValidation Loss: 0.892891\n",
      "Epoch: 3958 \tTraining Loss: 0.001914 \tValidation Loss: 0.853392\n",
      "Epoch: 3959 \tTraining Loss: 0.006067 \tValidation Loss: 0.951274\n",
      "Epoch: 3960 \tTraining Loss: 0.001635 \tValidation Loss: 0.946190\n",
      "Epoch: 3961 \tTraining Loss: 0.003531 \tValidation Loss: 0.919848\n",
      "Epoch: 3962 \tTraining Loss: 0.002517 \tValidation Loss: 0.934372\n",
      "Epoch: 3963 \tTraining Loss: 0.002689 \tValidation Loss: 0.961794\n",
      "Epoch: 3964 \tTraining Loss: 0.010530 \tValidation Loss: 0.999034\n",
      "Epoch: 3965 \tTraining Loss: 0.004882 \tValidation Loss: 0.979185\n",
      "Epoch: 3966 \tTraining Loss: 0.002296 \tValidation Loss: 0.994073\n",
      "Epoch: 3967 \tTraining Loss: 0.003772 \tValidation Loss: 1.099913\n",
      "Epoch: 3968 \tTraining Loss: 0.010070 \tValidation Loss: 0.970716\n",
      "Epoch: 3969 \tTraining Loss: 0.003031 \tValidation Loss: 1.051028\n",
      "Epoch: 3970 \tTraining Loss: 0.004980 \tValidation Loss: 1.142452\n",
      "Epoch: 3971 \tTraining Loss: 0.002240 \tValidation Loss: 1.173599\n",
      "Epoch: 3972 \tTraining Loss: 0.001702 \tValidation Loss: 1.211530\n",
      "Epoch: 3973 \tTraining Loss: 0.001873 \tValidation Loss: 1.195108\n",
      "Epoch: 3974 \tTraining Loss: 0.002019 \tValidation Loss: 1.215988\n",
      "Epoch: 3975 \tTraining Loss: 0.001539 \tValidation Loss: 1.211419\n",
      "Epoch: 3976 \tTraining Loss: 0.002807 \tValidation Loss: 1.187334\n",
      "Epoch: 3977 \tTraining Loss: 0.001790 \tValidation Loss: 1.263794\n",
      "Epoch: 3978 \tTraining Loss: 0.002188 \tValidation Loss: 1.272082\n",
      "Epoch: 3979 \tTraining Loss: 0.001480 \tValidation Loss: 1.198308\n",
      "Epoch: 3980 \tTraining Loss: 0.003065 \tValidation Loss: 1.165408\n",
      "Epoch: 3981 \tTraining Loss: 0.000799 \tValidation Loss: 1.220014\n",
      "Epoch: 3982 \tTraining Loss: 0.001526 \tValidation Loss: 1.203205\n",
      "Epoch: 3983 \tTraining Loss: 0.000910 \tValidation Loss: 1.260160\n",
      "Epoch: 3984 \tTraining Loss: 0.001431 \tValidation Loss: 1.216448\n",
      "Epoch: 3985 \tTraining Loss: 0.001840 \tValidation Loss: 1.226951\n",
      "Epoch: 3986 \tTraining Loss: 0.001028 \tValidation Loss: 1.236497\n",
      "Epoch: 3987 \tTraining Loss: 0.000881 \tValidation Loss: 1.312174\n",
      "Epoch: 3988 \tTraining Loss: 0.000876 \tValidation Loss: 1.321552\n",
      "Epoch: 3989 \tTraining Loss: 0.000893 \tValidation Loss: 1.251504\n",
      "Epoch: 3990 \tTraining Loss: 0.001625 \tValidation Loss: 1.226235\n",
      "Epoch: 3991 \tTraining Loss: 0.000889 \tValidation Loss: 1.248497\n",
      "Epoch: 3992 \tTraining Loss: 0.001229 \tValidation Loss: 1.305804\n",
      "Epoch: 3993 \tTraining Loss: 0.001749 \tValidation Loss: 1.237645\n",
      "Epoch: 3994 \tTraining Loss: 0.002035 \tValidation Loss: 1.317663\n",
      "Epoch: 3995 \tTraining Loss: 0.000971 \tValidation Loss: 1.217819\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 3996 \tTraining Loss: 0.001087 \tValidation Loss: 1.234449\n",
      "Epoch: 3997 \tTraining Loss: 0.008772 \tValidation Loss: 1.353015\n",
      "Epoch: 3998 \tTraining Loss: 0.002043 \tValidation Loss: 1.000865\n",
      "Epoch: 3999 \tTraining Loss: 0.002351 \tValidation Loss: 0.995007\n",
      "Epoch: 4000 \tTraining Loss: 0.002297 \tValidation Loss: 1.015494\n",
      "Epoch: 4001 \tTraining Loss: 0.003905 \tValidation Loss: 1.046580\n",
      "Epoch: 4002 \tTraining Loss: 0.003124 \tValidation Loss: 1.011754\n",
      "Epoch: 4003 \tTraining Loss: 0.001634 \tValidation Loss: 1.031183\n",
      "Epoch: 4004 \tTraining Loss: 0.001588 \tValidation Loss: 1.060069\n",
      "Epoch: 4005 \tTraining Loss: 0.001484 \tValidation Loss: 1.077171\n",
      "Epoch: 4006 \tTraining Loss: 0.002172 \tValidation Loss: 1.032461\n",
      "Epoch: 4007 \tTraining Loss: 0.003197 \tValidation Loss: 1.257350\n",
      "Epoch: 4008 \tTraining Loss: 0.000938 \tValidation Loss: 1.169678\n",
      "Epoch: 4009 \tTraining Loss: 0.001305 \tValidation Loss: 1.147262\n",
      "Epoch: 4010 \tTraining Loss: 0.000620 \tValidation Loss: 1.091434\n",
      "Epoch: 4011 \tTraining Loss: 0.000998 \tValidation Loss: 1.131243\n",
      "Epoch: 4012 \tTraining Loss: 0.000617 \tValidation Loss: 1.048379\n",
      "Epoch: 4013 \tTraining Loss: 0.000715 \tValidation Loss: 1.111762\n",
      "Epoch: 4014 \tTraining Loss: 0.000790 \tValidation Loss: 1.030557\n",
      "Epoch: 4015 \tTraining Loss: 0.001010 \tValidation Loss: 1.074080\n",
      "Epoch: 4016 \tTraining Loss: 0.000827 \tValidation Loss: 1.001034\n",
      "Epoch: 4017 \tTraining Loss: 0.000943 \tValidation Loss: 1.063745\n",
      "Epoch: 4018 \tTraining Loss: 0.000946 \tValidation Loss: 1.064209\n",
      "Epoch: 4019 \tTraining Loss: 0.001386 \tValidation Loss: 1.098640\n",
      "Epoch: 4020 \tTraining Loss: 0.001023 \tValidation Loss: 1.080071\n",
      "Epoch: 4021 \tTraining Loss: 0.001188 \tValidation Loss: 1.083265\n",
      "Epoch: 4022 \tTraining Loss: 0.001927 \tValidation Loss: 1.119522\n",
      "Epoch: 4023 \tTraining Loss: 0.000983 \tValidation Loss: 1.144670\n",
      "Epoch: 4024 \tTraining Loss: 0.000845 \tValidation Loss: 1.056487\n",
      "Epoch: 4025 \tTraining Loss: 0.000544 \tValidation Loss: 1.056704\n",
      "Epoch: 4026 \tTraining Loss: 0.001233 \tValidation Loss: 1.079315\n",
      "Epoch: 4027 \tTraining Loss: 0.001323 \tValidation Loss: 1.076063\n",
      "Epoch: 4028 \tTraining Loss: 0.000581 \tValidation Loss: 0.992806\n",
      "Epoch: 4029 \tTraining Loss: 0.001534 \tValidation Loss: 1.077139\n",
      "Epoch: 4030 \tTraining Loss: 0.001815 \tValidation Loss: 1.023187\n",
      "Epoch: 4031 \tTraining Loss: 0.001371 \tValidation Loss: 0.969745\n",
      "Epoch: 4032 \tTraining Loss: 0.001619 \tValidation Loss: 1.021688\n",
      "Epoch: 4033 \tTraining Loss: 0.000473 \tValidation Loss: 1.097204\n",
      "Epoch: 4034 \tTraining Loss: 0.000682 \tValidation Loss: 1.043663\n",
      "Epoch: 4035 \tTraining Loss: 0.000465 \tValidation Loss: 1.120273\n",
      "Epoch: 4036 \tTraining Loss: 0.000773 \tValidation Loss: 1.080007\n",
      "Epoch: 4037 \tTraining Loss: 0.000608 \tValidation Loss: 1.164775\n",
      "Epoch: 4038 \tTraining Loss: 0.000582 \tValidation Loss: 1.123644\n",
      "Epoch: 4039 \tTraining Loss: 0.000535 \tValidation Loss: 1.094453\n",
      "Epoch: 4040 \tTraining Loss: 0.000493 \tValidation Loss: 1.167859\n",
      "Epoch: 4041 \tTraining Loss: 0.001134 \tValidation Loss: 1.128071\n",
      "Epoch: 4042 \tTraining Loss: 0.000637 \tValidation Loss: 1.098516\n",
      "Epoch: 4043 \tTraining Loss: 0.002336 \tValidation Loss: 0.985148\n",
      "Epoch: 4044 \tTraining Loss: 0.000874 \tValidation Loss: 1.013659\n",
      "Epoch: 4045 \tTraining Loss: 0.001981 \tValidation Loss: 1.008044\n",
      "Epoch: 4046 \tTraining Loss: 0.020066 \tValidation Loss: 1.028233\n",
      "Epoch: 4047 \tTraining Loss: 0.007836 \tValidation Loss: 1.069555\n",
      "Epoch: 4048 \tTraining Loss: 0.004219 \tValidation Loss: 1.038852\n",
      "Epoch: 4049 \tTraining Loss: 0.002782 \tValidation Loss: 1.070407\n",
      "Epoch: 4050 \tTraining Loss: 0.009836 \tValidation Loss: 1.141317\n",
      "Epoch: 4051 \tTraining Loss: 0.002171 \tValidation Loss: 1.080246\n",
      "Epoch: 4052 \tTraining Loss: 0.002841 \tValidation Loss: 1.119290\n",
      "Epoch: 4053 \tTraining Loss: 0.004781 \tValidation Loss: 1.195448\n",
      "Epoch: 4054 \tTraining Loss: 0.001426 \tValidation Loss: 1.207596\n",
      "Epoch: 4055 \tTraining Loss: 0.001833 \tValidation Loss: 1.245352\n",
      "Epoch: 4056 \tTraining Loss: 0.001928 \tValidation Loss: 1.187068\n",
      "Epoch: 4057 \tTraining Loss: 0.007312 \tValidation Loss: 1.584856\n",
      "Epoch: 4058 \tTraining Loss: 0.004285 \tValidation Loss: 1.350644\n",
      "Epoch: 4059 \tTraining Loss: 0.040641 \tValidation Loss: 1.542366\n",
      "Epoch: 4060 \tTraining Loss: 0.155918 \tValidation Loss: 1.031090\n",
      "Epoch: 4061 \tTraining Loss: 0.058079 \tValidation Loss: 1.383617\n",
      "Epoch: 4062 \tTraining Loss: 0.089404 \tValidation Loss: 1.007839\n",
      "Epoch: 4063 \tTraining Loss: 0.034294 \tValidation Loss: 1.054130\n",
      "Epoch: 4064 \tTraining Loss: 0.010924 \tValidation Loss: 1.208001\n",
      "Epoch: 4065 \tTraining Loss: 0.003204 \tValidation Loss: 1.235431\n",
      "Epoch: 4066 \tTraining Loss: 0.008029 \tValidation Loss: 1.260471\n",
      "Epoch: 4067 \tTraining Loss: 0.001933 \tValidation Loss: 1.300023\n",
      "Epoch: 4068 \tTraining Loss: 0.003979 \tValidation Loss: 1.287332\n",
      "Epoch: 4069 \tTraining Loss: 0.024118 \tValidation Loss: 1.293861\n",
      "Epoch: 4070 \tTraining Loss: 0.053273 \tValidation Loss: 1.443832\n",
      "Epoch: 4071 \tTraining Loss: 0.040375 \tValidation Loss: 1.167925\n",
      "Epoch: 4072 \tTraining Loss: 0.032905 \tValidation Loss: 1.170838\n",
      "Epoch: 4073 \tTraining Loss: 0.015364 \tValidation Loss: 1.039345\n",
      "Epoch: 4074 \tTraining Loss: 0.006171 \tValidation Loss: 0.843558\n",
      "Epoch: 4075 \tTraining Loss: 0.002720 \tValidation Loss: 0.864073\n",
      "Epoch: 4076 \tTraining Loss: 0.009096 \tValidation Loss: 1.019941\n",
      "Epoch: 4077 \tTraining Loss: 0.001676 \tValidation Loss: 1.052323\n",
      "Epoch: 4078 \tTraining Loss: 0.004068 \tValidation Loss: 0.985149\n",
      "Epoch: 4079 \tTraining Loss: 0.003685 \tValidation Loss: 1.098990\n",
      "Epoch: 4080 \tTraining Loss: 0.002848 \tValidation Loss: 1.113863\n",
      "Epoch: 4081 \tTraining Loss: 0.010378 \tValidation Loss: 1.205905\n",
      "Epoch: 4082 \tTraining Loss: 0.001681 \tValidation Loss: 1.167365\n",
      "Epoch: 4083 \tTraining Loss: 0.001576 \tValidation Loss: 1.215976\n",
      "Epoch: 4084 \tTraining Loss: 0.005789 \tValidation Loss: 1.048912\n",
      "Epoch: 4085 \tTraining Loss: 0.002022 \tValidation Loss: 1.072768\n",
      "Epoch: 4086 \tTraining Loss: 0.001101 \tValidation Loss: 1.103759\n",
      "Epoch: 4087 \tTraining Loss: 0.002363 \tValidation Loss: 1.111566\n",
      "Epoch: 4088 \tTraining Loss: 0.000987 \tValidation Loss: 1.064276\n",
      "Epoch: 4089 \tTraining Loss: 0.006517 \tValidation Loss: 1.049914\n",
      "Epoch: 4090 \tTraining Loss: 0.004232 \tValidation Loss: 1.118028\n",
      "Epoch: 4091 \tTraining Loss: 0.002428 \tValidation Loss: 1.107895\n",
      "Epoch: 4092 \tTraining Loss: 0.019397 \tValidation Loss: 1.254572\n",
      "Epoch: 4093 \tTraining Loss: 0.035895 \tValidation Loss: 1.256034\n",
      "Epoch: 4094 \tTraining Loss: 0.102922 \tValidation Loss: 1.846771\n",
      "Epoch: 4095 \tTraining Loss: 0.266396 \tValidation Loss: 1.568703\n",
      "Epoch: 4096 \tTraining Loss: 0.295379 \tValidation Loss: 0.850275\n",
      "Epoch: 4097 \tTraining Loss: 0.089784 \tValidation Loss: 0.794230\n",
      "Epoch: 4098 \tTraining Loss: 0.047381 \tValidation Loss: 0.753422\n",
      "Epoch: 4099 \tTraining Loss: 0.016574 \tValidation Loss: 0.921584\n",
      "Epoch: 4100 \tTraining Loss: 0.011956 \tValidation Loss: 0.853870\n",
      "Epoch: 4101 \tTraining Loss: 0.008262 \tValidation Loss: 0.877386\n",
      "Epoch: 4102 \tTraining Loss: 0.011834 \tValidation Loss: 0.854367\n",
      "Epoch: 4103 \tTraining Loss: 0.004569 \tValidation Loss: 0.696224\n",
      "Epoch: 4104 \tTraining Loss: 0.005048 \tValidation Loss: 0.743459\n",
      "Epoch: 4105 \tTraining Loss: 0.003620 \tValidation Loss: 0.833235\n",
      "Epoch: 4106 \tTraining Loss: 0.004960 \tValidation Loss: 0.846412\n",
      "Epoch: 4107 \tTraining Loss: 0.003002 \tValidation Loss: 0.863930\n",
      "Epoch: 4108 \tTraining Loss: 0.002981 \tValidation Loss: 0.928350\n",
      "Epoch: 4109 \tTraining Loss: 0.002326 \tValidation Loss: 0.925164\n",
      "Epoch: 4110 \tTraining Loss: 0.096624 \tValidation Loss: 1.615925\n",
      "Epoch: 4111 \tTraining Loss: 0.075064 \tValidation Loss: 1.150060\n",
      "Epoch: 4112 \tTraining Loss: 0.032284 \tValidation Loss: 0.973199\n",
      "Epoch: 4113 \tTraining Loss: 0.029503 \tValidation Loss: 1.345618\n",
      "Epoch: 4114 \tTraining Loss: 0.028065 \tValidation Loss: 1.375687\n",
      "Epoch: 4115 \tTraining Loss: 0.006112 \tValidation Loss: 1.404005\n",
      "Epoch: 4116 \tTraining Loss: 0.007410 \tValidation Loss: 1.394340\n",
      "Epoch: 4117 \tTraining Loss: 0.004790 \tValidation Loss: 1.329844\n",
      "Epoch: 4118 \tTraining Loss: 0.001782 \tValidation Loss: 1.326183\n",
      "Epoch: 4119 \tTraining Loss: 0.003430 \tValidation Loss: 1.324435\n",
      "Epoch: 4120 \tTraining Loss: 0.004387 \tValidation Loss: 1.263267\n",
      "Epoch: 4121 \tTraining Loss: 0.002826 \tValidation Loss: 1.219547\n",
      "Epoch: 4122 \tTraining Loss: 0.003609 \tValidation Loss: 1.225636\n",
      "Epoch: 4123 \tTraining Loss: 0.004050 \tValidation Loss: 1.206930\n",
      "Epoch: 4124 \tTraining Loss: 0.001957 \tValidation Loss: 1.221835\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4125 \tTraining Loss: 0.001709 \tValidation Loss: 1.233310\n",
      "Epoch: 4126 \tTraining Loss: 0.002338 \tValidation Loss: 1.234653\n",
      "Epoch: 4127 \tTraining Loss: 0.001588 \tValidation Loss: 1.273948\n",
      "Epoch: 4128 \tTraining Loss: 0.001260 \tValidation Loss: 1.335696\n",
      "Epoch: 4129 \tTraining Loss: 0.001027 \tValidation Loss: 1.364607\n",
      "Epoch: 4130 \tTraining Loss: 0.002120 \tValidation Loss: 1.312036\n",
      "Epoch: 4131 \tTraining Loss: 0.001011 \tValidation Loss: 1.324835\n",
      "Epoch: 4132 \tTraining Loss: 0.002079 \tValidation Loss: 1.247610\n",
      "Epoch: 4133 \tTraining Loss: 0.002857 \tValidation Loss: 1.245489\n",
      "Epoch: 4134 \tTraining Loss: 0.003128 \tValidation Loss: 1.277573\n",
      "Epoch: 4135 \tTraining Loss: 0.002759 \tValidation Loss: 1.208512\n",
      "Epoch: 4136 \tTraining Loss: 0.001488 \tValidation Loss: 1.170175\n",
      "Epoch: 4137 \tTraining Loss: 0.001487 \tValidation Loss: 1.171773\n",
      "Epoch: 4138 \tTraining Loss: 0.003149 \tValidation Loss: 1.239242\n",
      "Epoch: 4139 \tTraining Loss: 0.001382 \tValidation Loss: 1.387503\n",
      "Epoch: 4140 \tTraining Loss: 0.002276 \tValidation Loss: 1.269905\n",
      "Epoch: 4141 \tTraining Loss: 0.002328 \tValidation Loss: 1.171592\n",
      "Epoch: 4142 \tTraining Loss: 0.001786 \tValidation Loss: 1.205815\n",
      "Epoch: 4143 \tTraining Loss: 0.001565 \tValidation Loss: 1.184288\n",
      "Epoch: 4144 \tTraining Loss: 0.001222 \tValidation Loss: 1.240952\n",
      "Epoch: 4145 \tTraining Loss: 0.001049 \tValidation Loss: 1.239679\n",
      "Epoch: 4146 \tTraining Loss: 0.002652 \tValidation Loss: 1.250399\n",
      "Epoch: 4147 \tTraining Loss: 0.000886 \tValidation Loss: 1.219126\n",
      "Epoch: 4148 \tTraining Loss: 0.001318 \tValidation Loss: 1.224177\n",
      "Epoch: 4149 \tTraining Loss: 0.000834 \tValidation Loss: 1.199751\n",
      "Epoch: 4150 \tTraining Loss: 0.000896 \tValidation Loss: 1.222488\n",
      "Epoch: 4151 \tTraining Loss: 0.001049 \tValidation Loss: 1.221907\n",
      "Epoch: 4152 \tTraining Loss: 0.000722 \tValidation Loss: 1.268786\n",
      "Epoch: 4153 \tTraining Loss: 0.000736 \tValidation Loss: 1.225693\n",
      "Epoch: 4154 \tTraining Loss: 0.001354 \tValidation Loss: 1.245540\n",
      "Epoch: 4155 \tTraining Loss: 0.000760 \tValidation Loss: 1.250187\n",
      "Epoch: 4156 \tTraining Loss: 0.000825 \tValidation Loss: 1.269469\n",
      "Epoch: 4157 \tTraining Loss: 0.003579 \tValidation Loss: 1.219911\n",
      "Epoch: 4158 \tTraining Loss: 0.001049 \tValidation Loss: 1.187736\n",
      "Epoch: 4159 \tTraining Loss: 0.000874 \tValidation Loss: 1.205910\n",
      "Epoch: 4160 \tTraining Loss: 0.001315 \tValidation Loss: 1.179588\n",
      "Epoch: 4161 \tTraining Loss: 0.001449 \tValidation Loss: 1.179118\n",
      "Epoch: 4162 \tTraining Loss: 0.001059 \tValidation Loss: 1.252420\n",
      "Epoch: 4163 \tTraining Loss: 0.001059 \tValidation Loss: 1.162231\n",
      "Epoch: 4164 \tTraining Loss: 0.000891 \tValidation Loss: 1.281841\n",
      "Epoch: 4165 \tTraining Loss: 0.000603 \tValidation Loss: 1.221416\n",
      "Epoch: 4166 \tTraining Loss: 0.001268 \tValidation Loss: 1.219657\n",
      "Epoch: 4167 \tTraining Loss: 0.000679 \tValidation Loss: 1.222870\n",
      "Epoch: 4168 \tTraining Loss: 0.000584 \tValidation Loss: 1.254027\n",
      "Epoch: 4169 \tTraining Loss: 0.000687 \tValidation Loss: 1.276375\n",
      "Epoch: 4170 \tTraining Loss: 0.001579 \tValidation Loss: 1.216078\n",
      "Epoch: 4171 \tTraining Loss: 0.000517 \tValidation Loss: 1.262438\n",
      "Epoch: 4172 \tTraining Loss: 0.001287 \tValidation Loss: 1.217964\n",
      "Epoch: 4173 \tTraining Loss: 0.000848 \tValidation Loss: 1.262561\n",
      "Epoch: 4174 \tTraining Loss: 0.001307 \tValidation Loss: 1.295015\n",
      "Epoch: 4175 \tTraining Loss: 0.000774 \tValidation Loss: 1.249660\n",
      "Epoch: 4176 \tTraining Loss: 0.002338 \tValidation Loss: 1.187725\n",
      "Epoch: 4177 \tTraining Loss: 0.000567 \tValidation Loss: 1.227353\n",
      "Epoch: 4178 \tTraining Loss: 0.001390 \tValidation Loss: 1.283295\n",
      "Epoch: 4179 \tTraining Loss: 0.000845 \tValidation Loss: 1.194126\n",
      "Epoch: 4180 \tTraining Loss: 0.000988 \tValidation Loss: 1.227583\n",
      "Epoch: 4181 \tTraining Loss: 0.000635 \tValidation Loss: 1.238012\n",
      "Epoch: 4182 \tTraining Loss: 0.000457 \tValidation Loss: 1.323881\n",
      "Epoch: 4183 \tTraining Loss: 0.001728 \tValidation Loss: 1.254940\n",
      "Epoch: 4184 \tTraining Loss: 0.072273 \tValidation Loss: 1.293858\n",
      "Epoch: 4185 \tTraining Loss: 0.098226 \tValidation Loss: 1.059553\n",
      "Epoch: 4186 \tTraining Loss: 0.087998 \tValidation Loss: 1.586192\n",
      "Epoch: 4187 \tTraining Loss: 0.048559 \tValidation Loss: 2.070040\n",
      "Epoch: 4188 \tTraining Loss: 0.054313 \tValidation Loss: 1.420157\n",
      "Epoch: 4189 \tTraining Loss: 0.012245 \tValidation Loss: 1.324202\n",
      "Epoch: 4190 \tTraining Loss: 0.005973 \tValidation Loss: 1.579686\n",
      "Epoch: 4191 \tTraining Loss: 0.005931 \tValidation Loss: 1.548713\n",
      "Epoch: 4192 \tTraining Loss: 0.016486 \tValidation Loss: 1.672579\n",
      "Epoch: 4193 \tTraining Loss: 0.012688 \tValidation Loss: 1.932332\n",
      "Epoch: 4194 \tTraining Loss: 0.006424 \tValidation Loss: 1.883741\n",
      "Epoch: 4195 \tTraining Loss: 0.005787 \tValidation Loss: 1.926095\n",
      "Epoch: 4196 \tTraining Loss: 0.022792 \tValidation Loss: 1.534420\n",
      "Epoch: 4197 \tTraining Loss: 0.008710 \tValidation Loss: 1.530441\n",
      "Epoch: 4198 \tTraining Loss: 0.002763 \tValidation Loss: 1.317224\n",
      "Epoch: 4199 \tTraining Loss: 0.005979 \tValidation Loss: 1.384699\n",
      "Epoch: 4200 \tTraining Loss: 0.003280 \tValidation Loss: 1.473921\n",
      "Epoch: 4201 \tTraining Loss: 0.002453 \tValidation Loss: 1.565684\n",
      "Epoch: 4202 \tTraining Loss: 0.090598 \tValidation Loss: 2.003783\n",
      "Epoch: 4203 \tTraining Loss: 0.721789 \tValidation Loss: 2.810378\n",
      "Epoch: 4204 \tTraining Loss: 0.247144 \tValidation Loss: 1.229651\n",
      "Epoch: 4205 \tTraining Loss: 0.053827 \tValidation Loss: 1.310179\n",
      "Epoch: 4206 \tTraining Loss: 0.036787 \tValidation Loss: 1.420662\n",
      "Epoch: 4207 \tTraining Loss: 0.026359 \tValidation Loss: 1.372682\n",
      "Epoch: 4208 \tTraining Loss: 0.012163 \tValidation Loss: 1.400905\n",
      "Epoch: 4209 \tTraining Loss: 0.014424 \tValidation Loss: 1.600605\n",
      "Epoch: 4210 \tTraining Loss: 0.006589 \tValidation Loss: 1.575536\n",
      "Epoch: 4211 \tTraining Loss: 0.007837 \tValidation Loss: 1.553764\n",
      "Epoch: 4212 \tTraining Loss: 0.010096 \tValidation Loss: 1.465706\n",
      "Epoch: 4213 \tTraining Loss: 0.020774 \tValidation Loss: 2.034021\n",
      "Epoch: 4214 \tTraining Loss: 0.024319 \tValidation Loss: 1.559476\n",
      "Epoch: 4215 \tTraining Loss: 0.084897 \tValidation Loss: 0.889822\n",
      "Epoch: 4216 \tTraining Loss: 0.022506 \tValidation Loss: 1.097845\n",
      "Epoch: 4217 \tTraining Loss: 0.015672 \tValidation Loss: 1.407067\n",
      "Epoch: 4218 \tTraining Loss: 0.007499 \tValidation Loss: 1.431290\n",
      "Epoch: 4219 \tTraining Loss: 0.007803 \tValidation Loss: 1.272803\n",
      "Epoch: 4220 \tTraining Loss: 0.005481 \tValidation Loss: 1.265988\n",
      "Epoch: 4221 \tTraining Loss: 0.006699 \tValidation Loss: 1.284459\n",
      "Epoch: 4222 \tTraining Loss: 0.005027 \tValidation Loss: 1.356949\n",
      "Epoch: 4223 \tTraining Loss: 0.003997 \tValidation Loss: 1.390344\n",
      "Epoch: 4224 \tTraining Loss: 0.005387 \tValidation Loss: 1.338737\n",
      "Epoch: 4225 \tTraining Loss: 0.005889 \tValidation Loss: 1.488079\n",
      "Epoch: 4226 \tTraining Loss: 0.007026 \tValidation Loss: 1.520411\n",
      "Epoch: 4227 \tTraining Loss: 0.003205 \tValidation Loss: 1.615966\n",
      "Epoch: 4228 \tTraining Loss: 0.002828 \tValidation Loss: 1.630632\n",
      "Epoch: 4229 \tTraining Loss: 0.003002 \tValidation Loss: 1.689805\n",
      "Epoch: 4230 \tTraining Loss: 0.002064 \tValidation Loss: 1.525106\n",
      "Epoch: 4231 \tTraining Loss: 0.001817 \tValidation Loss: 1.595278\n",
      "Epoch: 4232 \tTraining Loss: 0.003530 \tValidation Loss: 1.530746\n",
      "Epoch: 4233 \tTraining Loss: 0.002134 \tValidation Loss: 1.637465\n",
      "Epoch: 4234 \tTraining Loss: 0.002386 \tValidation Loss: 1.410346\n",
      "Epoch: 4235 \tTraining Loss: 0.004838 \tValidation Loss: 1.538070\n",
      "Epoch: 4236 \tTraining Loss: 0.001910 \tValidation Loss: 1.549335\n",
      "Epoch: 4237 \tTraining Loss: 0.000995 \tValidation Loss: 1.738921\n",
      "Epoch: 4238 \tTraining Loss: 0.002455 \tValidation Loss: 1.536011\n",
      "Epoch: 4239 \tTraining Loss: 0.001302 \tValidation Loss: 1.580216\n",
      "Epoch: 4240 \tTraining Loss: 0.001600 \tValidation Loss: 1.593639\n",
      "Epoch: 4241 \tTraining Loss: 0.001460 \tValidation Loss: 1.605683\n",
      "Epoch: 4242 \tTraining Loss: 0.004980 \tValidation Loss: 1.628629\n",
      "Epoch: 4243 \tTraining Loss: 0.001990 \tValidation Loss: 1.521401\n",
      "Epoch: 4244 \tTraining Loss: 0.010450 \tValidation Loss: 1.451948\n",
      "Epoch: 4245 \tTraining Loss: 0.004778 \tValidation Loss: 1.387115\n",
      "Epoch: 4246 \tTraining Loss: 0.002691 \tValidation Loss: 1.449582\n",
      "Epoch: 4247 \tTraining Loss: 0.003290 \tValidation Loss: 1.507938\n",
      "Epoch: 4248 \tTraining Loss: 0.001475 \tValidation Loss: 1.570183\n",
      "Epoch: 4249 \tTraining Loss: 0.001211 \tValidation Loss: 1.544085\n",
      "Epoch: 4250 \tTraining Loss: 0.003300 \tValidation Loss: 1.555358\n",
      "Epoch: 4251 \tTraining Loss: 0.001417 \tValidation Loss: 1.546173\n",
      "Epoch: 4252 \tTraining Loss: 0.001281 \tValidation Loss: 1.571725\n",
      "Epoch: 4253 \tTraining Loss: 0.001957 \tValidation Loss: 1.645391\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4254 \tTraining Loss: 0.001528 \tValidation Loss: 1.613871\n",
      "Epoch: 4255 \tTraining Loss: 0.001067 \tValidation Loss: 1.619798\n",
      "Epoch: 4256 \tTraining Loss: 0.001015 \tValidation Loss: 1.600226\n",
      "Epoch: 4257 \tTraining Loss: 0.007757 \tValidation Loss: 1.654683\n",
      "Epoch: 4258 \tTraining Loss: 0.003852 \tValidation Loss: 1.569894\n",
      "Epoch: 4259 \tTraining Loss: 0.001505 \tValidation Loss: 1.594826\n",
      "Epoch: 4260 \tTraining Loss: 0.001453 \tValidation Loss: 1.689901\n",
      "Epoch: 4261 \tTraining Loss: 0.001681 \tValidation Loss: 1.752755\n",
      "Epoch: 4262 \tTraining Loss: 0.001328 \tValidation Loss: 1.700344\n",
      "Epoch: 4263 \tTraining Loss: 0.001064 \tValidation Loss: 1.705297\n",
      "Epoch: 4264 \tTraining Loss: 0.001234 \tValidation Loss: 1.737288\n",
      "Epoch: 4265 \tTraining Loss: 0.001073 \tValidation Loss: 1.711629\n",
      "Epoch: 4266 \tTraining Loss: 0.001955 \tValidation Loss: 1.621857\n",
      "Epoch: 4267 \tTraining Loss: 0.000842 \tValidation Loss: 1.749931\n",
      "Epoch: 4268 \tTraining Loss: 0.001201 \tValidation Loss: 1.794881\n",
      "Epoch: 4269 \tTraining Loss: 0.000991 \tValidation Loss: 1.712893\n",
      "Epoch: 4270 \tTraining Loss: 0.002555 \tValidation Loss: 1.709295\n",
      "Epoch: 4271 \tTraining Loss: 0.000943 \tValidation Loss: 1.741177\n",
      "Epoch: 4272 \tTraining Loss: 0.001025 \tValidation Loss: 1.801052\n",
      "Epoch: 4273 \tTraining Loss: 0.001067 \tValidation Loss: 1.755153\n",
      "Epoch: 4274 \tTraining Loss: 0.003895 \tValidation Loss: 1.752788\n",
      "Epoch: 4275 \tTraining Loss: 0.002315 \tValidation Loss: 1.756776\n",
      "Epoch: 4276 \tTraining Loss: 0.001522 \tValidation Loss: 1.707107\n",
      "Epoch: 4277 \tTraining Loss: 0.001256 \tValidation Loss: 1.751217\n",
      "Epoch: 4278 \tTraining Loss: 0.000851 \tValidation Loss: 1.747732\n",
      "Epoch: 4279 \tTraining Loss: 0.000864 \tValidation Loss: 1.712349\n",
      "Epoch: 4280 \tTraining Loss: 0.001483 \tValidation Loss: 1.667908\n",
      "Epoch: 4281 \tTraining Loss: 0.001666 \tValidation Loss: 1.757747\n",
      "Epoch: 4282 \tTraining Loss: 0.000695 \tValidation Loss: 1.739619\n",
      "Epoch: 4283 \tTraining Loss: 0.000792 \tValidation Loss: 1.744992\n",
      "Epoch: 4284 \tTraining Loss: 0.000613 \tValidation Loss: 1.806655\n",
      "Epoch: 4285 \tTraining Loss: 0.000428 \tValidation Loss: 1.687465\n",
      "Epoch: 4286 \tTraining Loss: 0.002782 \tValidation Loss: 1.766244\n",
      "Epoch: 4287 \tTraining Loss: 0.000967 \tValidation Loss: 1.721635\n",
      "Epoch: 4288 \tTraining Loss: 0.000987 \tValidation Loss: 1.727943\n",
      "Epoch: 4289 \tTraining Loss: 0.001022 \tValidation Loss: 1.721281\n",
      "Epoch: 4290 \tTraining Loss: 0.001708 \tValidation Loss: 1.692303\n",
      "Epoch: 4291 \tTraining Loss: 0.000694 \tValidation Loss: 1.800008\n",
      "Epoch: 4292 \tTraining Loss: 0.000845 \tValidation Loss: 1.783261\n",
      "Epoch: 4293 \tTraining Loss: 0.001102 \tValidation Loss: 1.729061\n",
      "Epoch: 4294 \tTraining Loss: 0.001671 \tValidation Loss: 1.674493\n",
      "Epoch: 4295 \tTraining Loss: 0.000840 \tValidation Loss: 1.781366\n",
      "Epoch: 4296 \tTraining Loss: 0.001275 \tValidation Loss: 1.734229\n",
      "Epoch: 4297 \tTraining Loss: 0.000550 \tValidation Loss: 1.794198\n",
      "Epoch: 4298 \tTraining Loss: 0.000693 \tValidation Loss: 1.729140\n",
      "Epoch: 4299 \tTraining Loss: 0.001030 \tValidation Loss: 1.801271\n",
      "Epoch: 4300 \tTraining Loss: 0.000961 \tValidation Loss: 1.742685\n",
      "Epoch: 4301 \tTraining Loss: 0.002374 \tValidation Loss: 1.683706\n",
      "Epoch: 4302 \tTraining Loss: 0.000740 \tValidation Loss: 1.739419\n",
      "Epoch: 4303 \tTraining Loss: 0.001415 \tValidation Loss: 1.675487\n",
      "Epoch: 4304 \tTraining Loss: 0.000800 \tValidation Loss: 1.752575\n",
      "Epoch: 4305 \tTraining Loss: 0.000508 \tValidation Loss: 1.755406\n",
      "Epoch: 4306 \tTraining Loss: 0.000714 \tValidation Loss: 1.729641\n",
      "Epoch: 4307 \tTraining Loss: 0.001451 \tValidation Loss: 1.561479\n",
      "Epoch: 4308 \tTraining Loss: 0.000813 \tValidation Loss: 1.649288\n",
      "Epoch: 4309 \tTraining Loss: 0.000578 \tValidation Loss: 1.687314\n",
      "Epoch: 4310 \tTraining Loss: 0.001229 \tValidation Loss: 1.694995\n",
      "Epoch: 4311 \tTraining Loss: 0.000500 \tValidation Loss: 1.643002\n",
      "Epoch: 4312 \tTraining Loss: 0.001141 \tValidation Loss: 1.634762\n",
      "Epoch: 4313 \tTraining Loss: 0.000695 \tValidation Loss: 1.713314\n",
      "Epoch: 4314 \tTraining Loss: 0.000848 \tValidation Loss: 1.684278\n",
      "Epoch: 4315 \tTraining Loss: 0.001132 \tValidation Loss: 1.672114\n",
      "Epoch: 4316 \tTraining Loss: 0.000749 \tValidation Loss: 1.674959\n",
      "Epoch: 4317 \tTraining Loss: 0.000446 \tValidation Loss: 1.620464\n",
      "Epoch: 4318 \tTraining Loss: 0.000458 \tValidation Loss: 1.650972\n",
      "Epoch: 4319 \tTraining Loss: 0.001199 \tValidation Loss: 1.651143\n",
      "Epoch: 4320 \tTraining Loss: 0.001749 \tValidation Loss: 1.606379\n",
      "Epoch: 4321 \tTraining Loss: 0.001124 \tValidation Loss: 1.684470\n",
      "Epoch: 4322 \tTraining Loss: 0.001459 \tValidation Loss: 1.714852\n",
      "Epoch: 4323 \tTraining Loss: 0.000661 \tValidation Loss: 1.653689\n",
      "Epoch: 4324 \tTraining Loss: 0.002374 \tValidation Loss: 1.675985\n",
      "Epoch: 4325 \tTraining Loss: 0.000649 \tValidation Loss: 1.706127\n",
      "Epoch: 4326 \tTraining Loss: 0.001083 \tValidation Loss: 1.659623\n",
      "Epoch: 4327 \tTraining Loss: 0.000749 \tValidation Loss: 1.622134\n",
      "Epoch: 4328 \tTraining Loss: 0.001103 \tValidation Loss: 1.641047\n",
      "Epoch: 4329 \tTraining Loss: 0.001199 \tValidation Loss: 1.618612\n",
      "Epoch: 4330 \tTraining Loss: 0.000703 \tValidation Loss: 1.617865\n",
      "Epoch: 4331 \tTraining Loss: 0.000699 \tValidation Loss: 1.626581\n",
      "Epoch: 4332 \tTraining Loss: 0.001467 \tValidation Loss: 1.651037\n",
      "Epoch: 4333 \tTraining Loss: 0.000683 \tValidation Loss: 1.649169\n",
      "Epoch: 4334 \tTraining Loss: 0.000620 \tValidation Loss: 1.672868\n",
      "Epoch: 4335 \tTraining Loss: 0.000738 \tValidation Loss: 1.683265\n",
      "Epoch: 4336 \tTraining Loss: 0.000761 \tValidation Loss: 1.582403\n",
      "Epoch: 4337 \tTraining Loss: 0.001506 \tValidation Loss: 1.592131\n",
      "Epoch: 4338 \tTraining Loss: 0.001842 \tValidation Loss: 1.722875\n",
      "Epoch: 4339 \tTraining Loss: 0.001186 \tValidation Loss: 1.600828\n",
      "Epoch: 4340 \tTraining Loss: 0.000532 \tValidation Loss: 1.585948\n",
      "Epoch: 4341 \tTraining Loss: 0.000785 \tValidation Loss: 1.630999\n",
      "Epoch: 4342 \tTraining Loss: 0.000670 \tValidation Loss: 1.621190\n",
      "Epoch: 4343 \tTraining Loss: 0.001760 \tValidation Loss: 1.578708\n",
      "Epoch: 4344 \tTraining Loss: 0.001193 \tValidation Loss: 1.530305\n",
      "Epoch: 4345 \tTraining Loss: 0.000688 \tValidation Loss: 1.488865\n",
      "Epoch: 4346 \tTraining Loss: 0.000748 \tValidation Loss: 1.598958\n",
      "Epoch: 4347 \tTraining Loss: 0.000451 \tValidation Loss: 1.596394\n",
      "Epoch: 4348 \tTraining Loss: 0.000643 \tValidation Loss: 1.578153\n",
      "Epoch: 4349 \tTraining Loss: 0.000970 \tValidation Loss: 1.646617\n",
      "Epoch: 4350 \tTraining Loss: 0.001781 \tValidation Loss: 1.557986\n",
      "Epoch: 4351 \tTraining Loss: 0.001820 \tValidation Loss: 1.545309\n",
      "Epoch: 4352 \tTraining Loss: 0.001058 \tValidation Loss: 1.477598\n",
      "Epoch: 4353 \tTraining Loss: 0.001293 \tValidation Loss: 1.519221\n",
      "Epoch: 4354 \tTraining Loss: 0.000531 \tValidation Loss: 1.687546\n",
      "Epoch: 4355 \tTraining Loss: 0.001053 \tValidation Loss: 1.545999\n",
      "Epoch: 4356 \tTraining Loss: 0.000798 \tValidation Loss: 1.455238\n",
      "Epoch: 4357 \tTraining Loss: 0.001192 \tValidation Loss: 1.369051\n",
      "Epoch: 4358 \tTraining Loss: 0.000806 \tValidation Loss: 1.376711\n",
      "Epoch: 4359 \tTraining Loss: 0.000986 \tValidation Loss: 1.552559\n",
      "Epoch: 4360 \tTraining Loss: 0.000691 \tValidation Loss: 1.531887\n",
      "Epoch: 4361 \tTraining Loss: 0.001038 \tValidation Loss: 1.462549\n",
      "Epoch: 4362 \tTraining Loss: 0.001128 \tValidation Loss: 1.417129\n",
      "Epoch: 4363 \tTraining Loss: 0.000577 \tValidation Loss: 1.584051\n",
      "Epoch: 4364 \tTraining Loss: 0.002496 \tValidation Loss: 2.029175\n",
      "Epoch: 4365 \tTraining Loss: 0.001588 \tValidation Loss: 1.284860\n",
      "Epoch: 4366 \tTraining Loss: 0.000887 \tValidation Loss: 1.173933\n",
      "Epoch: 4367 \tTraining Loss: 0.011421 \tValidation Loss: 1.395103\n",
      "Epoch: 4368 \tTraining Loss: 0.110303 \tValidation Loss: 1.911456\n",
      "Epoch: 4369 \tTraining Loss: 0.098909 \tValidation Loss: 0.871179\n",
      "Epoch: 4370 \tTraining Loss: 0.102206 \tValidation Loss: 1.848936\n",
      "Epoch: 4371 \tTraining Loss: 0.110507 \tValidation Loss: 1.257508\n",
      "Epoch: 4372 \tTraining Loss: 0.267063 \tValidation Loss: 1.097777\n",
      "Epoch: 4373 \tTraining Loss: 0.161696 \tValidation Loss: 0.729429\n",
      "Epoch: 4374 \tTraining Loss: 0.061262 \tValidation Loss: 1.036714\n",
      "Epoch: 4375 \tTraining Loss: 0.033413 \tValidation Loss: 1.073795\n",
      "Epoch: 4376 \tTraining Loss: 0.027231 \tValidation Loss: 0.973380\n",
      "Epoch: 4377 \tTraining Loss: 0.011715 \tValidation Loss: 0.970920\n",
      "Epoch: 4378 \tTraining Loss: 0.005883 \tValidation Loss: 0.964975\n",
      "Epoch: 4379 \tTraining Loss: 0.009609 \tValidation Loss: 0.984857\n",
      "Epoch: 4380 \tTraining Loss: 0.010147 \tValidation Loss: 1.044234\n",
      "Epoch: 4381 \tTraining Loss: 0.027834 \tValidation Loss: 1.368332\n",
      "Epoch: 4382 \tTraining Loss: 0.091214 \tValidation Loss: 1.197018\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4383 \tTraining Loss: 0.018382 \tValidation Loss: 1.661678\n",
      "Epoch: 4384 \tTraining Loss: 0.008010 \tValidation Loss: 1.750263\n",
      "Epoch: 4385 \tTraining Loss: 0.005778 \tValidation Loss: 1.651591\n",
      "Epoch: 4386 \tTraining Loss: 0.003141 \tValidation Loss: 1.639181\n",
      "Epoch: 4387 \tTraining Loss: 0.008657 \tValidation Loss: 1.303973\n",
      "Epoch: 4388 \tTraining Loss: 0.007910 \tValidation Loss: 1.211027\n",
      "Epoch: 4389 \tTraining Loss: 0.005998 \tValidation Loss: 1.180936\n",
      "Epoch: 4390 \tTraining Loss: 0.009113 \tValidation Loss: 1.189697\n",
      "Epoch: 4391 \tTraining Loss: 0.002452 \tValidation Loss: 1.227568\n",
      "Epoch: 4392 \tTraining Loss: 0.005752 \tValidation Loss: 1.261214\n",
      "Epoch: 4393 \tTraining Loss: 0.002647 \tValidation Loss: 1.302352\n",
      "Epoch: 4394 \tTraining Loss: 0.003482 \tValidation Loss: 1.267770\n",
      "Epoch: 4395 \tTraining Loss: 0.003108 \tValidation Loss: 1.311811\n",
      "Epoch: 4396 \tTraining Loss: 0.001891 \tValidation Loss: 1.266641\n",
      "Epoch: 4397 \tTraining Loss: 0.002468 \tValidation Loss: 1.296638\n",
      "Epoch: 4398 \tTraining Loss: 0.003793 \tValidation Loss: 1.246697\n",
      "Epoch: 4399 \tTraining Loss: 0.000997 \tValidation Loss: 1.267471\n",
      "Epoch: 4400 \tTraining Loss: 0.001557 \tValidation Loss: 1.300014\n",
      "Epoch: 4401 \tTraining Loss: 0.001535 \tValidation Loss: 1.130617\n",
      "Epoch: 4402 \tTraining Loss: 0.002105 \tValidation Loss: 1.233661\n",
      "Epoch: 4403 \tTraining Loss: 0.003175 \tValidation Loss: 1.318745\n",
      "Epoch: 4404 \tTraining Loss: 0.002461 \tValidation Loss: 1.188937\n",
      "Epoch: 4405 \tTraining Loss: 0.001242 \tValidation Loss: 1.317630\n",
      "Epoch: 4406 \tTraining Loss: 0.002581 \tValidation Loss: 1.200004\n",
      "Epoch: 4407 \tTraining Loss: 0.002105 \tValidation Loss: 1.146368\n",
      "Epoch: 4408 \tTraining Loss: 0.001958 \tValidation Loss: 1.200462\n",
      "Epoch: 4409 \tTraining Loss: 0.005144 \tValidation Loss: 1.102760\n",
      "Epoch: 4410 \tTraining Loss: 0.002237 \tValidation Loss: 1.244469\n",
      "Epoch: 4411 \tTraining Loss: 0.001961 \tValidation Loss: 1.341641\n",
      "Epoch: 4412 \tTraining Loss: 0.003009 \tValidation Loss: 1.347106\n",
      "Epoch: 4413 \tTraining Loss: 0.000971 \tValidation Loss: 1.346591\n",
      "Epoch: 4414 \tTraining Loss: 0.000939 \tValidation Loss: 1.312380\n",
      "Epoch: 4415 \tTraining Loss: 0.001038 \tValidation Loss: 1.321245\n",
      "Epoch: 4416 \tTraining Loss: 0.000687 \tValidation Loss: 1.326245\n",
      "Epoch: 4417 \tTraining Loss: 0.013536 \tValidation Loss: 1.254965\n",
      "Epoch: 4418 \tTraining Loss: 0.105737 \tValidation Loss: 1.378779\n",
      "Epoch: 4419 \tTraining Loss: 0.041097 \tValidation Loss: 1.158233\n",
      "Epoch: 4420 \tTraining Loss: 0.019889 \tValidation Loss: 1.315855\n",
      "Epoch: 4421 \tTraining Loss: 0.014309 \tValidation Loss: 1.572416\n",
      "Epoch: 4422 \tTraining Loss: 0.008518 \tValidation Loss: 1.512714\n",
      "Epoch: 4423 \tTraining Loss: 0.156687 \tValidation Loss: 1.352245\n",
      "Epoch: 4424 \tTraining Loss: 0.727924 \tValidation Loss: 2.514930\n",
      "Epoch: 4425 \tTraining Loss: 0.100164 \tValidation Loss: 1.230177\n",
      "Epoch: 4426 \tTraining Loss: 0.062571 \tValidation Loss: 1.832093\n",
      "Epoch: 4427 \tTraining Loss: 0.077907 \tValidation Loss: 1.222038\n",
      "Epoch: 4428 \tTraining Loss: 0.027546 \tValidation Loss: 1.302589\n",
      "Epoch: 4429 \tTraining Loss: 0.028654 \tValidation Loss: 1.045340\n",
      "Epoch: 4430 \tTraining Loss: 0.025082 \tValidation Loss: 1.104151\n",
      "Epoch: 4431 \tTraining Loss: 0.067633 \tValidation Loss: 1.169898\n",
      "Epoch: 4432 \tTraining Loss: 0.030652 \tValidation Loss: 1.352854\n",
      "Epoch: 4433 \tTraining Loss: 0.015379 \tValidation Loss: 1.118593\n",
      "Epoch: 4434 \tTraining Loss: 0.005447 \tValidation Loss: 1.276409\n",
      "Epoch: 4435 \tTraining Loss: 0.010296 \tValidation Loss: 1.287267\n",
      "Epoch: 4436 \tTraining Loss: 0.003861 \tValidation Loss: 1.265775\n",
      "Epoch: 4437 \tTraining Loss: 0.004624 \tValidation Loss: 1.226186\n",
      "Epoch: 4438 \tTraining Loss: 0.007027 \tValidation Loss: 1.217486\n",
      "Epoch: 4439 \tTraining Loss: 0.004094 \tValidation Loss: 1.277758\n",
      "Epoch: 4440 \tTraining Loss: 0.003849 \tValidation Loss: 1.172434\n",
      "Epoch: 4441 \tTraining Loss: 0.002078 \tValidation Loss: 1.258340\n",
      "Epoch: 4442 \tTraining Loss: 0.006707 \tValidation Loss: 1.268755\n",
      "Epoch: 4443 \tTraining Loss: 0.003963 \tValidation Loss: 1.306693\n",
      "Epoch: 4444 \tTraining Loss: 0.006534 \tValidation Loss: 1.375043\n",
      "Epoch: 4445 \tTraining Loss: 0.003157 \tValidation Loss: 1.390006\n",
      "Epoch: 4446 \tTraining Loss: 0.002393 \tValidation Loss: 1.353143\n",
      "Epoch: 4447 \tTraining Loss: 0.005249 \tValidation Loss: 1.316567\n",
      "Epoch: 4448 \tTraining Loss: 0.003204 \tValidation Loss: 1.312642\n",
      "Epoch: 4449 \tTraining Loss: 0.004254 \tValidation Loss: 1.183957\n",
      "Epoch: 4450 \tTraining Loss: 0.004302 \tValidation Loss: 1.243068\n",
      "Epoch: 4451 \tTraining Loss: 0.006352 \tValidation Loss: 1.296229\n",
      "Epoch: 4452 \tTraining Loss: 0.002306 \tValidation Loss: 1.201353\n",
      "Epoch: 4453 \tTraining Loss: 0.003399 \tValidation Loss: 1.211870\n",
      "Epoch: 4454 \tTraining Loss: 0.002164 \tValidation Loss: 1.206602\n",
      "Epoch: 4455 \tTraining Loss: 0.002246 \tValidation Loss: 1.224399\n",
      "Epoch: 4456 \tTraining Loss: 0.001429 \tValidation Loss: 1.192983\n",
      "Epoch: 4457 \tTraining Loss: 0.001340 \tValidation Loss: 1.266581\n",
      "Epoch: 4458 \tTraining Loss: 0.002820 \tValidation Loss: 1.276710\n",
      "Epoch: 4459 \tTraining Loss: 0.003345 \tValidation Loss: 1.340504\n",
      "Epoch: 4460 \tTraining Loss: 0.002038 \tValidation Loss: 1.484600\n",
      "Epoch: 4461 \tTraining Loss: 0.041027 \tValidation Loss: 1.592897\n",
      "Epoch: 4462 \tTraining Loss: 0.073845 \tValidation Loss: 1.740145\n",
      "Epoch: 4463 \tTraining Loss: 0.116215 \tValidation Loss: 2.152450\n",
      "Epoch: 4464 \tTraining Loss: 0.492388 \tValidation Loss: 1.957772\n",
      "Epoch: 4465 \tTraining Loss: 0.266329 \tValidation Loss: 1.026851\n",
      "Epoch: 4466 \tTraining Loss: 0.082317 \tValidation Loss: 1.202178\n",
      "Epoch: 4467 \tTraining Loss: 0.051119 \tValidation Loss: 1.268144\n",
      "Epoch: 4468 \tTraining Loss: 0.130587 \tValidation Loss: 1.478558\n",
      "Epoch: 4469 \tTraining Loss: 0.124308 \tValidation Loss: 0.887328\n",
      "Epoch: 4470 \tTraining Loss: 0.123753 \tValidation Loss: 1.101877\n",
      "Epoch: 4471 \tTraining Loss: 0.083439 \tValidation Loss: 1.026511\n",
      "Epoch: 4472 \tTraining Loss: 0.078429 \tValidation Loss: 0.940601\n",
      "Epoch: 4473 \tTraining Loss: 0.049196 \tValidation Loss: 0.937098\n",
      "Epoch: 4474 \tTraining Loss: 0.044534 \tValidation Loss: 0.733782\n",
      "Epoch: 4475 \tTraining Loss: 0.014077 \tValidation Loss: 0.779707\n",
      "Epoch: 4476 \tTraining Loss: 0.018868 \tValidation Loss: 0.875389\n",
      "Epoch: 4477 \tTraining Loss: 0.012139 \tValidation Loss: 0.970125\n",
      "Epoch: 4478 \tTraining Loss: 0.022649 \tValidation Loss: 1.289820\n",
      "Epoch: 4479 \tTraining Loss: 0.011688 \tValidation Loss: 1.220226\n",
      "Epoch: 4480 \tTraining Loss: 0.015528 \tValidation Loss: 1.127133\n",
      "Epoch: 4481 \tTraining Loss: 0.027893 \tValidation Loss: 0.795435\n",
      "Epoch: 4482 \tTraining Loss: 0.012740 \tValidation Loss: 0.826973\n",
      "Epoch: 4483 \tTraining Loss: 0.029061 \tValidation Loss: 0.557327\n",
      "Epoch: 4484 \tTraining Loss: 0.031341 \tValidation Loss: 0.554900\n",
      "Epoch: 4485 \tTraining Loss: 0.004055 \tValidation Loss: 0.565784\n",
      "Epoch: 4486 \tTraining Loss: 0.003330 \tValidation Loss: 0.599685\n",
      "Epoch: 4487 \tTraining Loss: 0.015477 \tValidation Loss: 0.564591\n",
      "Epoch: 4488 \tTraining Loss: 0.004967 \tValidation Loss: 0.663572\n",
      "Epoch: 4489 \tTraining Loss: 0.006447 \tValidation Loss: 0.663736\n",
      "Epoch: 4490 \tTraining Loss: 0.004226 \tValidation Loss: 0.689836\n",
      "Epoch: 4491 \tTraining Loss: 0.003202 \tValidation Loss: 0.697260\n",
      "Epoch: 4492 \tTraining Loss: 0.002149 \tValidation Loss: 0.691487\n",
      "Epoch: 4493 \tTraining Loss: 0.003882 \tValidation Loss: 0.663931\n",
      "Epoch: 4494 \tTraining Loss: 0.009098 \tValidation Loss: 0.684128\n",
      "Epoch: 4495 \tTraining Loss: 0.004458 \tValidation Loss: 0.711279\n",
      "Epoch: 4496 \tTraining Loss: 0.002466 \tValidation Loss: 0.716666\n",
      "Epoch: 4497 \tTraining Loss: 0.005932 \tValidation Loss: 0.692729\n",
      "Epoch: 4498 \tTraining Loss: 0.002082 \tValidation Loss: 0.659605\n",
      "Epoch: 4499 \tTraining Loss: 0.002959 \tValidation Loss: 0.657145\n",
      "Epoch: 4500 \tTraining Loss: 0.002431 \tValidation Loss: 0.677373\n",
      "Epoch: 4501 \tTraining Loss: 0.003805 \tValidation Loss: 0.647603\n",
      "Epoch: 4502 \tTraining Loss: 0.002351 \tValidation Loss: 0.733872\n",
      "Epoch: 4503 \tTraining Loss: 0.002545 \tValidation Loss: 0.655016\n",
      "Epoch: 4504 \tTraining Loss: 0.002256 \tValidation Loss: 0.787118\n",
      "Epoch: 4505 \tTraining Loss: 0.001407 \tValidation Loss: 0.680731\n",
      "Epoch: 4506 \tTraining Loss: 0.001907 \tValidation Loss: 0.635542\n",
      "Epoch: 4507 \tTraining Loss: 0.003784 \tValidation Loss: 0.619863\n",
      "Epoch: 4508 \tTraining Loss: 0.003233 \tValidation Loss: 0.688606\n",
      "Epoch: 4509 \tTraining Loss: 0.002490 \tValidation Loss: 0.735183\n",
      "Epoch: 4510 \tTraining Loss: 0.004843 \tValidation Loss: 0.688151\n",
      "Epoch: 4511 \tTraining Loss: 0.003434 \tValidation Loss: 0.652715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4512 \tTraining Loss: 0.001179 \tValidation Loss: 0.673213\n",
      "Epoch: 4513 \tTraining Loss: 0.003095 \tValidation Loss: 0.703778\n",
      "Epoch: 4514 \tTraining Loss: 0.001636 \tValidation Loss: 0.678710\n",
      "Epoch: 4515 \tTraining Loss: 0.001856 \tValidation Loss: 0.706936\n",
      "Epoch: 4516 \tTraining Loss: 0.002916 \tValidation Loss: 0.697243\n",
      "Epoch: 4517 \tTraining Loss: 0.010878 \tValidation Loss: 0.663562\n",
      "Epoch: 4518 \tTraining Loss: 0.008584 \tValidation Loss: 0.943699\n",
      "Epoch: 4519 \tTraining Loss: 0.003641 \tValidation Loss: 0.795790\n",
      "Epoch: 4520 \tTraining Loss: 0.002116 \tValidation Loss: 0.846044\n",
      "Epoch: 4521 \tTraining Loss: 0.001340 \tValidation Loss: 0.872108\n",
      "Epoch: 4522 \tTraining Loss: 0.001672 \tValidation Loss: 0.870539\n",
      "Epoch: 4523 \tTraining Loss: 0.001601 \tValidation Loss: 0.907965\n",
      "Epoch: 4524 \tTraining Loss: 0.001056 \tValidation Loss: 0.787626\n",
      "Epoch: 4525 \tTraining Loss: 0.001146 \tValidation Loss: 0.907920\n",
      "Epoch: 4526 \tTraining Loss: 0.002722 \tValidation Loss: 0.894530\n",
      "Epoch: 4527 \tTraining Loss: 0.001541 \tValidation Loss: 0.843981\n",
      "Epoch: 4528 \tTraining Loss: 0.001119 \tValidation Loss: 0.850527\n",
      "Epoch: 4529 \tTraining Loss: 0.002472 \tValidation Loss: 0.842393\n",
      "Epoch: 4530 \tTraining Loss: 0.002579 \tValidation Loss: 0.785553\n",
      "Epoch: 4531 \tTraining Loss: 0.001625 \tValidation Loss: 0.790757\n",
      "Epoch: 4532 \tTraining Loss: 0.004638 \tValidation Loss: 0.835846\n",
      "Epoch: 4533 \tTraining Loss: 0.001740 \tValidation Loss: 0.733454\n",
      "Epoch: 4534 \tTraining Loss: 0.002962 \tValidation Loss: 0.734943\n",
      "Epoch: 4535 \tTraining Loss: 0.001518 \tValidation Loss: 0.823323\n",
      "Epoch: 4536 \tTraining Loss: 0.001189 \tValidation Loss: 0.857882\n",
      "Epoch: 4537 \tTraining Loss: 0.003180 \tValidation Loss: 0.815185\n",
      "Epoch: 4538 \tTraining Loss: 0.001168 \tValidation Loss: 0.847153\n",
      "Epoch: 4539 \tTraining Loss: 0.001062 \tValidation Loss: 0.770774\n",
      "Epoch: 4540 \tTraining Loss: 0.001582 \tValidation Loss: 0.814234\n",
      "Epoch: 4541 \tTraining Loss: 0.001343 \tValidation Loss: 0.728387\n",
      "Epoch: 4542 \tTraining Loss: 0.001079 \tValidation Loss: 0.848960\n",
      "Epoch: 4543 \tTraining Loss: 0.000854 \tValidation Loss: 0.794220\n",
      "Epoch: 4544 \tTraining Loss: 0.001853 \tValidation Loss: 0.814769\n",
      "Epoch: 4545 \tTraining Loss: 0.000726 \tValidation Loss: 0.815951\n",
      "Epoch: 4546 \tTraining Loss: 0.001814 \tValidation Loss: 0.790901\n",
      "Epoch: 4547 \tTraining Loss: 0.001040 \tValidation Loss: 0.793604\n",
      "Epoch: 4548 \tTraining Loss: 0.001977 \tValidation Loss: 0.801983\n",
      "Epoch: 4549 \tTraining Loss: 0.000818 \tValidation Loss: 0.870094\n",
      "Epoch: 4550 \tTraining Loss: 0.000900 \tValidation Loss: 0.864149\n",
      "Epoch: 4551 \tTraining Loss: 0.001257 \tValidation Loss: 0.770516\n",
      "Epoch: 4552 \tTraining Loss: 0.000982 \tValidation Loss: 0.791752\n",
      "Epoch: 4553 \tTraining Loss: 0.002482 \tValidation Loss: 0.719868\n",
      "Epoch: 4554 \tTraining Loss: 0.001359 \tValidation Loss: 0.781106\n",
      "Epoch: 4555 \tTraining Loss: 0.001248 \tValidation Loss: 0.750040\n",
      "Epoch: 4556 \tTraining Loss: 0.001281 \tValidation Loss: 0.831973\n",
      "Epoch: 4557 \tTraining Loss: 0.000887 \tValidation Loss: 1.072253\n",
      "Epoch: 4558 \tTraining Loss: 0.000655 \tValidation Loss: 0.829071\n",
      "Epoch: 4559 \tTraining Loss: 0.000733 \tValidation Loss: 0.833589\n",
      "Epoch: 4560 \tTraining Loss: 0.000949 \tValidation Loss: 0.803332\n",
      "Epoch: 4561 \tTraining Loss: 0.000716 \tValidation Loss: 0.873115\n",
      "Epoch: 4562 \tTraining Loss: 0.002022 \tValidation Loss: 0.773406\n",
      "Epoch: 4563 \tTraining Loss: 0.001009 \tValidation Loss: 1.083253\n",
      "Epoch: 4564 \tTraining Loss: 0.001691 \tValidation Loss: 1.035494\n",
      "Epoch: 4565 \tTraining Loss: 0.001238 \tValidation Loss: 0.990784\n",
      "Epoch: 4566 \tTraining Loss: 0.001192 \tValidation Loss: 1.027697\n",
      "Epoch: 4567 \tTraining Loss: 0.000922 \tValidation Loss: 0.902313\n",
      "Epoch: 4568 \tTraining Loss: 0.000856 \tValidation Loss: 0.937284\n",
      "Epoch: 4569 \tTraining Loss: 0.002693 \tValidation Loss: 0.997694\n",
      "Epoch: 4570 \tTraining Loss: 0.000930 \tValidation Loss: 1.078070\n",
      "Epoch: 4571 \tTraining Loss: 0.001139 \tValidation Loss: 1.322570\n",
      "Epoch: 4572 \tTraining Loss: 0.000767 \tValidation Loss: 1.088071\n",
      "Epoch: 4573 \tTraining Loss: 0.000800 \tValidation Loss: 1.043814\n",
      "Epoch: 4574 \tTraining Loss: 0.002879 \tValidation Loss: 1.023021\n",
      "Epoch: 4575 \tTraining Loss: 0.003098 \tValidation Loss: 0.973500\n",
      "Epoch: 4576 \tTraining Loss: 0.001111 \tValidation Loss: 1.024972\n",
      "Epoch: 4577 \tTraining Loss: 0.002027 \tValidation Loss: 1.130091\n",
      "Epoch: 4578 \tTraining Loss: 0.001804 \tValidation Loss: 1.035794\n",
      "Epoch: 4579 \tTraining Loss: 0.001269 \tValidation Loss: 0.949532\n",
      "Epoch: 4580 \tTraining Loss: 0.002767 \tValidation Loss: 0.896624\n",
      "Epoch: 4581 \tTraining Loss: 0.001120 \tValidation Loss: 0.785333\n",
      "Epoch: 4582 \tTraining Loss: 0.010312 \tValidation Loss: 0.727835\n",
      "Epoch: 4583 \tTraining Loss: 0.028258 \tValidation Loss: 1.002399\n",
      "Epoch: 4584 \tTraining Loss: 0.013072 \tValidation Loss: 1.003286\n",
      "Epoch: 4585 \tTraining Loss: 0.004347 \tValidation Loss: 1.165710\n",
      "Epoch: 4586 \tTraining Loss: 0.010386 \tValidation Loss: 1.218828\n",
      "Epoch: 4587 \tTraining Loss: 0.027384 \tValidation Loss: 1.183147\n",
      "Epoch: 4588 \tTraining Loss: 0.165433 \tValidation Loss: 0.821159\n",
      "Epoch: 4589 \tTraining Loss: 0.212899 \tValidation Loss: 4.078535\n",
      "Epoch: 4590 \tTraining Loss: 0.215918 \tValidation Loss: 1.502899\n",
      "Epoch: 4591 \tTraining Loss: 0.052657 \tValidation Loss: 1.174034\n",
      "Epoch: 4592 \tTraining Loss: 0.031818 \tValidation Loss: 0.984142\n",
      "Epoch: 4593 \tTraining Loss: 0.015490 \tValidation Loss: 1.053358\n",
      "Epoch: 4594 \tTraining Loss: 0.010839 \tValidation Loss: 1.232647\n",
      "Epoch: 4595 \tTraining Loss: 0.020469 \tValidation Loss: 1.495105\n",
      "Epoch: 4596 \tTraining Loss: 0.007735 \tValidation Loss: 1.132565\n",
      "Epoch: 4597 \tTraining Loss: 0.012866 \tValidation Loss: 0.969326\n",
      "Epoch: 4598 \tTraining Loss: 0.018168 \tValidation Loss: 1.120632\n",
      "Epoch: 4599 \tTraining Loss: 0.017393 \tValidation Loss: 0.992574\n",
      "Epoch: 4600 \tTraining Loss: 0.019852 \tValidation Loss: 1.051983\n",
      "Epoch: 4601 \tTraining Loss: 0.040471 \tValidation Loss: 0.912261\n",
      "Epoch: 4602 \tTraining Loss: 0.041250 \tValidation Loss: 0.925736\n",
      "Epoch: 4603 \tTraining Loss: 0.082757 \tValidation Loss: 1.431786\n",
      "Epoch: 4604 \tTraining Loss: 0.045306 \tValidation Loss: 1.204324\n",
      "Epoch: 4605 \tTraining Loss: 0.048390 \tValidation Loss: 1.174542\n",
      "Epoch: 4606 \tTraining Loss: 0.152959 \tValidation Loss: 1.119504\n",
      "Epoch: 4607 \tTraining Loss: 0.152778 \tValidation Loss: 1.084770\n",
      "Epoch: 4608 \tTraining Loss: 0.080798 \tValidation Loss: 1.086725\n",
      "Epoch: 4609 \tTraining Loss: 0.013554 \tValidation Loss: 1.050304\n",
      "Epoch: 4610 \tTraining Loss: 0.008450 \tValidation Loss: 1.113465\n",
      "Epoch: 4611 \tTraining Loss: 0.011248 \tValidation Loss: 1.124733\n",
      "Epoch: 4612 \tTraining Loss: 0.007161 \tValidation Loss: 1.170882\n",
      "Epoch: 4613 \tTraining Loss: 0.015583 \tValidation Loss: 1.279421\n",
      "Epoch: 4614 \tTraining Loss: 0.005644 \tValidation Loss: 1.161109\n",
      "Epoch: 4615 \tTraining Loss: 0.007088 \tValidation Loss: 1.272632\n",
      "Epoch: 4616 \tTraining Loss: 0.062832 \tValidation Loss: 1.566126\n",
      "Epoch: 4617 \tTraining Loss: 0.262492 \tValidation Loss: 1.558800\n",
      "Epoch: 4618 \tTraining Loss: 0.122857 \tValidation Loss: 1.061384\n",
      "Epoch: 4619 \tTraining Loss: 0.033641 \tValidation Loss: 1.023232\n",
      "Epoch: 4620 \tTraining Loss: 0.013273 \tValidation Loss: 1.104787\n",
      "Epoch: 4621 \tTraining Loss: 0.014175 \tValidation Loss: 1.057690\n",
      "Epoch: 4622 \tTraining Loss: 0.019588 \tValidation Loss: 1.029793\n",
      "Epoch: 4623 \tTraining Loss: 0.023612 \tValidation Loss: 0.750745\n",
      "Epoch: 4624 \tTraining Loss: 0.006850 \tValidation Loss: 0.863312\n",
      "Epoch: 4625 \tTraining Loss: 0.016128 \tValidation Loss: 0.851006\n",
      "Epoch: 4626 \tTraining Loss: 0.007153 \tValidation Loss: 0.731198\n",
      "Epoch: 4627 \tTraining Loss: 0.017596 \tValidation Loss: 0.696349\n",
      "Epoch: 4628 \tTraining Loss: 0.146743 \tValidation Loss: 0.885191\n",
      "Epoch: 4629 \tTraining Loss: 0.060364 \tValidation Loss: 1.014313\n",
      "Epoch: 4630 \tTraining Loss: 0.045519 \tValidation Loss: 0.951703\n",
      "Epoch: 4631 \tTraining Loss: 0.010727 \tValidation Loss: 0.489959\n",
      "Epoch: 4632 \tTraining Loss: 0.006839 \tValidation Loss: 0.488042\n",
      "Epoch: 4633 \tTraining Loss: 0.009457 \tValidation Loss: 0.579109\n",
      "Epoch: 4634 \tTraining Loss: 0.018318 \tValidation Loss: 0.778527\n",
      "Epoch: 4635 \tTraining Loss: 0.004593 \tValidation Loss: 0.719457\n",
      "Epoch: 4636 \tTraining Loss: 0.007971 \tValidation Loss: 0.691106\n",
      "Epoch: 4637 \tTraining Loss: 0.009362 \tValidation Loss: 0.687360\n",
      "Epoch: 4638 \tTraining Loss: 0.009800 \tValidation Loss: 0.639973\n",
      "Epoch: 4639 \tTraining Loss: 0.006103 \tValidation Loss: 0.598761\n",
      "Epoch: 4640 \tTraining Loss: 0.008867 \tValidation Loss: 0.593810\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4641 \tTraining Loss: 0.002985 \tValidation Loss: 0.612513\n",
      "Epoch: 4642 \tTraining Loss: 0.002531 \tValidation Loss: 0.625415\n",
      "Epoch: 4643 \tTraining Loss: 0.024724 \tValidation Loss: 0.638819\n",
      "Epoch: 4644 \tTraining Loss: 0.003011 \tValidation Loss: 0.589927\n",
      "Epoch: 4645 \tTraining Loss: 0.003062 \tValidation Loss: 0.589381\n",
      "Epoch: 4646 \tTraining Loss: 0.002661 \tValidation Loss: 0.587713\n",
      "Epoch: 4647 \tTraining Loss: 0.009523 \tValidation Loss: 0.707986\n",
      "Epoch: 4648 \tTraining Loss: 0.004092 \tValidation Loss: 0.738467\n",
      "Epoch: 4649 \tTraining Loss: 0.006165 \tValidation Loss: 0.675990\n",
      "Epoch: 4650 \tTraining Loss: 0.005405 \tValidation Loss: 0.678367\n",
      "Epoch: 4651 \tTraining Loss: 0.007769 \tValidation Loss: 0.699827\n",
      "Epoch: 4652 \tTraining Loss: 0.001778 \tValidation Loss: 0.762115\n",
      "Epoch: 4653 \tTraining Loss: 0.003812 \tValidation Loss: 0.752372\n",
      "Epoch: 4654 \tTraining Loss: 0.003547 \tValidation Loss: 0.733273\n",
      "Epoch: 4655 \tTraining Loss: 0.001961 \tValidation Loss: 0.737327\n",
      "Epoch: 4656 \tTraining Loss: 0.001298 \tValidation Loss: 0.736661\n",
      "Epoch: 4657 \tTraining Loss: 0.001479 \tValidation Loss: 0.762266\n",
      "Epoch: 4658 \tTraining Loss: 0.004968 \tValidation Loss: 0.747308\n",
      "Epoch: 4659 \tTraining Loss: 0.003139 \tValidation Loss: 0.732441\n",
      "Epoch: 4660 \tTraining Loss: 0.001820 \tValidation Loss: 0.804575\n",
      "Epoch: 4661 \tTraining Loss: 0.001509 \tValidation Loss: 0.754775\n",
      "Epoch: 4662 \tTraining Loss: 0.002951 \tValidation Loss: 0.723822\n",
      "Epoch: 4663 \tTraining Loss: 0.002427 \tValidation Loss: 0.716431\n",
      "Epoch: 4664 \tTraining Loss: 0.000844 \tValidation Loss: 0.709094\n",
      "Epoch: 4665 \tTraining Loss: 0.003549 \tValidation Loss: 0.716983\n",
      "Epoch: 4666 \tTraining Loss: 0.001238 \tValidation Loss: 0.684552\n",
      "Epoch: 4667 \tTraining Loss: 0.002784 \tValidation Loss: 0.709937\n",
      "Epoch: 4668 \tTraining Loss: 0.002054 \tValidation Loss: 0.688884\n",
      "Epoch: 4669 \tTraining Loss: 0.001314 \tValidation Loss: 0.726175\n",
      "Epoch: 4670 \tTraining Loss: 0.002028 \tValidation Loss: 0.746982\n",
      "Epoch: 4671 \tTraining Loss: 0.001288 \tValidation Loss: 0.649263\n",
      "Epoch: 4672 \tTraining Loss: 0.001111 \tValidation Loss: 0.682339\n",
      "Epoch: 4673 \tTraining Loss: 0.001364 \tValidation Loss: 0.777003\n",
      "Epoch: 4674 \tTraining Loss: 0.001303 \tValidation Loss: 0.722655\n",
      "Epoch: 4675 \tTraining Loss: 0.002084 \tValidation Loss: 0.689890\n",
      "Epoch: 4676 \tTraining Loss: 0.001864 \tValidation Loss: 0.712781\n",
      "Epoch: 4677 \tTraining Loss: 0.001044 \tValidation Loss: 0.687539\n",
      "Epoch: 4678 \tTraining Loss: 0.001332 \tValidation Loss: 0.719681\n",
      "Epoch: 4679 \tTraining Loss: 0.000806 \tValidation Loss: 0.680119\n",
      "Epoch: 4680 \tTraining Loss: 0.000979 \tValidation Loss: 0.696974\n",
      "Epoch: 4681 \tTraining Loss: 0.004945 \tValidation Loss: 0.767437\n",
      "Epoch: 4682 \tTraining Loss: 0.001193 \tValidation Loss: 0.764152\n",
      "Epoch: 4683 \tTraining Loss: 0.000987 \tValidation Loss: 0.800780\n",
      "Epoch: 4684 \tTraining Loss: 0.002159 \tValidation Loss: 0.792924\n",
      "Epoch: 4685 \tTraining Loss: 0.002318 \tValidation Loss: 0.795146\n",
      "Epoch: 4686 \tTraining Loss: 0.001553 \tValidation Loss: 0.740985\n",
      "Epoch: 4687 \tTraining Loss: 0.003418 \tValidation Loss: 0.639572\n",
      "Epoch: 4688 \tTraining Loss: 0.002288 \tValidation Loss: 0.730450\n",
      "Epoch: 4689 \tTraining Loss: 0.035884 \tValidation Loss: 0.950202\n",
      "Epoch: 4690 \tTraining Loss: 0.145412 \tValidation Loss: 0.968086\n",
      "Epoch: 4691 \tTraining Loss: 0.011228 \tValidation Loss: 0.742098\n",
      "Epoch: 4692 \tTraining Loss: 0.023642 \tValidation Loss: 0.769438\n",
      "Epoch: 4693 \tTraining Loss: 0.024612 \tValidation Loss: 0.898610\n",
      "Epoch: 4694 \tTraining Loss: 0.007557 \tValidation Loss: 0.807224\n",
      "Epoch: 4695 \tTraining Loss: 0.013017 \tValidation Loss: 0.770490\n",
      "Epoch: 4696 \tTraining Loss: 0.010728 \tValidation Loss: 0.856091\n",
      "Epoch: 4697 \tTraining Loss: 0.047575 \tValidation Loss: 1.139860\n",
      "Epoch: 4698 \tTraining Loss: 0.055894 \tValidation Loss: 1.681681\n",
      "Epoch: 4699 \tTraining Loss: 0.017474 \tValidation Loss: 1.069162\n",
      "Epoch: 4700 \tTraining Loss: 0.017120 \tValidation Loss: 0.770660\n",
      "Epoch: 4701 \tTraining Loss: 0.019141 \tValidation Loss: 1.143761\n",
      "Epoch: 4702 \tTraining Loss: 0.082936 \tValidation Loss: 1.091677\n",
      "Epoch: 4703 \tTraining Loss: 0.056656 \tValidation Loss: 1.052035\n",
      "Epoch: 4704 \tTraining Loss: 0.022333 \tValidation Loss: 1.006413\n",
      "Epoch: 4705 \tTraining Loss: 0.003150 \tValidation Loss: 1.064682\n",
      "Epoch: 4706 \tTraining Loss: 0.007903 \tValidation Loss: 0.937267\n",
      "Epoch: 4707 \tTraining Loss: 0.002549 \tValidation Loss: 0.914718\n",
      "Epoch: 4708 \tTraining Loss: 0.005044 \tValidation Loss: 0.975429\n",
      "Epoch: 4709 \tTraining Loss: 0.003860 \tValidation Loss: 1.000425\n",
      "Epoch: 4710 \tTraining Loss: 0.003424 \tValidation Loss: 0.980279\n",
      "Epoch: 4711 \tTraining Loss: 0.002371 \tValidation Loss: 1.012888\n",
      "Epoch: 4712 \tTraining Loss: 0.002176 \tValidation Loss: 0.889170\n",
      "Epoch: 4713 \tTraining Loss: 0.004031 \tValidation Loss: 0.936281\n",
      "Epoch: 4714 \tTraining Loss: 0.002483 \tValidation Loss: 0.905389\n",
      "Epoch: 4715 \tTraining Loss: 0.002287 \tValidation Loss: 0.866582\n",
      "Epoch: 4716 \tTraining Loss: 0.002114 \tValidation Loss: 0.926865\n",
      "Epoch: 4717 \tTraining Loss: 0.001142 \tValidation Loss: 0.988478\n",
      "Epoch: 4718 \tTraining Loss: 0.005200 \tValidation Loss: 0.966841\n",
      "Epoch: 4719 \tTraining Loss: 0.004775 \tValidation Loss: 0.954740\n",
      "Epoch: 4720 \tTraining Loss: 0.070926 \tValidation Loss: 0.345163\n",
      "Validation loss decreased (0.478779 --> 0.345163).  Saving model ...\n",
      "Epoch: 4721 \tTraining Loss: 0.009013 \tValidation Loss: 0.552321\n",
      "Epoch: 4722 \tTraining Loss: 0.035304 \tValidation Loss: 0.572925\n",
      "Epoch: 4723 \tTraining Loss: 0.031363 \tValidation Loss: 1.140242\n",
      "Epoch: 4724 \tTraining Loss: 0.022644 \tValidation Loss: 1.207379\n",
      "Epoch: 4725 \tTraining Loss: 0.008602 \tValidation Loss: 1.261600\n",
      "Epoch: 4726 \tTraining Loss: 0.004068 \tValidation Loss: 0.976327\n",
      "Epoch: 4727 \tTraining Loss: 0.005710 \tValidation Loss: 1.120382\n",
      "Epoch: 4728 \tTraining Loss: 0.002121 \tValidation Loss: 1.183407\n",
      "Epoch: 4729 \tTraining Loss: 0.004529 \tValidation Loss: 1.113498\n",
      "Epoch: 4730 \tTraining Loss: 0.013566 \tValidation Loss: 1.081257\n",
      "Epoch: 4731 \tTraining Loss: 0.001491 \tValidation Loss: 1.115017\n",
      "Epoch: 4732 \tTraining Loss: 0.002315 \tValidation Loss: 1.112442\n",
      "Epoch: 4733 \tTraining Loss: 0.004545 \tValidation Loss: 1.127989\n",
      "Epoch: 4734 \tTraining Loss: 0.001884 \tValidation Loss: 1.067225\n",
      "Epoch: 4735 \tTraining Loss: 0.001521 \tValidation Loss: 1.080104\n",
      "Epoch: 4736 \tTraining Loss: 0.001862 \tValidation Loss: 1.105688\n",
      "Epoch: 4737 \tTraining Loss: 0.006232 \tValidation Loss: 1.076961\n",
      "Epoch: 4738 \tTraining Loss: 0.001121 \tValidation Loss: 1.145541\n",
      "Epoch: 4739 \tTraining Loss: 0.003685 \tValidation Loss: 0.981650\n",
      "Epoch: 4740 \tTraining Loss: 0.001692 \tValidation Loss: 1.072448\n",
      "Epoch: 4741 \tTraining Loss: 0.001559 \tValidation Loss: 1.197846\n",
      "Epoch: 4742 \tTraining Loss: 0.002870 \tValidation Loss: 1.041853\n",
      "Epoch: 4743 \tTraining Loss: 0.002245 \tValidation Loss: 1.105701\n",
      "Epoch: 4744 \tTraining Loss: 0.005610 \tValidation Loss: 0.952747\n",
      "Epoch: 4745 \tTraining Loss: 0.003310 \tValidation Loss: 1.117634\n",
      "Epoch: 4746 \tTraining Loss: 0.003482 \tValidation Loss: 0.983822\n",
      "Epoch: 4747 \tTraining Loss: 0.002093 \tValidation Loss: 0.927691\n",
      "Epoch: 4748 \tTraining Loss: 0.001777 \tValidation Loss: 0.991800\n",
      "Epoch: 4749 \tTraining Loss: 0.001384 \tValidation Loss: 1.114245\n",
      "Epoch: 4750 \tTraining Loss: 0.001727 \tValidation Loss: 1.018897\n",
      "Epoch: 4751 \tTraining Loss: 0.001366 \tValidation Loss: 0.984159\n",
      "Epoch: 4752 \tTraining Loss: 0.001482 \tValidation Loss: 1.166542\n",
      "Epoch: 4753 \tTraining Loss: 0.001691 \tValidation Loss: 1.192722\n",
      "Epoch: 4754 \tTraining Loss: 0.000776 \tValidation Loss: 1.159502\n",
      "Epoch: 4755 \tTraining Loss: 0.000805 \tValidation Loss: 1.080356\n",
      "Epoch: 4756 \tTraining Loss: 0.000930 \tValidation Loss: 1.120097\n",
      "Epoch: 4757 \tTraining Loss: 0.000969 \tValidation Loss: 1.067086\n",
      "Epoch: 4758 \tTraining Loss: 0.000858 \tValidation Loss: 1.181621\n",
      "Epoch: 4759 \tTraining Loss: 0.001868 \tValidation Loss: 1.095739\n",
      "Epoch: 4760 \tTraining Loss: 0.000989 \tValidation Loss: 1.253333\n",
      "Epoch: 4761 \tTraining Loss: 0.001045 \tValidation Loss: 1.066760\n",
      "Epoch: 4762 \tTraining Loss: 0.000907 \tValidation Loss: 1.099935\n",
      "Epoch: 4763 \tTraining Loss: 0.001218 \tValidation Loss: 1.072413\n",
      "Epoch: 4764 \tTraining Loss: 0.001217 \tValidation Loss: 1.036928\n",
      "Epoch: 4765 \tTraining Loss: 0.000804 \tValidation Loss: 1.070803\n",
      "Epoch: 4766 \tTraining Loss: 0.000789 \tValidation Loss: 1.162521\n",
      "Epoch: 4767 \tTraining Loss: 0.001079 \tValidation Loss: 1.190545\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4768 \tTraining Loss: 0.000720 \tValidation Loss: 1.072355\n",
      "Epoch: 4769 \tTraining Loss: 0.000763 \tValidation Loss: 1.075647\n",
      "Epoch: 4770 \tTraining Loss: 0.003819 \tValidation Loss: 1.135445\n",
      "Epoch: 4771 \tTraining Loss: 0.003713 \tValidation Loss: 1.084059\n",
      "Epoch: 4772 \tTraining Loss: 0.003167 \tValidation Loss: 0.999307\n",
      "Epoch: 4773 \tTraining Loss: 0.001682 \tValidation Loss: 1.008118\n",
      "Epoch: 4774 \tTraining Loss: 0.001420 \tValidation Loss: 0.944596\n",
      "Epoch: 4775 \tTraining Loss: 0.001252 \tValidation Loss: 1.073421\n",
      "Epoch: 4776 \tTraining Loss: 0.001511 \tValidation Loss: 0.989870\n",
      "Epoch: 4777 \tTraining Loss: 0.000764 \tValidation Loss: 0.965402\n",
      "Epoch: 4778 \tTraining Loss: 0.000842 \tValidation Loss: 0.995163\n",
      "Epoch: 4779 \tTraining Loss: 0.001399 \tValidation Loss: 1.010094\n",
      "Epoch: 4780 \tTraining Loss: 0.000890 \tValidation Loss: 1.149442\n",
      "Epoch: 4781 \tTraining Loss: 0.002059 \tValidation Loss: 1.287959\n",
      "Epoch: 4782 \tTraining Loss: 0.000673 \tValidation Loss: 1.346494\n",
      "Epoch: 4783 \tTraining Loss: 0.001069 \tValidation Loss: 1.115247\n",
      "Epoch: 4784 \tTraining Loss: 0.001071 \tValidation Loss: 1.138099\n",
      "Epoch: 4785 \tTraining Loss: 0.000633 \tValidation Loss: 1.224724\n",
      "Epoch: 4786 \tTraining Loss: 0.000668 \tValidation Loss: 1.165141\n",
      "Epoch: 4787 \tTraining Loss: 0.000718 \tValidation Loss: 1.345136\n",
      "Epoch: 4788 \tTraining Loss: 0.001138 \tValidation Loss: 1.238275\n",
      "Epoch: 4789 \tTraining Loss: 0.001208 \tValidation Loss: 1.228501\n",
      "Epoch: 4790 \tTraining Loss: 0.000628 \tValidation Loss: 1.191256\n",
      "Epoch: 4791 \tTraining Loss: 0.001332 \tValidation Loss: 1.487218\n",
      "Epoch: 4792 \tTraining Loss: 0.000534 \tValidation Loss: 1.182229\n",
      "Epoch: 4793 \tTraining Loss: 0.000770 \tValidation Loss: 1.261157\n",
      "Epoch: 4794 \tTraining Loss: 0.000789 \tValidation Loss: 1.263712\n",
      "Epoch: 4795 \tTraining Loss: 0.000728 \tValidation Loss: 1.244901\n",
      "Epoch: 4796 \tTraining Loss: 0.000752 \tValidation Loss: 1.152982\n",
      "Epoch: 4797 \tTraining Loss: 0.001273 \tValidation Loss: 1.146421\n",
      "Epoch: 4798 \tTraining Loss: 0.000802 \tValidation Loss: 1.207647\n",
      "Epoch: 4799 \tTraining Loss: 0.000406 \tValidation Loss: 1.241943\n",
      "Epoch: 4800 \tTraining Loss: 0.000561 \tValidation Loss: 1.193674\n",
      "Epoch: 4801 \tTraining Loss: 0.002691 \tValidation Loss: 1.013632\n",
      "Epoch: 4802 \tTraining Loss: 0.001961 \tValidation Loss: 0.984845\n",
      "Epoch: 4803 \tTraining Loss: 0.000658 \tValidation Loss: 1.059439\n",
      "Epoch: 4804 \tTraining Loss: 0.001242 \tValidation Loss: 1.104494\n",
      "Epoch: 4805 \tTraining Loss: 0.000804 \tValidation Loss: 1.036715\n",
      "Epoch: 4806 \tTraining Loss: 0.001440 \tValidation Loss: 1.066543\n",
      "Epoch: 4807 \tTraining Loss: 0.000810 \tValidation Loss: 1.060243\n",
      "Epoch: 4808 \tTraining Loss: 0.000650 \tValidation Loss: 0.960574\n",
      "Epoch: 4809 \tTraining Loss: 0.004896 \tValidation Loss: 1.037978\n",
      "Epoch: 4810 \tTraining Loss: 0.001354 \tValidation Loss: 1.109062\n",
      "Epoch: 4811 \tTraining Loss: 0.001171 \tValidation Loss: 1.198879\n",
      "Epoch: 4812 \tTraining Loss: 0.001187 \tValidation Loss: 1.093247\n",
      "Epoch: 4813 \tTraining Loss: 0.001871 \tValidation Loss: 1.102927\n",
      "Epoch: 4814 \tTraining Loss: 0.001350 \tValidation Loss: 1.087806\n",
      "Epoch: 4815 \tTraining Loss: 0.001017 \tValidation Loss: 1.182545\n",
      "Epoch: 4816 \tTraining Loss: 0.000980 \tValidation Loss: 1.181174\n",
      "Epoch: 4817 \tTraining Loss: 0.000555 \tValidation Loss: 1.147847\n",
      "Epoch: 4818 \tTraining Loss: 0.002484 \tValidation Loss: 1.109379\n",
      "Epoch: 4819 \tTraining Loss: 0.001731 \tValidation Loss: 1.295670\n",
      "Epoch: 4820 \tTraining Loss: 0.000616 \tValidation Loss: 1.096689\n",
      "Epoch: 4821 \tTraining Loss: 0.002251 \tValidation Loss: 0.909845\n",
      "Epoch: 4822 \tTraining Loss: 0.000953 \tValidation Loss: 0.948661\n",
      "Epoch: 4823 \tTraining Loss: 0.001074 \tValidation Loss: 0.902695\n",
      "Epoch: 4824 \tTraining Loss: 0.000859 \tValidation Loss: 0.966806\n",
      "Epoch: 4825 \tTraining Loss: 0.000721 \tValidation Loss: 1.039444\n",
      "Epoch: 4826 \tTraining Loss: 0.000796 \tValidation Loss: 0.929602\n",
      "Epoch: 4827 \tTraining Loss: 0.001010 \tValidation Loss: 0.919319\n",
      "Epoch: 4828 \tTraining Loss: 0.000493 \tValidation Loss: 1.030532\n",
      "Epoch: 4829 \tTraining Loss: 0.000833 \tValidation Loss: 0.987621\n",
      "Epoch: 4830 \tTraining Loss: 0.000932 \tValidation Loss: 0.970352\n",
      "Epoch: 4831 \tTraining Loss: 0.000603 \tValidation Loss: 0.957438\n",
      "Epoch: 4832 \tTraining Loss: 0.000661 \tValidation Loss: 1.016554\n",
      "Epoch: 4833 \tTraining Loss: 0.000695 \tValidation Loss: 0.991027\n",
      "Epoch: 4834 \tTraining Loss: 0.000670 \tValidation Loss: 1.051680\n",
      "Epoch: 4835 \tTraining Loss: 0.001514 \tValidation Loss: 1.062943\n",
      "Epoch: 4836 \tTraining Loss: 0.001107 \tValidation Loss: 0.959843\n",
      "Epoch: 4837 \tTraining Loss: 0.000614 \tValidation Loss: 1.093445\n",
      "Epoch: 4838 \tTraining Loss: 0.000441 \tValidation Loss: 1.075302\n",
      "Epoch: 4839 \tTraining Loss: 0.002255 \tValidation Loss: 1.289417\n",
      "Epoch: 4840 \tTraining Loss: 0.002508 \tValidation Loss: 1.285658\n",
      "Epoch: 4841 \tTraining Loss: 0.000761 \tValidation Loss: 1.049775\n",
      "Epoch: 4842 \tTraining Loss: 0.001176 \tValidation Loss: 1.083396\n",
      "Epoch: 4843 \tTraining Loss: 0.001315 \tValidation Loss: 1.074197\n",
      "Epoch: 4844 \tTraining Loss: 0.001088 \tValidation Loss: 1.082070\n",
      "Epoch: 4845 \tTraining Loss: 0.000866 \tValidation Loss: 1.061443\n",
      "Epoch: 4846 \tTraining Loss: 0.000616 \tValidation Loss: 1.052793\n",
      "Epoch: 4847 \tTraining Loss: 0.000535 \tValidation Loss: 1.049522\n",
      "Epoch: 4848 \tTraining Loss: 0.000430 \tValidation Loss: 1.085297\n",
      "Epoch: 4849 \tTraining Loss: 0.005717 \tValidation Loss: 1.094852\n",
      "Epoch: 4850 \tTraining Loss: 0.001761 \tValidation Loss: 1.260271\n",
      "Epoch: 4851 \tTraining Loss: 0.008002 \tValidation Loss: 1.315040\n",
      "Epoch: 4852 \tTraining Loss: 0.112333 \tValidation Loss: 1.437192\n",
      "Epoch: 4853 \tTraining Loss: 0.439312 \tValidation Loss: 3.589722\n",
      "Epoch: 4854 \tTraining Loss: 0.197756 \tValidation Loss: 2.193584\n",
      "Epoch: 4855 \tTraining Loss: 0.034083 \tValidation Loss: 1.582119\n",
      "Epoch: 4856 \tTraining Loss: 0.035442 \tValidation Loss: 1.198754\n",
      "Epoch: 4857 \tTraining Loss: 0.143252 \tValidation Loss: 1.178767\n",
      "Epoch: 4858 \tTraining Loss: 0.023101 \tValidation Loss: 1.310300\n",
      "Epoch: 4859 \tTraining Loss: 0.010104 \tValidation Loss: 1.322063\n",
      "Epoch: 4860 \tTraining Loss: 0.039127 \tValidation Loss: 1.175101\n",
      "Epoch: 4861 \tTraining Loss: 0.013028 \tValidation Loss: 1.213612\n",
      "Epoch: 4862 \tTraining Loss: 0.052133 \tValidation Loss: 1.155246\n",
      "Epoch: 4863 \tTraining Loss: 0.015399 \tValidation Loss: 1.062004\n",
      "Epoch: 4864 \tTraining Loss: 0.029116 \tValidation Loss: 1.104053\n",
      "Epoch: 4865 \tTraining Loss: 0.009264 \tValidation Loss: 1.336555\n",
      "Epoch: 4866 \tTraining Loss: 0.003908 \tValidation Loss: 1.300873\n",
      "Epoch: 4867 \tTraining Loss: 0.003973 \tValidation Loss: 1.207930\n",
      "Epoch: 4868 \tTraining Loss: 0.002166 \tValidation Loss: 1.322466\n",
      "Epoch: 4869 \tTraining Loss: 0.010586 \tValidation Loss: 1.327357\n",
      "Epoch: 4870 \tTraining Loss: 0.006210 \tValidation Loss: 1.182167\n",
      "Epoch: 4871 \tTraining Loss: 0.003591 \tValidation Loss: 1.294741\n",
      "Epoch: 4872 \tTraining Loss: 0.004340 \tValidation Loss: 1.433925\n",
      "Epoch: 4873 \tTraining Loss: 0.002859 \tValidation Loss: 1.388459\n",
      "Epoch: 4874 \tTraining Loss: 0.003340 \tValidation Loss: 1.407048\n",
      "Epoch: 4875 \tTraining Loss: 0.003073 \tValidation Loss: 1.410194\n",
      "Epoch: 4876 \tTraining Loss: 0.001824 \tValidation Loss: 1.463366\n",
      "Epoch: 4877 \tTraining Loss: 0.003360 \tValidation Loss: 1.484701\n",
      "Epoch: 4878 \tTraining Loss: 0.004570 \tValidation Loss: 1.402543\n",
      "Epoch: 4879 \tTraining Loss: 0.002792 \tValidation Loss: 1.573795\n",
      "Epoch: 4880 \tTraining Loss: 0.002484 \tValidation Loss: 1.551293\n",
      "Epoch: 4881 \tTraining Loss: 0.002204 \tValidation Loss: 1.536942\n",
      "Epoch: 4882 \tTraining Loss: 0.002946 \tValidation Loss: 1.660215\n",
      "Epoch: 4883 \tTraining Loss: 0.002368 \tValidation Loss: 1.531242\n",
      "Epoch: 4884 \tTraining Loss: 0.001459 \tValidation Loss: 1.542894\n",
      "Epoch: 4885 \tTraining Loss: 0.001895 \tValidation Loss: 1.626835\n",
      "Epoch: 4886 \tTraining Loss: 0.002217 \tValidation Loss: 1.566055\n",
      "Epoch: 4887 \tTraining Loss: 0.001205 \tValidation Loss: 1.775229\n",
      "Epoch: 4888 \tTraining Loss: 0.001602 \tValidation Loss: 1.665288\n",
      "Epoch: 4889 \tTraining Loss: 0.001829 \tValidation Loss: 1.682853\n",
      "Epoch: 4890 \tTraining Loss: 0.001164 \tValidation Loss: 1.786874\n",
      "Epoch: 4891 \tTraining Loss: 0.001169 \tValidation Loss: 1.876091\n",
      "Epoch: 4892 \tTraining Loss: 0.001209 \tValidation Loss: 1.883885\n",
      "Epoch: 4893 \tTraining Loss: 0.001392 \tValidation Loss: 1.694328\n",
      "Epoch: 4894 \tTraining Loss: 0.003863 \tValidation Loss: 1.636884\n",
      "Epoch: 4895 \tTraining Loss: 0.003043 \tValidation Loss: 1.595092\n",
      "Epoch: 4896 \tTraining Loss: 0.002061 \tValidation Loss: 1.477911\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 4897 \tTraining Loss: 0.006421 \tValidation Loss: 1.550399\n",
      "Epoch: 4898 \tTraining Loss: 0.001692 \tValidation Loss: 1.634780\n",
      "Epoch: 4899 \tTraining Loss: 0.001461 \tValidation Loss: 1.739898\n",
      "Epoch: 4900 \tTraining Loss: 0.001470 \tValidation Loss: 1.783551\n",
      "Epoch: 4901 \tTraining Loss: 0.001367 \tValidation Loss: 1.641313\n",
      "Epoch: 4902 \tTraining Loss: 0.003178 \tValidation Loss: 1.758186\n",
      "Epoch: 4903 \tTraining Loss: 0.001213 \tValidation Loss: 1.814692\n",
      "Epoch: 4904 \tTraining Loss: 0.001223 \tValidation Loss: 1.874274\n",
      "Epoch: 4905 \tTraining Loss: 0.001558 \tValidation Loss: 1.836564\n",
      "Epoch: 4906 \tTraining Loss: 0.002331 \tValidation Loss: 1.739322\n",
      "Epoch: 4907 \tTraining Loss: 0.000720 \tValidation Loss: 1.800062\n",
      "Epoch: 4908 \tTraining Loss: 0.001245 \tValidation Loss: 1.721577\n",
      "Epoch: 4909 \tTraining Loss: 0.000891 \tValidation Loss: 1.669530\n",
      "Epoch: 4910 \tTraining Loss: 0.002472 \tValidation Loss: 1.736958\n",
      "Epoch: 4911 \tTraining Loss: 0.001171 \tValidation Loss: 1.755741\n",
      "Epoch: 4912 \tTraining Loss: 0.001558 \tValidation Loss: 1.690777\n",
      "Epoch: 4913 \tTraining Loss: 0.000944 \tValidation Loss: 1.655631\n",
      "Epoch: 4914 \tTraining Loss: 0.001444 \tValidation Loss: 1.705613\n",
      "Epoch: 4915 \tTraining Loss: 0.001093 \tValidation Loss: 1.776032\n",
      "Epoch: 4916 \tTraining Loss: 0.001585 \tValidation Loss: 1.656736\n",
      "Epoch: 4917 \tTraining Loss: 0.015920 \tValidation Loss: 1.830939\n",
      "Epoch: 4918 \tTraining Loss: 0.080600 \tValidation Loss: 1.154090\n",
      "Epoch: 4919 \tTraining Loss: 0.280203 \tValidation Loss: 1.455890\n",
      "Epoch: 4920 \tTraining Loss: 0.195829 \tValidation Loss: 0.657260\n",
      "Epoch: 4921 \tTraining Loss: 0.085111 \tValidation Loss: 1.386326\n",
      "Epoch: 4922 \tTraining Loss: 0.056412 \tValidation Loss: 1.011983\n",
      "Epoch: 4923 \tTraining Loss: 0.054144 \tValidation Loss: 0.857563\n",
      "Epoch: 4924 \tTraining Loss: 0.014395 \tValidation Loss: 0.723249\n",
      "Epoch: 4925 \tTraining Loss: 0.010310 \tValidation Loss: 0.879398\n",
      "Epoch: 4926 \tTraining Loss: 0.009151 \tValidation Loss: 0.919208\n",
      "Epoch: 4927 \tTraining Loss: 0.015328 \tValidation Loss: 1.175478\n",
      "Epoch: 4928 \tTraining Loss: 0.027376 \tValidation Loss: 1.191084\n",
      "Epoch: 4929 \tTraining Loss: 0.012876 \tValidation Loss: 1.074595\n",
      "Epoch: 4930 \tTraining Loss: 0.009913 \tValidation Loss: 0.952328\n",
      "Epoch: 4931 \tTraining Loss: 0.006310 \tValidation Loss: 0.992701\n",
      "Epoch: 4932 \tTraining Loss: 0.023607 \tValidation Loss: 1.010645\n",
      "Epoch: 4933 \tTraining Loss: 0.022820 \tValidation Loss: 1.039523\n",
      "Epoch: 4934 \tTraining Loss: 0.054364 \tValidation Loss: 0.804380\n",
      "Epoch: 4935 \tTraining Loss: 0.069739 \tValidation Loss: 1.615660\n",
      "Epoch: 4936 \tTraining Loss: 0.223883 \tValidation Loss: 1.357816\n",
      "Epoch: 4937 \tTraining Loss: 0.047949 \tValidation Loss: 1.148359\n",
      "Epoch: 4938 \tTraining Loss: 0.035112 \tValidation Loss: 1.153403\n",
      "Epoch: 4939 \tTraining Loss: 0.012408 \tValidation Loss: 0.959083\n",
      "Epoch: 4940 \tTraining Loss: 0.007886 \tValidation Loss: 1.056870\n",
      "Epoch: 4941 \tTraining Loss: 0.003384 \tValidation Loss: 1.144608\n",
      "Epoch: 4942 \tTraining Loss: 0.003826 \tValidation Loss: 1.147025\n",
      "Epoch: 4943 \tTraining Loss: 0.002740 \tValidation Loss: 1.169225\n",
      "Epoch: 4944 \tTraining Loss: 0.002976 \tValidation Loss: 1.195149\n",
      "Epoch: 4945 \tTraining Loss: 0.003618 \tValidation Loss: 1.279483\n",
      "Epoch: 4946 \tTraining Loss: 0.003234 \tValidation Loss: 1.142708\n",
      "Epoch: 4947 \tTraining Loss: 0.001650 \tValidation Loss: 1.242530\n",
      "Epoch: 4948 \tTraining Loss: 0.001835 \tValidation Loss: 1.251006\n",
      "Epoch: 4949 \tTraining Loss: 0.004980 \tValidation Loss: 1.146870\n",
      "Epoch: 4950 \tTraining Loss: 0.004462 \tValidation Loss: 1.262052\n",
      "Epoch: 4951 \tTraining Loss: 0.002053 \tValidation Loss: 1.273613\n",
      "Epoch: 4952 \tTraining Loss: 0.010178 \tValidation Loss: 1.259643\n",
      "Epoch: 4953 \tTraining Loss: 0.004474 \tValidation Loss: 1.219805\n",
      "Epoch: 4954 \tTraining Loss: 0.003539 \tValidation Loss: 1.326131\n",
      "Epoch: 4955 \tTraining Loss: 0.009756 \tValidation Loss: 1.148869\n",
      "Epoch: 4956 \tTraining Loss: 0.010951 \tValidation Loss: 1.063346\n",
      "Epoch: 4957 \tTraining Loss: 0.013404 \tValidation Loss: 0.952491\n",
      "Epoch: 4958 \tTraining Loss: 0.003911 \tValidation Loss: 1.066512\n",
      "Epoch: 4959 \tTraining Loss: 0.004444 \tValidation Loss: 1.087281\n",
      "Epoch: 4960 \tTraining Loss: 0.005444 \tValidation Loss: 1.033549\n",
      "Epoch: 4961 \tTraining Loss: 0.001218 \tValidation Loss: 1.121635\n",
      "Epoch: 4962 \tTraining Loss: 0.000894 \tValidation Loss: 1.153383\n",
      "Epoch: 4963 \tTraining Loss: 0.002576 \tValidation Loss: 1.050262\n",
      "Epoch: 4964 \tTraining Loss: 0.001351 \tValidation Loss: 1.015797\n",
      "Epoch: 4965 \tTraining Loss: 0.002072 \tValidation Loss: 1.071544\n",
      "Epoch: 4966 \tTraining Loss: 0.001116 \tValidation Loss: 1.029315\n",
      "Epoch: 4967 \tTraining Loss: 0.001355 \tValidation Loss: 1.052820\n",
      "Epoch: 4968 \tTraining Loss: 0.001543 \tValidation Loss: 1.035368\n",
      "Epoch: 4969 \tTraining Loss: 0.001031 \tValidation Loss: 1.023695\n",
      "Epoch: 4970 \tTraining Loss: 0.001793 \tValidation Loss: 1.063132\n",
      "Epoch: 4971 \tTraining Loss: 0.000937 \tValidation Loss: 0.956257\n",
      "Epoch: 4972 \tTraining Loss: 0.008492 \tValidation Loss: 0.960668\n",
      "Epoch: 4973 \tTraining Loss: 0.001479 \tValidation Loss: 0.928727\n",
      "Epoch: 4974 \tTraining Loss: 0.001505 \tValidation Loss: 1.028272\n",
      "Epoch: 4975 \tTraining Loss: 0.002806 \tValidation Loss: 0.897627\n",
      "Epoch: 4976 \tTraining Loss: 0.001871 \tValidation Loss: 0.948606\n",
      "Epoch: 4977 \tTraining Loss: 0.002038 \tValidation Loss: 0.875043\n",
      "Epoch: 4978 \tTraining Loss: 0.001038 \tValidation Loss: 0.933332\n",
      "Epoch: 4979 \tTraining Loss: 0.001698 \tValidation Loss: 0.946637\n",
      "Epoch: 4980 \tTraining Loss: 0.002038 \tValidation Loss: 0.938747\n",
      "Epoch: 4981 \tTraining Loss: 0.001312 \tValidation Loss: 0.905177\n",
      "Epoch: 4982 \tTraining Loss: 0.001462 \tValidation Loss: 0.862415\n",
      "Epoch: 4983 \tTraining Loss: 0.002116 \tValidation Loss: 0.878165\n",
      "Epoch: 4984 \tTraining Loss: 0.001093 \tValidation Loss: 0.915237\n",
      "Epoch: 4985 \tTraining Loss: 0.002708 \tValidation Loss: 0.927741\n",
      "Epoch: 4986 \tTraining Loss: 0.001988 \tValidation Loss: 1.021961\n",
      "Epoch: 4987 \tTraining Loss: 0.004224 \tValidation Loss: 0.871266\n",
      "Epoch: 4988 \tTraining Loss: 0.000999 \tValidation Loss: 1.009216\n",
      "Epoch: 4989 \tTraining Loss: 0.010161 \tValidation Loss: 1.027517\n",
      "Epoch: 4990 \tTraining Loss: 0.002174 \tValidation Loss: 1.772831\n",
      "Epoch: 4991 \tTraining Loss: 0.007954 \tValidation Loss: 1.816990\n",
      "Epoch: 4992 \tTraining Loss: 0.006767 \tValidation Loss: 1.587487\n",
      "Epoch: 4993 \tTraining Loss: 0.001568 \tValidation Loss: 1.422836\n",
      "Epoch: 4994 \tTraining Loss: 0.001834 \tValidation Loss: 1.298899\n",
      "Epoch: 4995 \tTraining Loss: 0.002272 \tValidation Loss: 1.383392\n",
      "Epoch: 4996 \tTraining Loss: 0.000907 \tValidation Loss: 1.295307\n",
      "Epoch: 4997 \tTraining Loss: 0.002000 \tValidation Loss: 1.336387\n",
      "Epoch: 4998 \tTraining Loss: 0.001552 \tValidation Loss: 1.127644\n",
      "Epoch: 4999 \tTraining Loss: 0.000854 \tValidation Loss: 1.154838\n",
      "Epoch: 5000 \tTraining Loss: 0.002240 \tValidation Loss: 1.244325\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 5000 #학습횟수입니다. 100이상 설정하시고 오래 보시는 것을 추천드립니다. \n",
    "save_transfer = model_path.joinpath('model_transfer_wide_Resnet101_SGD0001_2021_11_18.pt') #저장할 모델의 파일명을 입력하시면 됩니다.2\n",
    "\n",
    "model_transfer, train_losses, valid_losses = train(epochs, train_batches, valid_batches, model_transfer, optimizer_transfer, criterion_transfer, use_cuda, save_transfer)\n",
    "\n",
    "model_transfer.load_state_dict(torch.load(save_transfer)) #학습 종료 후 저장된 모델을 다시 로드하는 부분입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "#training loss와 validation loss를 그래프로 표현합니다.\n",
    "train_losses = train_losses\n",
    "valid_losses = valid_losses\n",
    "\n",
    "plt.plot(train_losses, label='Training loss')\n",
    "plt.plot(valid_losses,label='Validation loss')\n",
    "plt.legend(frameon=False)\n",
    "plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_transfer.load_state_dict(torch.load(save_transfer)) #학습 종료 후 저장된 모델을 다시 로드하는 부분입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST 기능입니다\n",
    "\n",
    "def test(test_loader, model, criterion, use_cuda):\n",
    "    class_correct = list(0. for i in range(2))\n",
    "    class_total = list(0. for i in range(2))\n",
    "    test_loss = 0.0\n",
    "    accuracy = 0\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss\n",
    "            _, pred = torch.max(output, 1)    \n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.numpy())  if not use_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            \n",
    "            for i in range(2): #batch-size에 따라 조절하시면 됩니다.\n",
    "                label = target[i].data\n",
    "                class_correct[label] += correct[i].item()\n",
    "                class_total[label] += 1\n",
    "\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total),np.sum(class_correct), np.sum(class_total)))\n",
    "    print('\\n')\n",
    "    for i in range(2):\n",
    "        print('Accuracy of %5s : %2d %% (%2d/%2d)' % (classes[i], 100 * class_correct[i] / class_total[i],np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "### TEST 기능입니다\n",
    "\n",
    "def test(test_loader, model, criterion, use_cuda):\n",
    "    class_correct = list(0. for i in range(2))\n",
    "    class_total = list(0. for i in range(2))\n",
    "    test_loss = 0.0\n",
    "    accuracy = 0\n",
    "    if use_cuda:\n",
    "        model = model.cuda()\n",
    "    model.eval()\n",
    "    with torch.no_grad(): \n",
    "        for data, target in test_loader:\n",
    "            \n",
    "            if use_cuda:\n",
    "                data, target = data.cuda(), target.cuda()\n",
    "            output = model(data)\n",
    "            loss = criterion(output, target)\n",
    "            test_loss += loss\n",
    "            _, pred = torch.max(output, 1)    \n",
    "            correct_tensor = pred.eq(target.data.view_as(pred))\n",
    "            correct = np.squeeze(correct_tensor.numpy())  if not use_cuda else np.squeeze(correct_tensor.cpu().numpy())\n",
    "            \n",
    "            label = target.data\n",
    "            class_correct[label] += correct.item()\n",
    "            class_total[label] += 1\n",
    "\n",
    "    test_loss = test_loss/len(test_loader.dataset)\n",
    "    print('Test Loss: {:.6f}\\n'.format(test_loss))\n",
    "    print('\\nTest Accuracy (Overall): %2d%% (%2d/%2d)' % (100. * np.sum(class_correct) / np.sum(class_total),np.sum(class_correct), np.sum(class_total)))\n",
    "    print('\\n')\n",
    "    for i in range(2):\n",
    "        print('Accuracy of %5s : %2d %% (%2d/%2d)' % (classes[i], 100 * class_correct[i] / class_total[i],np.sum(class_correct[i]), np.sum(class_total[i])))\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_batches = torch.utils.data.DataLoader(testing, batch_size=1,num_workers=0,pin_memory=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "23"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_batches)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.438536\n",
      "\n",
      "\n",
      "Test Accuracy (Overall): 56% (13/23)\n",
      "\n",
      "\n",
      "Accuracy of Negative : 100 % (11/11)\n",
      "Accuracy of Positive : 16 % ( 2/12)\n"
     ]
    }
   ],
   "source": [
    "classes = ('Negative','Positive')\n",
    "test(test_batches, model_transfer, criterion_transfer, use_cuda) #저는 가장 잘 나왔을때 평균 82%까지 나왔습니다. 노트북으로 돌리는 바람에 vram 부족으로 여러 조합을 실시해보지는 못했으나 여러 조합으로 튜닝해보시면 결과가 더 잘 나올것 같습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
